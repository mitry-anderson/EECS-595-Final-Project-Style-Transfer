If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
loaded 1220 training samples
loaded 273 validation samples
loaded 33 test samples
Begin testing style transfer!
Traceback (most recent call last):
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 431, in <module>
    main(params)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 412, in main
    evaluate_transfer(model,classifier, train_dataloader, eval_dataloader, params, input_tokenizer, output_tokenizer)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 200, in evaluate_transfer
    z_alt = fgim_attack(model, classifier, ((cls_pred + 1)%2), z)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 87, in fgim_attack
    loss.backward(retain_graph=True)
  File "/home/mitryand/.local/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mitryand/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 166, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/mitryand/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 67, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
