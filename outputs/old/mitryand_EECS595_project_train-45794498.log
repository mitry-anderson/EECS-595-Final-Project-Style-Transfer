If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`
loaded 1220 training samples
loaded 273 validation samples
loaded 33 test samples
Begin testing style transfer!
Traceback (most recent call last):
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 415, in <module>
    main(params)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 396, in main
    evaluate_transfer(model,classifier, train_dataloader, eval_dataloader, params, input_tokenizer, output_tokenizer)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 184, in evaluate_transfer
    z_alt = fgim_attack(model, classifier, ((cls_pred + 1)%2), z)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 71, in fgim_attack
    loss.backward()
  File "/home/mitryand/.local/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mitryand/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
