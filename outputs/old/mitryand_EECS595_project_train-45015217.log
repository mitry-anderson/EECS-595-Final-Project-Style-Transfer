Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.1.crossattention.masked_bias', 'h.5.crossattention.c_proj.weight', 'h.0.crossattention.bias', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.masked_bias', 'h.7.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.weight', 'h.0.crossattention.masked_bias', 'h.1.ln_cross_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.5.ln_cross_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.5.crossattention.masked_bias', 'h.11.crossattention.c_proj.bias', 'h.8.crossattention.masked_bias', 'h.7.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.10.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.4.ln_cross_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.5.crossattention.bias', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.bias', 'h.9.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.3.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.10.crossattention.bias', 'h.6.crossattention.c_attn.weight', 'h.10.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.7.crossattention.masked_bias', 'h.6.ln_cross_attn.weight', 'h.6.crossattention.bias', 'h.7.ln_cross_attn.weight', 'h.11.crossattention.masked_bias', 'h.2.crossattention.masked_bias', 'h.9.ln_cross_attn.weight', 'h.8.crossattention.bias', 'h.8.crossattention.c_proj.weight', 'h.8.ln_cross_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.weight', 'h.7.crossattention.bias', 'h.10.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.4.crossattention.bias', 'h.0.ln_cross_attn.weight', 'h.4.crossattention.masked_bias', 'h.1.crossattention.bias', 'h.0.crossattention.c_proj.weight', 'h.6.crossattention.masked_bias', 'h.3.crossattention.c_attn.weight', 'h.2.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.9.crossattention.bias', 'h.4.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.weight', 'h.11.crossattention.bias', 'h.3.crossattention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded 232 training samples
loaded 53 validation samples
loaded 6 test samples
created model
Begin training!
  0%|          | 0/232 [00:00<?, ?it/s]/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
  0%|          | 1/232 [00:01<05:37,  1.46s/it]  1%|          | 2/232 [00:02<03:59,  1.04s/it]  1%|▏         | 3/232 [00:02<03:26,  1.11it/s]  2%|▏         | 4/232 [00:03<03:11,  1.19it/s]  2%|▏         | 5/232 [00:04<03:02,  1.24it/s]  3%|▎         | 6/232 [00:05<02:56,  1.28it/s]  3%|▎         | 7/232 [00:05<02:52,  1.30it/s]  3%|▎         | 8/232 [00:06<02:50,  1.32it/s]  4%|▍         | 9/232 [00:07<02:48,  1.32it/s]  4%|▍         | 10/232 [00:08<02:46,  1.33it/s]  5%|▍         | 11/232 [00:08<02:45,  1.33it/s]  5%|▌         | 12/232 [00:09<02:44,  1.34it/s]  6%|▌         | 13/232 [00:10<02:43,  1.34it/s]  6%|▌         | 14/232 [00:11<02:42,  1.34it/s]  6%|▋         | 15/232 [00:11<02:41,  1.34it/s]  7%|▋         | 16/232 [00:12<02:41,  1.34it/s]  7%|▋         | 17/232 [00:13<02:40,  1.34it/s]  8%|▊         | 18/232 [00:14<02:39,  1.34it/s]  8%|▊         | 19/232 [00:14<02:38,  1.34it/s]  9%|▊         | 20/232 [00:15<02:37,  1.34it/s]  9%|▉         | 21/232 [00:16<02:36,  1.35it/s]  9%|▉         | 22/232 [00:17<02:35,  1.35it/s] 10%|▉         | 23/232 [00:17<02:35,  1.35it/s] 10%|█         | 24/232 [00:18<02:34,  1.35it/s] 11%|█         | 25/232 [00:19<02:33,  1.35it/s] 11%|█         | 26/232 [00:20<02:32,  1.35it/s] 12%|█▏        | 27/232 [00:20<02:31,  1.35it/s] 12%|█▏        | 28/232 [00:21<02:31,  1.35it/s] 12%|█▎        | 29/232 [00:22<02:30,  1.35it/s] 13%|█▎        | 30/232 [00:22<02:29,  1.35it/s] 13%|█▎        | 31/232 [00:23<02:29,  1.35it/s] 14%|█▍        | 32/232 [00:24<02:28,  1.35it/s] 14%|█▍        | 33/232 [00:25<02:27,  1.35it/s] 15%|█▍        | 34/232 [00:25<02:27,  1.35it/s] 15%|█▌        | 35/232 [00:26<02:26,  1.35it/s] 16%|█▌        | 36/232 [00:27<02:25,  1.35it/s] 16%|█▌        | 37/232 [00:28<02:24,  1.35it/s] 16%|█▋        | 38/232 [00:28<02:24,  1.35it/s] 17%|█▋        | 39/232 [00:29<02:23,  1.34it/s] 17%|█▋        | 40/232 [00:30<02:22,  1.34it/s] 18%|█▊        | 41/232 [00:31<02:22,  1.34it/s] 18%|█▊        | 42/232 [00:31<02:21,  1.34it/s] 19%|█▊        | 43/232 [00:32<02:20,  1.34it/s] 19%|█▉        | 44/232 [00:33<02:20,  1.34it/s] 19%|█▉        | 45/232 [00:34<02:19,  1.34it/s] 20%|█▉        | 46/232 [00:34<02:18,  1.34it/s] 20%|██        | 47/232 [00:35<02:17,  1.34it/s] 21%|██        | 48/232 [00:36<02:17,  1.34it/s] 21%|██        | 49/232 [00:37<02:16,  1.34it/s] 22%|██▏       | 50/232 [00:37<02:15,  1.34it/s] 22%|██▏       | 51/232 [00:38<02:15,  1.34it/s] 22%|██▏       | 52/232 [00:39<02:14,  1.34it/s] 23%|██▎       | 53/232 [00:40<02:13,  1.34it/s] 23%|██▎       | 54/232 [00:40<02:12,  1.34it/s] 24%|██▎       | 55/232 [00:41<02:12,  1.34it/s] 24%|██▍       | 56/232 [00:42<02:11,  1.34it/s] 25%|██▍       | 57/232 [00:43<02:10,  1.34it/s] 25%|██▌       | 58/232 [00:43<02:09,  1.34it/s] 25%|██▌       | 59/232 [00:44<02:09,  1.34it/s] 26%|██▌       | 60/232 [00:45<02:08,  1.34it/s] 26%|██▋       | 61/232 [00:46<02:07,  1.34it/s] 27%|██▋       | 62/232 [00:46<02:07,  1.34it/s] 27%|██▋       | 63/232 [00:47<02:06,  1.34it/s] 28%|██▊       | 64/232 [00:48<02:05,  1.34it/s] 28%|██▊       | 65/232 [00:49<02:04,  1.34it/s] 28%|██▊       | 66/232 [00:49<02:04,  1.34it/s] 29%|██▉       | 67/232 [00:50<02:03,  1.34it/s] 29%|██▉       | 68/232 [00:51<02:02,  1.34it/s] 30%|██▉       | 69/232 [00:52<02:01,  1.34it/s] 30%|███       | 70/232 [00:52<02:00,  1.34it/s] 31%|███       | 71/232 [00:53<02:00,  1.34it/s] 31%|███       | 72/232 [00:54<01:59,  1.34it/s] 31%|███▏      | 73/232 [00:55<01:59,  1.33it/s] 32%|███▏      | 74/232 [00:55<01:58,  1.33it/s] 32%|███▏      | 75/232 [00:56<01:58,  1.33it/s] 33%|███▎      | 76/232 [00:57<01:57,  1.33it/s] 33%|███▎      | 77/232 [00:58<01:56,  1.33it/s] 34%|███▎      | 78/232 [00:58<01:55,  1.34it/s] 34%|███▍      | 79/232 [00:59<01:55,  1.33it/s] 34%|███▍      | 80/232 [01:00<01:54,  1.33it/s] 35%|███▍      | 81/232 [01:01<01:53,  1.33it/s] 35%|███▌      | 82/232 [01:01<01:58,  1.27it/s] 36%|███▌      | 83/232 [01:02<01:55,  1.29it/s] 36%|███▌      | 84/232 [01:03<01:53,  1.30it/s] 37%|███▋      | 85/232 [01:04<01:52,  1.31it/s] 37%|███▋      | 86/232 [01:04<01:51,  1.31it/s] 38%|███▊      | 87/232 [01:05<01:50,  1.32it/s] 38%|███▊      | 88/232 [01:06<01:49,  1.32it/s] 38%|███▊      | 89/232 [01:07<01:48,  1.32it/s] 39%|███▉      | 90/232 [01:08<01:47,  1.32it/s] 39%|███▉      | 91/232 [01:08<01:46,  1.32it/s] 40%|███▉      | 92/232 [01:09<01:45,  1.32it/s] 40%|████      | 93/232 [01:10<01:45,  1.32it/s] 41%|████      | 94/232 [01:11<01:44,  1.32it/s] 41%|████      | 95/232 [01:11<01:43,  1.32it/s] 41%|████▏     | 96/232 [01:12<01:42,  1.32it/s] 42%|████▏     | 97/232 [01:13<01:42,  1.32it/s] 42%|████▏     | 98/232 [01:14<01:41,  1.32it/s] 43%|████▎     | 99/232 [01:14<01:40,  1.32it/s] 43%|████▎     | 100/232 [01:15<01:39,  1.32it/s] 44%|████▎     | 101/232 [01:16<01:38,  1.32it/s] 44%|████▍     | 102/232 [01:17<01:38,  1.32it/s] 44%|████▍     | 103/232 [01:17<01:37,  1.32it/s] 45%|████▍     | 104/232 [01:18<01:36,  1.32it/s] 45%|████▌     | 105/232 [01:19<01:35,  1.32it/s] 46%|████▌     | 106/232 [01:20<01:35,  1.32it/s] 46%|████▌     | 107/232 [01:20<01:34,  1.32it/s] 47%|████▋     | 108/232 [01:21<01:33,  1.32it/s] 47%|████▋     | 109/232 [01:22<01:32,  1.32it/s] 47%|████▋     | 110/232 [01:23<01:32,  1.33it/s] 48%|████▊     | 111/232 [01:23<01:31,  1.33it/s] 48%|████▊     | 112/232 [01:24<01:30,  1.32it/s] 49%|████▊     | 113/232 [01:25<01:29,  1.32it/s] 49%|████▉     | 114/232 [01:26<01:29,  1.32it/s] 50%|████▉     | 115/232 [01:26<01:28,  1.33it/s] 50%|█████     | 116/232 [01:27<01:27,  1.33it/s] 50%|█████     | 117/232 [01:28<01:26,  1.33it/s] 51%|█████     | 118/232 [01:29<01:26,  1.33it/s] 51%|█████▏    | 119/232 [01:29<01:25,  1.32it/s] 52%|█████▏    | 120/232 [01:30<01:24,  1.32it/s] 52%|█████▏    | 121/232 [01:31<01:23,  1.32it/s] 53%|█████▎    | 122/232 [01:32<01:23,  1.32it/s] 53%|█████▎    | 123/232 [01:32<01:22,  1.32it/s] 53%|█████▎    | 124/232 [01:33<01:21,  1.32it/s] 54%|█████▍    | 125/232 [01:34<01:21,  1.32it/s] 54%|█████▍    | 126/232 [01:35<01:20,  1.32it/s] 55%|█████▍    | 127/232 [01:35<01:19,  1.32it/s] 55%|█████▌    | 128/232 [01:36<01:18,  1.32it/s] 56%|█████▌    | 129/232 [01:37<01:18,  1.32it/s] 56%|█████▌    | 130/232 [01:38<01:17,  1.32it/s] 56%|█████▋    | 131/232 [01:38<01:16,  1.32it/s] 57%|█████▋    | 132/232 [01:39<01:15,  1.32it/s] 57%|█████▋    | 133/232 [01:40<01:15,  1.32it/s] 58%|█████▊    | 134/232 [01:41<01:14,  1.32it/s] 58%|█████▊    | 135/232 [01:42<01:13,  1.32it/s] 59%|█████▊    | 136/232 [01:42<01:12,  1.32it/s] 59%|█████▉    | 137/232 [01:43<01:12,  1.32it/s] 59%|█████▉    | 138/232 [01:44<01:11,  1.32it/s] 60%|█████▉    | 139/232 [01:45<01:10,  1.32it/s] 60%|██████    | 140/232 [01:45<01:09,  1.32it/s] 61%|██████    | 141/232 [01:46<01:08,  1.32it/s] 61%|██████    | 142/232 [01:47<01:08,  1.32it/s] 62%|██████▏   | 143/232 [01:48<01:07,  1.32it/s] 62%|██████▏   | 144/232 [01:48<01:06,  1.32it/s] 62%|██████▎   | 145/232 [01:49<01:05,  1.32it/s] 63%|██████▎   | 146/232 [01:50<01:05,  1.32it/s] 63%|██████▎   | 147/232 [01:51<01:04,  1.32it/s] 64%|██████▍   | 148/232 [01:51<01:03,  1.32it/s] 64%|██████▍   | 149/232 [01:52<01:02,  1.32it/s] 65%|██████▍   | 150/232 [01:53<01:02,  1.32it/s] 65%|██████▌   | 151/232 [01:54<01:01,  1.32it/s] 66%|██████▌   | 152/232 [01:54<01:00,  1.32it/s] 66%|██████▌   | 153/232 [01:55<00:59,  1.32it/s] 66%|██████▋   | 154/232 [01:56<00:59,  1.32it/s] 67%|██████▋   | 155/232 [01:57<00:58,  1.32it/s] 67%|██████▋   | 156/232 [01:57<00:57,  1.32it/s] 68%|██████▊   | 157/232 [01:58<00:56,  1.32it/s] 68%|██████▊   | 158/232 [01:59<00:56,  1.32it/s] 69%|██████▊   | 159/232 [02:00<00:55,  1.32it/s] 69%|██████▉   | 160/232 [02:00<00:54,  1.32it/s] 69%|██████▉   | 161/232 [02:01<00:53,  1.32it/s] 70%|██████▉   | 162/232 [02:02<00:53,  1.32it/s] 70%|███████   | 163/232 [02:03<00:52,  1.32it/s] 71%|███████   | 164/232 [02:04<00:51,  1.32it/s] 71%|███████   | 165/232 [02:04<00:50,  1.32it/s] 72%|███████▏  | 166/232 [02:05<00:50,  1.32it/s] 72%|███████▏  | 167/232 [02:06<00:49,  1.32it/s] 72%|███████▏  | 168/232 [02:07<00:48,  1.32it/s] 73%|███████▎  | 169/232 [02:07<00:47,  1.32it/s] 73%|███████▎  | 170/232 [02:08<00:47,  1.32it/s] 74%|███████▎  | 171/232 [02:09<00:46,  1.32it/s] 74%|███████▍  | 172/232 [02:10<00:45,  1.32it/s] 75%|███████▍  | 173/232 [02:10<00:44,  1.32it/s] 75%|███████▌  | 174/232 [02:11<00:43,  1.32it/s] 75%|███████▌  | 175/232 [02:12<00:43,  1.32it/s] 76%|███████▌  | 176/232 [02:13<00:42,  1.32it/s] 76%|███████▋  | 177/232 [02:13<00:41,  1.32it/s] 77%|███████▋  | 178/232 [02:14<00:41,  1.32it/s] 77%|███████▋  | 179/232 [02:15<00:40,  1.32it/s] 78%|███████▊  | 180/232 [02:16<00:39,  1.32it/s] 78%|███████▊  | 181/232 [02:16<00:38,  1.32it/s] 78%|███████▊  | 182/232 [02:17<00:37,  1.32it/s] 79%|███████▉  | 183/232 [02:18<00:37,  1.32it/s] 79%|███████▉  | 184/232 [02:19<00:36,  1.32it/s] 80%|███████▉  | 185/232 [02:19<00:35,  1.32it/s] 80%|████████  | 186/232 [02:20<00:34,  1.32it/s] 81%|████████  | 187/232 [02:21<00:34,  1.32it/s] 81%|████████  | 188/232 [02:22<00:33,  1.32it/s] 81%|████████▏ | 189/232 [02:22<00:32,  1.32it/s] 82%|████████▏ | 190/232 [02:23<00:31,  1.32it/s] 82%|████████▏ | 191/232 [02:24<00:31,  1.32it/s] 83%|████████▎ | 192/232 [02:25<00:30,  1.32it/s] 83%|████████▎ | 193/232 [02:26<00:29,  1.32it/s] 84%|████████▎ | 194/232 [02:26<00:28,  1.32it/s] 84%|████████▍ | 195/232 [02:27<00:28,  1.32it/s] 84%|████████▍ | 196/232 [02:28<00:27,  1.32it/s] 85%|████████▍ | 197/232 [02:29<00:26,  1.32it/s] 85%|████████▌ | 198/232 [02:29<00:25,  1.32it/s] 86%|████████▌ | 199/232 [02:30<00:25,  1.32it/s] 86%|████████▌ | 200/232 [02:31<00:24,  1.32it/s] 87%|████████▋ | 201/232 [02:32<00:23,  1.32it/s] 87%|████████▋ | 202/232 [02:32<00:22,  1.32it/s] 88%|████████▊ | 203/232 [02:33<00:21,  1.32it/s] 88%|████████▊ | 204/232 [02:34<00:21,  1.32it/s] 88%|████████▊ | 205/232 [02:35<00:20,  1.32it/s] 89%|████████▉ | 206/232 [02:35<00:19,  1.32it/s] 89%|████████▉ | 207/232 [02:36<00:18,  1.32it/s] 90%|████████▉ | 208/232 [02:37<00:18,  1.32it/s] 90%|█████████ | 209/232 [02:38<00:17,  1.32it/s] 91%|█████████ | 210/232 [02:38<00:16,  1.32it/s] 91%|█████████ | 211/232 [02:39<00:15,  1.31it/s] 91%|█████████▏| 212/232 [02:40<00:15,  1.32it/s] 92%|█████████▏| 213/232 [02:41<00:14,  1.32it/s] 92%|█████████▏| 214/232 [02:41<00:13,  1.32it/s] 93%|█████████▎| 215/232 [02:42<00:12,  1.32it/s] 93%|█████████▎| 216/232 [02:43<00:12,  1.32it/s] 94%|█████████▎| 217/232 [02:44<00:11,  1.32it/s] 94%|█████████▍| 218/232 [02:45<00:10,  1.32it/s] 94%|█████████▍| 219/232 [02:45<00:09,  1.32it/s] 95%|█████████▍| 220/232 [02:46<00:09,  1.32it/s] 95%|█████████▌| 221/232 [02:47<00:08,  1.32it/s] 96%|█████████▌| 222/232 [02:48<00:07,  1.32it/s] 96%|█████████▌| 223/232 [02:48<00:06,  1.32it/s] 97%|█████████▋| 224/232 [02:49<00:06,  1.32it/s] 97%|█████████▋| 225/232 [02:50<00:05,  1.32it/s] 97%|█████████▋| 226/232 [02:51<00:04,  1.32it/s] 98%|█████████▊| 227/232 [02:51<00:03,  1.32it/s] 98%|█████████▊| 228/232 [02:52<00:03,  1.32it/s] 99%|█████████▊| 229/232 [02:53<00:02,  1.32it/s] 99%|█████████▉| 230/232 [02:54<00:01,  1.32it/s]100%|█████████▉| 231/232 [02:54<00:00,  1.32it/s]100%|██████████| 232/232 [02:55<00:00,  1.53it/s]/home/mitryand/.local/lib/python3.10/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
loss: 0.5840947031974792
tensor([[  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   314,   373,   523,  6568,    13,   314],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   886,   286,   262],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   412,    18, 32330],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,   284],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   314,   373,   523,  6568,    13,   314],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   886,   286,   262],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   412,    18, 32330],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  3776, 34152,  8785],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,  1517,   314,   750,   373,   284,
          1011,   257,   804,   379,   262,  1353,   286,   262,  2443,   290],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,   284],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  3776, 19485,   287],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,   284],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13]],
       device='cuda:0')
Traceback (most recent call last):
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 167, in <module>
    main(params)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 148, in main
    model = train(model, train_dataloader, eval_dataloader, params)
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 113, in train
    metric.add_batch(predictions=outputs.long(), references=batch[1].long())
  File "/home/mitryand/.local/lib/python3.10/site-packages/evaluate/module.py", line 512, in add_batch
    raise ValueError(error_msg) from None
ValueError: Predictions and/or references don't match the expected format.
Expected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},
Input predictions: tensor([[  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   314,   373,   523,  6568,    13,   314],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   886,   286,   262],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   412,    18, 32330],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  9714,   304, 29917],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,   284],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   314,   373,   523,  6568,    13,   314],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,  2008,
            11,   314,   373,   523,  6568,    13,   314,   373,   523,  6568],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   886,   286,   262],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,   412,    18, 32330],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   287,   262,  1903,  1528,   286],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  3776, 34152,  8785],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13],
        [  101,   198,   198,   464,   717,  1517,   314,   750,   373,   284,
          1011,   257,   804,   379,   262,  1353,   286,   262,  2443,   290],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,   284],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,   373,   379,   262,  3776, 19485,   287],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,   284],
        [  101,   198,   198,   464,   717,   640,   314,  2497,   262,   649,
          2196,   286,   262,   983,    11,   314,   373,   523,  6568,    13]],
       device='cuda:0'),
Input references: tensor([[ 7003,   968, 12255,  ..., 50256, 50256, 50256],
        [    8, 18348,  2178,  ..., 50256, 50256, 50256],
        [ 1544, 13488,   287,  ..., 50256, 50256, 50256],
        ...,
        [   11,  5544,   287,  ..., 50256, 50256, 50256],
        [   44, 19231,    78,  ..., 50256, 50256, 50256],
        [ 1544, 17293,   837,  ..., 50256, 50256, 50256]], device='cuda:0')
100%|██████████| 232/232 [02:56<00:00,  1.32it/s]
