Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loaded 128 training samples
loaded 29 validation samples
loaded 4 test samples
EncoderDecoderModel(
  (encoder): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(28996, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (decoder): BertLMHeadModel(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (crossattention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (cls): BertOnlyMLMHead(
      (predictions): BertLMPredictionHead(
        (transform): BertPredictionHeadTransform(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (transform_act_fn): GELUActivation()
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (decoder): Linear(in_features=768, out_features=28996, bias=True)
      )
    )
  )
)
GenreClassifier(
  (lin1): Linear(in_features=76800, out_features=256, bias=True)
  (relu1): LeakyReLU(negative_slope=0.01)
  (lin2): Linear(in_features=256, out_features=50, bias=True)
  (relu2): LeakyReLU(negative_slope=0.01)
  (lin3): Linear(in_features=50, out_features=2, bias=True)
)
Begin training all of them!
  0%|          | 0/4096 [00:00<?, ?it/s]/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
  0%|          | 1/4096 [00:01<1:16:44,  1.12s/it]  0%|          | 2/4096 [00:01<49:30,  1.38it/s]    0%|          | 3/4096 [00:01<40:10,  1.70it/s]  0%|          | 4/4096 [00:02<35:51,  1.90it/s]  0%|          | 5/4096 [00:02<33:25,  2.04it/s]  0%|          | 6/4096 [00:03<31:58,  2.13it/s]  0%|          | 7/4096 [00:03<31:13,  2.18it/s]  0%|          | 8/4096 [00:04<30:30,  2.23it/s]  0%|          | 9/4096 [00:04<30:07,  2.26it/s]  0%|          | 10/4096 [00:05<29:49,  2.28it/s]  0%|          | 11/4096 [00:05<29:38,  2.30it/s]  0%|          | 12/4096 [00:05<29:28,  2.31it/s]  0%|          | 13/4096 [00:06<29:23,  2.32it/s]  0%|          | 14/4096 [00:06<29:18,  2.32it/s]  0%|          | 15/4096 [00:07<29:13,  2.33it/s]  0%|          | 16/4096 [00:07<29:12,  2.33it/s]  0%|          | 17/4096 [00:08<29:13,  2.33it/s]  0%|          | 18/4096 [00:08<29:14,  2.32it/s]  0%|          | 19/4096 [00:08<29:13,  2.32it/s]  0%|          | 20/4096 [00:09<29:11,  2.33it/s]  1%|          | 21/4096 [00:09<29:08,  2.33it/s]  1%|          | 22/4096 [00:10<29:07,  2.33it/s]  1%|          | 23/4096 [00:10<29:07,  2.33it/s]  1%|          | 24/4096 [00:11<29:07,  2.33it/s]  1%|          | 25/4096 [00:11<29:05,  2.33it/s]  1%|          | 26/4096 [00:11<29:05,  2.33it/s]  1%|          | 27/4096 [00:12<29:03,  2.33it/s]  1%|          | 28/4096 [00:12<29:02,  2.33it/s]  1%|          | 29/4096 [00:13<29:05,  2.33it/s]  1%|          | 30/4096 [00:13<29:04,  2.33it/s]  1%|          | 31/4096 [00:14<29:02,  2.33it/s]  1%|          | 32/4096 [00:14<29:03,  2.33it/s]  1%|          | 33/4096 [00:14<29:05,  2.33it/s]  1%|          | 34/4096 [00:15<29:05,  2.33it/s]  1%|          | 35/4096 [00:15<29:02,  2.33it/s]  1%|          | 36/4096 [00:16<29:00,  2.33it/s]  1%|          | 37/4096 [00:16<28:58,  2.33it/s]  1%|          | 38/4096 [00:17<28:55,  2.34it/s]  1%|          | 39/4096 [00:17<28:56,  2.34it/s]  1%|          | 40/4096 [00:17<28:57,  2.33it/s]  1%|          | 41/4096 [00:18<28:57,  2.33it/s]  1%|          | 42/4096 [00:18<28:56,  2.33it/s]  1%|          | 43/4096 [00:19<28:58,  2.33it/s]  1%|          | 44/4096 [00:19<29:00,  2.33it/s]  1%|          | 45/4096 [00:20<28:58,  2.33it/s]  1%|          | 46/4096 [00:20<28:55,  2.33it/s]  1%|          | 47/4096 [00:20<28:55,  2.33it/s]  1%|          | 48/4096 [00:21<28:56,  2.33it/s]  1%|          | 49/4096 [00:21<28:57,  2.33it/s]  1%|          | 50/4096 [00:22<28:57,  2.33it/s]  1%|          | 51/4096 [00:22<28:56,  2.33it/s]  1%|▏         | 52/4096 [00:23<28:55,  2.33it/s]  1%|▏         | 53/4096 [00:23<28:54,  2.33it/s]  1%|▏         | 54/4096 [00:23<28:54,  2.33it/s]  1%|▏         | 55/4096 [00:24<28:54,  2.33it/s]  1%|▏         | 56/4096 [00:24<28:53,  2.33it/s]  1%|▏         | 57/4096 [00:25<28:54,  2.33it/s]  1%|▏         | 58/4096 [00:25<28:53,  2.33it/s]  1%|▏         | 59/4096 [00:26<28:53,  2.33it/s]  1%|▏         | 60/4096 [00:26<28:52,  2.33it/s]  1%|▏         | 61/4096 [00:26<28:53,  2.33it/s]  2%|▏         | 62/4096 [00:27<28:53,  2.33it/s]  2%|▏         | 63/4096 [00:27<28:53,  2.33it/s]  2%|▏         | 64/4096 [00:28<28:53,  2.33it/s]  2%|▏         | 65/4096 [00:28<28:50,  2.33it/s]  2%|▏         | 66/4096 [00:29<28:49,  2.33it/s]  2%|▏         | 67/4096 [00:29<28:47,  2.33it/s]  2%|▏         | 68/4096 [00:29<28:49,  2.33it/s]  2%|▏         | 69/4096 [00:30<28:50,  2.33it/s]  2%|▏         | 70/4096 [00:30<28:50,  2.33it/s]  2%|▏         | 71/4096 [00:31<28:51,  2.33it/s]  2%|▏         | 72/4096 [00:31<28:50,  2.32it/s]  2%|▏         | 73/4096 [00:32<28:50,  2.32it/s]  2%|▏         | 74/4096 [00:32<28:50,  2.32it/s]  2%|▏         | 75/4096 [00:32<28:49,  2.32it/s]  2%|▏         | 76/4096 [00:33<28:49,  2.32it/s]  2%|▏         | 77/4096 [00:33<28:49,  2.32it/s]  2%|▏         | 78/4096 [00:34<28:49,  2.32it/s]  2%|▏         | 79/4096 [00:34<28:48,  2.32it/s]  2%|▏         | 80/4096 [00:35<28:47,  2.33it/s]  2%|▏         | 81/4096 [00:35<28:48,  2.32it/s]  2%|▏         | 82/4096 [00:35<28:46,  2.32it/s]  2%|▏         | 83/4096 [00:36<28:44,  2.33it/s]  2%|▏         | 84/4096 [00:36<28:47,  2.32it/s]  2%|▏         | 85/4096 [00:37<28:47,  2.32it/s]  2%|▏         | 86/4096 [00:37<28:47,  2.32it/s]  2%|▏         | 87/4096 [00:38<28:47,  2.32it/s]  2%|▏         | 88/4096 [00:38<28:44,  2.32it/s]  2%|▏         | 89/4096 [00:38<28:42,  2.33it/s]  2%|▏         | 90/4096 [00:39<28:42,  2.33it/s]  2%|▏         | 91/4096 [00:39<28:42,  2.33it/s]  2%|▏         | 92/4096 [00:40<28:43,  2.32it/s]  2%|▏         | 93/4096 [00:40<28:43,  2.32it/s]  2%|▏         | 94/4096 [00:41<28:42,  2.32it/s]  2%|▏         | 95/4096 [00:41<28:41,  2.32it/s]  2%|▏         | 96/4096 [00:41<28:48,  2.31it/s]  2%|▏         | 97/4096 [00:42<28:45,  2.32it/s]  2%|▏         | 98/4096 [00:42<28:47,  2.31it/s]  2%|▏         | 99/4096 [00:43<28:46,  2.32it/s]  2%|▏         | 100/4096 [00:43<28:44,  2.32it/s]  2%|▏         | 101/4096 [00:44<28:44,  2.32it/s]  2%|▏         | 102/4096 [00:44<28:43,  2.32it/s]  3%|▎         | 103/4096 [00:44<28:42,  2.32it/s]  3%|▎         | 104/4096 [00:45<28:43,  2.32it/s]  3%|▎         | 105/4096 [00:45<28:44,  2.31it/s]  3%|▎         | 106/4096 [00:46<28:44,  2.31it/s]  3%|▎         | 107/4096 [00:46<28:40,  2.32it/s]  3%|▎         | 108/4096 [00:47<28:37,  2.32it/s]  3%|▎         | 109/4096 [00:47<28:35,  2.32it/s]  3%|▎         | 110/4096 [00:47<28:38,  2.32it/s]  3%|▎         | 111/4096 [00:48<28:37,  2.32it/s]  3%|▎         | 112/4096 [00:48<28:39,  2.32it/s]  3%|▎         | 113/4096 [00:49<28:39,  2.32it/s]  3%|▎         | 114/4096 [00:49<28:39,  2.32it/s]  3%|▎         | 115/4096 [00:50<28:36,  2.32it/s]  3%|▎         | 116/4096 [00:50<28:36,  2.32it/s]  3%|▎         | 117/4096 [00:51<28:36,  2.32it/s]  3%|▎         | 118/4096 [00:51<28:36,  2.32it/s]  3%|▎         | 119/4096 [00:51<28:33,  2.32it/s]  3%|▎         | 120/4096 [00:52<28:33,  2.32it/s]  3%|▎         | 121/4096 [00:52<28:31,  2.32it/s]  3%|▎         | 122/4096 [00:53<28:31,  2.32it/s]  3%|▎         | 123/4096 [00:53<28:31,  2.32it/s]  3%|▎         | 124/4096 [00:54<28:30,  2.32it/s]  3%|▎         | 125/4096 [00:54<28:29,  2.32it/s]  3%|▎         | 126/4096 [00:54<28:30,  2.32it/s]  3%|▎         | 127/4096 [00:55<28:31,  2.32it/s]  3%|▎         | 128/4096 [00:55<25:44,  2.57it/s]loss: 5.355125904083252
===========================
epoch 1/32 | loss: 5.355125904083252
---------------------------
example true genres: 
tensor([0, 0, 1, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.7155361050328227
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["That place is crawling with Bill Doolin and his gang''. Even as he spoke those words Billy Tilghman's life hung on a thread. Back in the house a hoodlum named Red Buck, sore because Billy had been allowed to leave unscathed, jumped from a bunk and swore he was going after him to kill him right then.", "You did this you like to hurt to beat people I want to go home''. These were the last words he ever uttered. Convulsively, he spat up some blood and collapsed into the arms of Senator Gaston Berche, crimsoning the frilly shirt and waistcoat the politician wore.", 'Fair enough. What are the relevant data?? For every person on Taiwan, there are sixty in Mainland China.', 'Then there was no saying how many times the marine had blown his nose on the handkerchief. Too bad the marine had no water. From its holder he took his own canteen.', 'His years of campaigning had taught him the value of water discipline. He began to uncap the bottle, the rusty cap squealing on its threads. Popping upright, the marine waved both hands and shouted.']
---------------------------
example output paragraph: 
["..... the the... and..''.. the the. the the..... '.. the the a.... the. a.. a... the.. a a the to the the. the the,. the a... a he was the the. to......", ",,..... to. the the... to to to''''. the, the he the the....ly, he the.,. and he. the. of the...., the the the the the the the and the the the the the..", ',,.,.. and the the.... the...,.........', ',,,, the no the the the the the the and the the the the the the. the the the the the the the.. the. the and the.....', '..... the and a the the of and the... the. the. the., the,... the the the the.. the the, the........']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
  7%|▋         | 2/29 [00:00<00:01, 17.54it/s][A
 31%|███       | 9/29 [00:00<00:00, 45.06it/s][A
 55%|█████▌    | 16/29 [00:00<00:00, 54.38it/s][A
 79%|███████▉  | 23/29 [00:00<00:00, 58.85it/s][A100%|██████████| 29/29 [00:00<00:00, 55.43it/s]
Mean Perplexity: 274.80405892219795
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
  3%|▎         | 129/4096 [01:13<6:04:58,  5.52s/it]  3%|▎         | 130/4096 [01:13<4:24:17,  4.00s/it]  3%|▎         | 131/4096 [01:13<3:13:32,  2.93s/it]  3%|▎         | 132/4096 [01:14<2:23:56,  2.18s/it]  3%|▎         | 133/4096 [01:14<1:49:14,  1.65s/it]  3%|▎         | 134/4096 [01:15<1:24:59,  1.29s/it]  3%|▎         | 135/4096 [01:15<1:07:55,  1.03s/it]  3%|▎         | 136/4096 [01:16<56:03,  1.18it/s]    3%|▎         | 137/4096 [01:16<47:46,  1.38it/s]  3%|▎         | 138/4096 [01:16<41:56,  1.57it/s]  3%|▎         | 139/4096 [01:17<37:52,  1.74it/s]  3%|▎         | 140/4096 [01:17<35:02,  1.88it/s]  3%|▎         | 141/4096 [01:18<33:06,  1.99it/s]  3%|▎         | 142/4096 [01:18<31:41,  2.08it/s]  3%|▎         | 143/4096 [01:19<30:44,  2.14it/s]  4%|▎         | 144/4096 [01:19<30:03,  2.19it/s]  4%|▎         | 145/4096 [01:20<29:34,  2.23it/s]  4%|▎         | 146/4096 [01:20<29:13,  2.25it/s]  4%|▎         | 147/4096 [01:20<28:56,  2.27it/s]  4%|▎         | 148/4096 [01:21<28:44,  2.29it/s]  4%|▎         | 149/4096 [01:21<28:37,  2.30it/s]  4%|▎         | 150/4096 [01:22<28:31,  2.31it/s]  4%|▎         | 151/4096 [01:22<28:27,  2.31it/s]  4%|▎         | 152/4096 [01:23<28:23,  2.32it/s]  4%|▎         | 153/4096 [01:23<28:21,  2.32it/s]  4%|▍         | 154/4096 [01:23<28:20,  2.32it/s]  4%|▍         | 155/4096 [01:24<28:19,  2.32it/s]  4%|▍         | 156/4096 [01:24<28:21,  2.32it/s]  4%|▍         | 157/4096 [01:25<28:18,  2.32it/s]  4%|▍         | 158/4096 [01:25<28:17,  2.32it/s]  4%|▍         | 159/4096 [01:26<28:18,  2.32it/s]  4%|▍         | 160/4096 [01:26<28:16,  2.32it/s]  4%|▍         | 161/4096 [01:26<28:17,  2.32it/s]  4%|▍         | 162/4096 [01:27<28:16,  2.32it/s]  4%|▍         | 163/4096 [01:27<28:17,  2.32it/s]  4%|▍         | 164/4096 [01:28<28:16,  2.32it/s]  4%|▍         | 165/4096 [01:28<28:15,  2.32it/s]  4%|▍         | 166/4096 [01:29<28:13,  2.32it/s]  4%|▍         | 167/4096 [01:29<28:12,  2.32it/s]  4%|▍         | 168/4096 [01:29<28:12,  2.32it/s]  4%|▍         | 169/4096 [01:30<28:11,  2.32it/s]  4%|▍         | 170/4096 [01:30<28:11,  2.32it/s]  4%|▍         | 171/4096 [01:31<28:11,  2.32it/s]  4%|▍         | 172/4096 [01:31<28:10,  2.32it/s]  4%|▍         | 173/4096 [01:32<28:08,  2.32it/s]  4%|▍         | 174/4096 [01:32<28:09,  2.32it/s]  4%|▍         | 175/4096 [01:32<28:09,  2.32it/s]  4%|▍         | 176/4096 [01:33<28:09,  2.32it/s]  4%|▍         | 177/4096 [01:33<28:09,  2.32it/s]  4%|▍         | 178/4096 [01:34<28:10,  2.32it/s]  4%|▍         | 179/4096 [01:34<28:11,  2.32it/s]  4%|▍         | 180/4096 [01:35<28:10,  2.32it/s]  4%|▍         | 181/4096 [01:35<28:11,  2.31it/s]  4%|▍         | 182/4096 [01:35<28:49,  2.26it/s]  4%|▍         | 183/4096 [01:36<28:39,  2.28it/s]  4%|▍         | 184/4096 [01:36<28:33,  2.28it/s]  5%|▍         | 185/4096 [01:37<28:24,  2.29it/s]  5%|▍         | 186/4096 [01:37<28:21,  2.30it/s]  5%|▍         | 187/4096 [01:38<28:17,  2.30it/s]  5%|▍         | 188/4096 [01:38<28:14,  2.31it/s]  5%|▍         | 189/4096 [01:39<28:11,  2.31it/s]  5%|▍         | 190/4096 [01:39<28:09,  2.31it/s]  5%|▍         | 191/4096 [01:39<28:08,  2.31it/s]  5%|▍         | 192/4096 [01:40<28:09,  2.31it/s]  5%|▍         | 193/4096 [01:40<28:09,  2.31it/s]  5%|▍         | 194/4096 [01:41<28:09,  2.31it/s]  5%|▍         | 195/4096 [01:41<28:07,  2.31it/s]  5%|▍         | 196/4096 [01:42<28:06,  2.31it/s]  5%|▍         | 197/4096 [01:42<28:08,  2.31it/s]  5%|▍         | 198/4096 [01:42<28:10,  2.31it/s]  5%|▍         | 199/4096 [01:43<28:08,  2.31it/s]  5%|▍         | 200/4096 [01:43<28:10,  2.30it/s]  5%|▍         | 201/4096 [01:44<28:07,  2.31it/s]  5%|▍         | 202/4096 [01:44<28:07,  2.31it/s]  5%|▍         | 203/4096 [01:45<28:06,  2.31it/s]  5%|▍         | 204/4096 [01:45<28:05,  2.31it/s]  5%|▌         | 205/4096 [01:45<28:06,  2.31it/s]  5%|▌         | 206/4096 [01:46<28:15,  2.29it/s]  5%|▌         | 207/4096 [01:46<28:14,  2.30it/s]  5%|▌         | 208/4096 [01:47<28:12,  2.30it/s]  5%|▌         | 209/4096 [01:47<28:07,  2.30it/s]  5%|▌         | 210/4096 [01:48<28:04,  2.31it/s]  5%|▌         | 211/4096 [01:48<28:02,  2.31it/s]  5%|▌         | 212/4096 [01:48<28:02,  2.31it/s]  5%|▌         | 213/4096 [01:49<28:02,  2.31it/s]  5%|▌         | 214/4096 [01:49<28:02,  2.31it/s]  5%|▌         | 215/4096 [01:50<28:00,  2.31it/s]  5%|▌         | 216/4096 [01:50<27:59,  2.31it/s]  5%|▌         | 217/4096 [01:51<27:58,  2.31it/s]  5%|▌         | 218/4096 [01:51<27:58,  2.31it/s]  5%|▌         | 219/4096 [01:52<27:59,  2.31it/s]  5%|▌         | 220/4096 [01:52<28:00,  2.31it/s]  5%|▌         | 221/4096 [01:52<27:59,  2.31it/s]  5%|▌         | 222/4096 [01:53<27:59,  2.31it/s]  5%|▌         | 223/4096 [01:53<27:55,  2.31it/s]  5%|▌         | 224/4096 [01:54<27:55,  2.31it/s]  5%|▌         | 225/4096 [01:54<27:55,  2.31it/s]  6%|▌         | 226/4096 [01:55<27:55,  2.31it/s]  6%|▌         | 227/4096 [01:55<27:53,  2.31it/s]  6%|▌         | 228/4096 [01:55<27:53,  2.31it/s]  6%|▌         | 229/4096 [01:56<27:51,  2.31it/s]  6%|▌         | 230/4096 [01:56<27:51,  2.31it/s]  6%|▌         | 231/4096 [01:57<27:50,  2.31it/s]  6%|▌         | 232/4096 [01:57<27:48,  2.32it/s]  6%|▌         | 233/4096 [01:58<27:50,  2.31it/s]  6%|▌         | 234/4096 [01:58<27:51,  2.31it/s]  6%|▌         | 235/4096 [01:58<27:51,  2.31it/s]  6%|▌         | 236/4096 [01:59<27:51,  2.31it/s]  6%|▌         | 237/4096 [01:59<27:49,  2.31it/s]  6%|▌         | 238/4096 [02:00<27:49,  2.31it/s]  6%|▌         | 239/4096 [02:00<27:50,  2.31it/s]  6%|▌         | 240/4096 [02:01<27:49,  2.31it/s]  6%|▌         | 241/4096 [02:01<27:50,  2.31it/s]  6%|▌         | 242/4096 [02:01<27:49,  2.31it/s]  6%|▌         | 243/4096 [02:02<27:48,  2.31it/s]  6%|▌         | 244/4096 [02:02<27:46,  2.31it/s]  6%|▌         | 245/4096 [02:03<27:48,  2.31it/s]  6%|▌         | 246/4096 [02:03<27:49,  2.31it/s]  6%|▌         | 247/4096 [02:04<27:49,  2.31it/s]  6%|▌         | 248/4096 [02:04<27:50,  2.30it/s]  6%|▌         | 249/4096 [02:05<27:49,  2.30it/s]  6%|▌         | 250/4096 [02:05<27:48,  2.30it/s]  6%|▌         | 251/4096 [02:05<27:48,  2.30it/s]  6%|▌         | 252/4096 [02:06<27:46,  2.31it/s]  6%|▌         | 253/4096 [02:06<27:41,  2.31it/s]  6%|▌         | 254/4096 [02:07<27:40,  2.31it/s]  6%|▌         | 255/4096 [02:07<27:39,  2.31it/s]  6%|▋         | 256/4096 [02:07<22:47,  2.81it/s]loss: 3.263772964477539
===========================
epoch 2/32 | loss: 3.263772964477539
---------------------------
example true genres: 
tensor([1, 0, 1, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 1, 0, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.849015317286652
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["and, though he repeated, over and over again, the spectacular figures of industrial and agricultural production in 1980, the ` ` ordinary'' people in Russia are still a little uncertain as to how ` ` communism'' is really going to work in practice, especially in respect of food. Would agriculture progress as rapidly as industry?? This was something on which K. himself seemed to have some doubts ; ;", "` ` Aristide!! I want you to find Monsieur Prieur at once and give him this money for the boy's purchase. There's $ 600 in gold in this chamois sack.", "A brief for the negativeI disagree with Mr. Burnham's position on the Common Market ( Nov. 18 ) as a desirable organization for us to join. For him to ignore the political consequences involved in an Atlantic Union of this kind is difficult to understand.", 'He smiled. ( He always smiles - - at least at visitors, I gather. He smiled also at a British bloke seated next to me, who asked the most asinine questions.', "One less shouldn't matter to him''. Aristide Devol, the sardonic manservant who had been brought in chains years before from his native Sierra Leone, smiled thinly and touched his well - brushed beaver hat. His bold eyes raked the woman, and a perceptive spectator might sense that there was more to their relationship than that of slave to owner."]
---------------------------
example output paragraph: 
["..., the was the, the the the, and,, of of the and and and the the, the,,,'''the the to to'''to to the,'''''''be the the the, the the the the '.'and and to the a,.?'was a the the '.. to to have the,...", "......... I. to be the...,. the to to to.. for.,'s '.'s s '.....''. '..", "..... the, I to the....'' to on the........ a '. for the be have.. to to the the. of the the the..... to the have the..", "..'I He'was was. - - the the the., I,. was., the the,,. of,, to me, who the,, the the, the..", ".. ', the'' the the.''''.,.., the.t,... was been been to to to, before from....,..ly, the hand. -. the the,.. was,..., and a., '. to the the was was more the the. than that of. the the.."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.63it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.78it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.05it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.18it/s][A100%|██████████| 29/29 [00:00<00:00, 66.16it/s]
Mean Perplexity: 427.3436130047888
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
  6%|▋         | 257/4096 [02:16<3:08:23,  2.94s/it]  6%|▋         | 258/4096 [02:17<2:20:09,  2.19s/it]  6%|▋         | 259/4096 [02:17<1:46:23,  1.66s/it]  6%|▋         | 260/4096 [02:18<1:22:48,  1.30s/it]  6%|▋         | 261/4096 [02:18<1:06:09,  1.04s/it]  6%|▋         | 262/4096 [02:18<54:36,  1.17it/s]    6%|▋         | 263/4096 [02:19<46:29,  1.37it/s]  6%|▋         | 264/4096 [02:19<40:50,  1.56it/s]  6%|▋         | 265/4096 [02:20<36:52,  1.73it/s]  6%|▋         | 266/4096 [02:20<34:04,  1.87it/s]  7%|▋         | 267/4096 [02:21<32:06,  1.99it/s]  7%|▋         | 268/4096 [02:21<30:44,  2.07it/s]  7%|▋         | 269/4096 [02:21<29:46,  2.14it/s]  7%|▋         | 270/4096 [02:22<29:05,  2.19it/s]  7%|▋         | 271/4096 [02:22<28:38,  2.23it/s]  7%|▋         | 272/4096 [02:23<28:20,  2.25it/s]  7%|▋         | 273/4096 [02:23<28:07,  2.27it/s]  7%|▋         | 274/4096 [02:24<27:59,  2.28it/s]  7%|▋         | 275/4096 [02:24<27:51,  2.29it/s]  7%|▋         | 276/4096 [02:24<27:48,  2.29it/s]  7%|▋         | 277/4096 [02:25<27:46,  2.29it/s]  7%|▋         | 278/4096 [02:25<27:41,  2.30it/s]  7%|▋         | 279/4096 [02:26<27:40,  2.30it/s]  7%|▋         | 280/4096 [02:26<27:39,  2.30it/s]  7%|▋         | 281/4096 [02:27<27:35,  2.30it/s]  7%|▋         | 282/4096 [02:27<27:32,  2.31it/s]  7%|▋         | 283/4096 [02:28<27:30,  2.31it/s]  7%|▋         | 284/4096 [02:28<27:32,  2.31it/s]  7%|▋         | 285/4096 [02:28<27:32,  2.31it/s]  7%|▋         | 286/4096 [02:29<27:34,  2.30it/s]  7%|▋         | 287/4096 [02:29<27:35,  2.30it/s]  7%|▋         | 288/4096 [02:30<27:33,  2.30it/s]  7%|▋         | 289/4096 [02:30<27:31,  2.30it/s]  7%|▋         | 290/4096 [02:31<27:32,  2.30it/s]  7%|▋         | 291/4096 [02:31<27:32,  2.30it/s]  7%|▋         | 292/4096 [02:31<27:31,  2.30it/s]  7%|▋         | 293/4096 [02:32<27:32,  2.30it/s]  7%|▋         | 294/4096 [02:32<27:30,  2.30it/s]  7%|▋         | 295/4096 [02:33<27:28,  2.31it/s]  7%|▋         | 296/4096 [02:33<27:26,  2.31it/s]  7%|▋         | 297/4096 [02:34<27:24,  2.31it/s]  7%|▋         | 298/4096 [02:34<27:24,  2.31it/s]  7%|▋         | 299/4096 [02:34<27:25,  2.31it/s]  7%|▋         | 300/4096 [02:35<27:23,  2.31it/s]  7%|▋         | 301/4096 [02:35<27:24,  2.31it/s]  7%|▋         | 302/4096 [02:36<27:25,  2.31it/s]  7%|▋         | 303/4096 [02:36<27:24,  2.31it/s]  7%|▋         | 304/4096 [02:37<27:22,  2.31it/s]  7%|▋         | 305/4096 [02:37<27:43,  2.28it/s]  7%|▋         | 306/4096 [02:38<27:35,  2.29it/s]  7%|▋         | 307/4096 [02:38<27:30,  2.30it/s]  8%|▊         | 308/4096 [02:38<27:32,  2.29it/s]  8%|▊         | 309/4096 [02:39<27:29,  2.30it/s]  8%|▊         | 310/4096 [02:39<27:25,  2.30it/s]  8%|▊         | 311/4096 [02:40<27:21,  2.31it/s]  8%|▊         | 312/4096 [02:40<27:17,  2.31it/s]  8%|▊         | 313/4096 [02:41<27:18,  2.31it/s]  8%|▊         | 314/4096 [02:41<27:17,  2.31it/s]  8%|▊         | 315/4096 [02:41<27:17,  2.31it/s]  8%|▊         | 316/4096 [02:42<27:17,  2.31it/s]  8%|▊         | 317/4096 [02:42<27:17,  2.31it/s]  8%|▊         | 318/4096 [02:43<27:15,  2.31it/s]  8%|▊         | 319/4096 [02:43<27:16,  2.31it/s]  8%|▊         | 320/4096 [02:44<27:14,  2.31it/s]  8%|▊         | 321/4096 [02:44<27:16,  2.31it/s]  8%|▊         | 322/4096 [02:44<27:15,  2.31it/s]  8%|▊         | 323/4096 [02:45<27:15,  2.31it/s]  8%|▊         | 324/4096 [02:45<27:14,  2.31it/s]  8%|▊         | 325/4096 [02:46<27:12,  2.31it/s]  8%|▊         | 326/4096 [02:46<27:11,  2.31it/s]  8%|▊         | 327/4096 [02:47<27:10,  2.31it/s]  8%|▊         | 328/4096 [02:47<27:09,  2.31it/s]  8%|▊         | 329/4096 [02:47<27:11,  2.31it/s]  8%|▊         | 330/4096 [02:48<27:10,  2.31it/s]  8%|▊         | 331/4096 [02:48<27:08,  2.31it/s]  8%|▊         | 332/4096 [02:49<27:07,  2.31it/s]  8%|▊         | 333/4096 [02:49<27:06,  2.31it/s]  8%|▊         | 334/4096 [02:50<27:09,  2.31it/s]  8%|▊         | 335/4096 [02:50<27:10,  2.31it/s]  8%|▊         | 336/4096 [02:51<27:08,  2.31it/s]  8%|▊         | 337/4096 [02:51<27:08,  2.31it/s]  8%|▊         | 338/4096 [02:51<27:07,  2.31it/s]  8%|▊         | 339/4096 [02:52<27:06,  2.31it/s]  8%|▊         | 340/4096 [02:52<27:06,  2.31it/s]  8%|▊         | 341/4096 [02:53<27:05,  2.31it/s]  8%|▊         | 342/4096 [02:53<27:06,  2.31it/s]  8%|▊         | 343/4096 [02:54<27:06,  2.31it/s]  8%|▊         | 344/4096 [02:54<27:06,  2.31it/s]  8%|▊         | 345/4096 [02:54<27:06,  2.31it/s]  8%|▊         | 346/4096 [02:55<27:04,  2.31it/s]  8%|▊         | 347/4096 [02:55<27:01,  2.31it/s]  8%|▊         | 348/4096 [02:56<27:02,  2.31it/s]  9%|▊         | 349/4096 [02:56<27:03,  2.31it/s]  9%|▊         | 350/4096 [02:57<27:01,  2.31it/s]  9%|▊         | 351/4096 [02:57<27:02,  2.31it/s]  9%|▊         | 352/4096 [02:57<27:03,  2.31it/s]  9%|▊         | 353/4096 [02:58<27:01,  2.31it/s]  9%|▊         | 354/4096 [02:58<27:01,  2.31it/s]  9%|▊         | 355/4096 [02:59<27:00,  2.31it/s]  9%|▊         | 356/4096 [02:59<26:59,  2.31it/s]  9%|▊         | 357/4096 [03:00<26:57,  2.31it/s]  9%|▊         | 358/4096 [03:00<26:56,  2.31it/s]  9%|▉         | 359/4096 [03:00<26:57,  2.31it/s]  9%|▉         | 360/4096 [03:01<27:00,  2.31it/s]  9%|▉         | 361/4096 [03:01<26:59,  2.31it/s]  9%|▉         | 362/4096 [03:02<27:01,  2.30it/s]  9%|▉         | 363/4096 [03:02<27:03,  2.30it/s]  9%|▉         | 364/4096 [03:03<27:02,  2.30it/s]  9%|▉         | 365/4096 [03:03<27:00,  2.30it/s]  9%|▉         | 366/4096 [03:04<26:57,  2.31it/s]  9%|▉         | 367/4096 [03:04<26:56,  2.31it/s]  9%|▉         | 368/4096 [03:04<26:58,  2.30it/s]  9%|▉         | 369/4096 [03:05<26:58,  2.30it/s]  9%|▉         | 370/4096 [03:05<26:56,  2.30it/s]  9%|▉         | 371/4096 [03:06<26:54,  2.31it/s]  9%|▉         | 372/4096 [03:06<26:52,  2.31it/s]  9%|▉         | 373/4096 [03:07<26:52,  2.31it/s]  9%|▉         | 374/4096 [03:07<26:51,  2.31it/s]  9%|▉         | 375/4096 [03:07<26:50,  2.31it/s]  9%|▉         | 376/4096 [03:08<28:32,  2.17it/s]  9%|▉         | 377/4096 [03:08<28:04,  2.21it/s]  9%|▉         | 378/4096 [03:09<27:41,  2.24it/s]  9%|▉         | 379/4096 [03:09<27:27,  2.26it/s]  9%|▉         | 380/4096 [03:10<27:15,  2.27it/s]  9%|▉         | 381/4096 [03:10<27:04,  2.29it/s]  9%|▉         | 382/4096 [03:11<26:59,  2.29it/s]  9%|▉         | 383/4096 [03:11<26:55,  2.30it/s]  9%|▉         | 384/4096 [03:11<22:10,  2.79it/s]loss: 4.155083656311035
===========================
epoch 3/32 | loss: 4.155083656311035
---------------------------
example true genres: 
tensor([0, 0, 0, 0, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 0, 0, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9037199124726477
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["Strange. At last he reached for the knife. Even the bone handle scorched, and he retrieved the marine's handkerchief to wrap it.", "Come inside now''. The children grudgingly stopped playing then and straggled into the schoolhouse. Jack watched Miss Langford all morning.", "Waddell asked, frowning. ` ` Please let me explain'', the German said earnestly, his face still devoid of deceit. ` ` I have in Europe a gross business of seven million dollars the year.", "` ` It was my fault''. With one hand she held her skirt down while she took Jack's extended hand with the other. When her hand touched his, fire went through Jack and he felt weak, but he managed somehow to get her on her feet.", 'Red China is trying to do this, and she is not likely ever to succeed. Tibet is too vast, the terrain is too difficult. Tibet may bleed China as Algeria is bleeding France.']
---------------------------
example output paragraph: 
['He the the was the the man. He he man of,terhing the the the was the man. s head. the the. He', ", the,'' I man ofentlyly,,, thehingtingly the house,, He was the.'' was, He `", "to and, I `, you,, '. I man man,lyly and voice,,, theclamationce. ` am been a. little. of the hundred dollars dollars time of", "He'a.,'' I the,, held the the. the the was the.'hand.. the pistol hand He he she was was. he. and the. he was the. but not was, to the the.... He", 'the the be the. the the was a the to to be to The is a much. and country, a much. The is be be. much. a.. The']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.45it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.61it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.92it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.13it/s][A100%|██████████| 29/29 [00:00<00:00, 66.11it/s]
Mean Perplexity: 664.2688296378572
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
  9%|▉         | 385/4096 [03:19<2:44:29,  2.66s/it]  9%|▉         | 386/4096 [03:20<2:03:05,  1.99s/it]  9%|▉         | 387/4096 [03:20<1:34:11,  1.52s/it]  9%|▉         | 388/4096 [03:20<1:14:08,  1.20s/it]  9%|▉         | 389/4096 [03:21<59:53,  1.03it/s]   10%|▉         | 390/4096 [03:21<49:55,  1.24it/s] 10%|▉         | 391/4096 [03:22<42:55,  1.44it/s] 10%|▉         | 392/4096 [03:22<38:03,  1.62it/s] 10%|▉         | 393/4096 [03:23<34:38,  1.78it/s] 10%|▉         | 394/4096 [03:23<33:06,  1.86it/s] 10%|▉         | 395/4096 [03:24<31:08,  1.98it/s] 10%|▉         | 396/4096 [03:24<29:46,  2.07it/s] 10%|▉         | 397/4096 [03:24<28:49,  2.14it/s] 10%|▉         | 398/4096 [03:25<28:09,  2.19it/s] 10%|▉         | 399/4096 [03:25<27:42,  2.22it/s] 10%|▉         | 400/4096 [03:26<27:22,  2.25it/s] 10%|▉         | 401/4096 [03:26<27:07,  2.27it/s] 10%|▉         | 402/4096 [03:27<26:57,  2.28it/s] 10%|▉         | 403/4096 [03:27<26:51,  2.29it/s] 10%|▉         | 404/4096 [03:27<26:46,  2.30it/s] 10%|▉         | 405/4096 [03:28<26:42,  2.30it/s] 10%|▉         | 406/4096 [03:28<26:40,  2.31it/s] 10%|▉         | 407/4096 [03:29<26:37,  2.31it/s] 10%|▉         | 408/4096 [03:29<26:35,  2.31it/s] 10%|▉         | 409/4096 [03:30<26:34,  2.31it/s] 10%|█         | 410/4096 [03:30<26:34,  2.31it/s] 10%|█         | 411/4096 [03:30<26:32,  2.31it/s] 10%|█         | 412/4096 [03:31<26:32,  2.31it/s] 10%|█         | 413/4096 [03:31<26:36,  2.31it/s] 10%|█         | 414/4096 [03:32<26:36,  2.31it/s] 10%|█         | 415/4096 [03:32<26:36,  2.31it/s] 10%|█         | 416/4096 [03:33<26:35,  2.31it/s] 10%|█         | 417/4096 [03:33<26:34,  2.31it/s] 10%|█         | 418/4096 [03:34<26:33,  2.31it/s] 10%|█         | 419/4096 [03:34<26:31,  2.31it/s] 10%|█         | 420/4096 [03:34<26:32,  2.31it/s] 10%|█         | 421/4096 [03:35<26:30,  2.31it/s] 10%|█         | 422/4096 [03:35<26:28,  2.31it/s] 10%|█         | 423/4096 [03:36<26:27,  2.31it/s] 10%|█         | 424/4096 [03:36<26:29,  2.31it/s] 10%|█         | 425/4096 [03:37<26:30,  2.31it/s] 10%|█         | 426/4096 [03:37<26:31,  2.31it/s] 10%|█         | 427/4096 [03:37<26:32,  2.30it/s] 10%|█         | 428/4096 [03:38<26:38,  2.29it/s] 10%|█         | 429/4096 [03:38<26:35,  2.30it/s] 10%|█         | 430/4096 [03:39<26:33,  2.30it/s] 11%|█         | 431/4096 [03:39<26:31,  2.30it/s] 11%|█         | 432/4096 [03:40<26:30,  2.30it/s] 11%|█         | 433/4096 [03:40<26:27,  2.31it/s] 11%|█         | 434/4096 [03:40<26:28,  2.31it/s] 11%|█         | 435/4096 [03:41<26:25,  2.31it/s] 11%|█         | 436/4096 [03:41<26:26,  2.31it/s] 11%|█         | 437/4096 [03:42<26:24,  2.31it/s] 11%|█         | 438/4096 [03:42<26:22,  2.31it/s] 11%|█         | 439/4096 [03:43<26:24,  2.31it/s] 11%|█         | 440/4096 [03:43<26:22,  2.31it/s] 11%|█         | 441/4096 [03:43<26:22,  2.31it/s] 11%|█         | 442/4096 [03:44<26:23,  2.31it/s] 11%|█         | 443/4096 [03:44<26:22,  2.31it/s] 11%|█         | 444/4096 [03:45<26:22,  2.31it/s] 11%|█         | 445/4096 [03:45<26:25,  2.30it/s] 11%|█         | 446/4096 [03:46<26:22,  2.31it/s] 11%|█         | 447/4096 [03:46<26:21,  2.31it/s] 11%|█         | 448/4096 [03:47<26:22,  2.30it/s] 11%|█         | 449/4096 [03:47<26:21,  2.31it/s] 11%|█         | 450/4096 [03:47<26:22,  2.30it/s] 11%|█         | 451/4096 [03:48<26:20,  2.31it/s] 11%|█         | 452/4096 [03:48<26:20,  2.31it/s] 11%|█         | 453/4096 [03:49<26:17,  2.31it/s] 11%|█         | 454/4096 [03:49<26:18,  2.31it/s] 11%|█         | 455/4096 [03:50<26:20,  2.30it/s] 11%|█         | 456/4096 [03:50<26:16,  2.31it/s] 11%|█         | 457/4096 [03:50<26:17,  2.31it/s] 11%|█         | 458/4096 [03:51<26:17,  2.31it/s] 11%|█         | 459/4096 [03:51<26:15,  2.31it/s] 11%|█         | 460/4096 [03:52<26:18,  2.30it/s] 11%|█▏        | 461/4096 [03:52<26:18,  2.30it/s] 11%|█▏        | 462/4096 [03:53<26:18,  2.30it/s] 11%|█▏        | 463/4096 [03:53<26:13,  2.31it/s] 11%|█▏        | 464/4096 [03:53<26:14,  2.31it/s] 11%|█▏        | 465/4096 [03:54<26:13,  2.31it/s] 11%|█▏        | 466/4096 [03:54<26:14,  2.31it/s] 11%|█▏        | 467/4096 [03:55<26:13,  2.31it/s] 11%|█▏        | 468/4096 [03:55<26:13,  2.31it/s] 11%|█▏        | 469/4096 [03:56<26:12,  2.31it/s] 11%|█▏        | 470/4096 [03:56<26:11,  2.31it/s] 11%|█▏        | 471/4096 [03:56<26:11,  2.31it/s] 12%|█▏        | 472/4096 [03:57<26:09,  2.31it/s] 12%|█▏        | 473/4096 [03:57<26:08,  2.31it/s] 12%|█▏        | 474/4096 [03:58<26:10,  2.31it/s] 12%|█▏        | 475/4096 [03:58<26:10,  2.31it/s] 12%|█▏        | 476/4096 [03:59<26:09,  2.31it/s] 12%|█▏        | 477/4096 [03:59<26:10,  2.30it/s] 12%|█▏        | 478/4096 [04:00<26:09,  2.30it/s] 12%|█▏        | 479/4096 [04:00<26:08,  2.31it/s] 12%|█▏        | 480/4096 [04:00<26:08,  2.31it/s] 12%|█▏        | 481/4096 [04:01<26:09,  2.30it/s] 12%|█▏        | 482/4096 [04:01<26:07,  2.31it/s] 12%|█▏        | 483/4096 [04:02<26:07,  2.31it/s] 12%|█▏        | 484/4096 [04:02<26:08,  2.30it/s] 12%|█▏        | 485/4096 [04:03<26:08,  2.30it/s] 12%|█▏        | 486/4096 [04:03<26:05,  2.31it/s] 12%|█▏        | 487/4096 [04:03<26:08,  2.30it/s] 12%|█▏        | 488/4096 [04:04<26:08,  2.30it/s] 12%|█▏        | 489/4096 [04:04<26:05,  2.30it/s] 12%|█▏        | 490/4096 [04:05<26:03,  2.31it/s] 12%|█▏        | 491/4096 [04:05<26:03,  2.31it/s] 12%|█▏        | 492/4096 [04:06<26:02,  2.31it/s] 12%|█▏        | 493/4096 [04:06<26:01,  2.31it/s] 12%|█▏        | 494/4096 [04:06<26:00,  2.31it/s] 12%|█▏        | 495/4096 [04:07<25:58,  2.31it/s] 12%|█▏        | 496/4096 [04:07<25:58,  2.31it/s] 12%|█▏        | 497/4096 [04:08<25:58,  2.31it/s] 12%|█▏        | 498/4096 [04:08<25:58,  2.31it/s] 12%|█▏        | 499/4096 [04:09<25:57,  2.31it/s] 12%|█▏        | 500/4096 [04:09<25:58,  2.31it/s] 12%|█▏        | 501/4096 [04:09<25:57,  2.31it/s] 12%|█▏        | 502/4096 [04:10<25:54,  2.31it/s] 12%|█▏        | 503/4096 [04:10<25:54,  2.31it/s] 12%|█▏        | 504/4096 [04:11<25:55,  2.31it/s] 12%|█▏        | 505/4096 [04:11<25:55,  2.31it/s] 12%|█▏        | 506/4096 [04:12<25:55,  2.31it/s] 12%|█▏        | 507/4096 [04:12<25:54,  2.31it/s] 12%|█▏        | 508/4096 [04:13<25:53,  2.31it/s] 12%|█▏        | 509/4096 [04:13<25:54,  2.31it/s] 12%|█▏        | 510/4096 [04:13<25:52,  2.31it/s] 12%|█▏        | 511/4096 [04:14<25:52,  2.31it/s] 12%|█▎        | 512/4096 [04:14<21:16,  2.81it/s]loss: 3.05416202545166
===========================
epoch 4/32 | loss: 3.05416202545166
---------------------------
example true genres: 
tensor([1, 1, 1, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 1, 1, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9234135667396062
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["ConfrontationIt seems to me that N. C., in his editorial ` ` Confrontation'' ( SR, Mar. 25 ), has hit upon the real problem that bothers all of us in a complex world : how do we retain our personal relationship with those who suffer?? This affects us all intimately, and can leave us hopeless in the face of widespread distress.", 'It reappears, in whole or part, whenever a new crisis exposes the reality : in Cuba last spring ( with which the Dominican events of last month should be paired ) ; ; at the peaks of the nuclear test and the Berlin cycles ; ; in relation to Laos, Algeria, South Africa ; ;', "When a dancer does well, she provokes a quiet bombardment of dollar bills - - although the Manhattan clubs prohibit the more cosmopolitan practice of slipping the tips into the dancers'costumes. With tips, the girls average between $ 150 and $ 200 a week, depending on basic salary. Although they are forbidden to sit with the customers, the dancers are sometimes proffered drinks, and most of them can bolt one down in mid - shimmy.", "Any posse riding down the street to demand Blue Throat's surrender would be wiped out with one deadly burst of fire. The law - abiding citizens of Petrie had gathered inside Kaster's Store, halfway down the street. Several were firing into the barn when Billy Tilghman arrived.", "One by one he tossed the objects aside. He didn't smoke and could not light fires with a flintless lighter ; ; he had no use any longer for exact time, even had the watch been running."]
---------------------------
example output paragraph: 
["of ` ` to be know the'' C. I the opinion, ` `traration of s. (.ner.,., been the the head man of is tos the the. the sense, of I he he have a own,, the people are '?? is the..,ly and and be it.. the world of the religion.", 'wascted., he the the the the the the man new,s the same of the the, war, the the the new Republic, the war, be the with. ; ; the first of the war.. the war -, ; ; the to the, the, the Korea Africa ; ;', "the man was a to but wasvenlyly little, of the - and - - the man,'the city -ons - of the the city of the city, s. the, he city'were the $, the $,., and on the conditions, the are a to pay in the men who and city'a amos,. and and of the, be., the the - -tiesties,", "##ed, the the horse, the the,racireed s s to be a by of the of, of the. man of was manly the had theti, been in thema's s. and the the street. days out on the house. he had'' was,.", "the man had the man of the He had't know, the see be the. the fireinging.. he he was a way of of. the.. he the a time. a."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.66it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.00it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.10it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.26it/s][A100%|██████████| 29/29 [00:00<00:00, 66.27it/s]
Mean Perplexity: 768.7697614507028
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 13%|█▎        | 513/4096 [04:21<2:17:17,  2.30s/it] 13%|█▎        | 514/4096 [04:21<1:43:48,  1.74s/it] 13%|█▎        | 515/4096 [04:22<1:20:26,  1.35s/it] 13%|█▎        | 516/4096 [04:22<1:04:24,  1.08s/it] 13%|█▎        | 517/4096 [04:23<52:48,  1.13it/s]   13%|█▎        | 518/4096 [04:23<44:43,  1.33it/s] 13%|█▎        | 519/4096 [04:23<39:03,  1.53it/s] 13%|█▎        | 520/4096 [04:24<35:03,  1.70it/s] 13%|█▎        | 521/4096 [04:24<32:15,  1.85it/s] 13%|█▎        | 522/4096 [04:25<30:18,  1.96it/s] 13%|█▎        | 523/4096 [04:25<28:56,  2.06it/s] 13%|█▎        | 524/4096 [04:26<27:59,  2.13it/s] 13%|█▎        | 525/4096 [04:26<27:20,  2.18it/s] 13%|█▎        | 526/4096 [04:26<26:50,  2.22it/s] 13%|█▎        | 527/4096 [04:27<26:32,  2.24it/s] 13%|█▎        | 528/4096 [04:27<26:16,  2.26it/s] 13%|█▎        | 529/4096 [04:28<26:05,  2.28it/s] 13%|█▎        | 530/4096 [04:28<26:00,  2.29it/s] 13%|█▎        | 531/4096 [04:29<25:53,  2.30it/s] 13%|█▎        | 532/4096 [04:29<25:47,  2.30it/s] 13%|█▎        | 533/4096 [04:30<25:45,  2.31it/s] 13%|█▎        | 534/4096 [04:30<25:43,  2.31it/s] 13%|█▎        | 535/4096 [04:30<25:44,  2.31it/s] 13%|█▎        | 536/4096 [04:31<25:42,  2.31it/s] 13%|█▎        | 537/4096 [04:31<25:40,  2.31it/s] 13%|█▎        | 538/4096 [04:32<25:41,  2.31it/s] 13%|█▎        | 539/4096 [04:32<25:42,  2.31it/s] 13%|█▎        | 540/4096 [04:33<25:41,  2.31it/s] 13%|█▎        | 541/4096 [04:33<25:40,  2.31it/s] 13%|█▎        | 542/4096 [04:33<25:41,  2.31it/s] 13%|█▎        | 543/4096 [04:34<25:43,  2.30it/s] 13%|█▎        | 544/4096 [04:34<25:45,  2.30it/s] 13%|█▎        | 545/4096 [04:35<25:43,  2.30it/s] 13%|█▎        | 546/4096 [04:35<25:42,  2.30it/s] 13%|█▎        | 547/4096 [04:36<25:39,  2.31it/s] 13%|█▎        | 548/4096 [04:36<25:43,  2.30it/s] 13%|█▎        | 549/4096 [04:36<25:41,  2.30it/s] 13%|█▎        | 550/4096 [04:37<25:39,  2.30it/s] 13%|█▎        | 551/4096 [04:37<25:42,  2.30it/s] 13%|█▎        | 552/4096 [04:38<25:40,  2.30it/s] 14%|█▎        | 553/4096 [04:38<25:36,  2.31it/s] 14%|█▎        | 554/4096 [04:39<25:34,  2.31it/s] 14%|█▎        | 555/4096 [04:39<25:34,  2.31it/s] 14%|█▎        | 556/4096 [04:39<25:33,  2.31it/s] 14%|█▎        | 557/4096 [04:40<25:34,  2.31it/s] 14%|█▎        | 558/4096 [04:40<25:34,  2.31it/s] 14%|█▎        | 559/4096 [04:41<25:34,  2.31it/s] 14%|█▎        | 560/4096 [04:41<25:32,  2.31it/s] 14%|█▎        | 561/4096 [04:42<25:28,  2.31it/s] 14%|█▎        | 562/4096 [04:42<25:30,  2.31it/s] 14%|█▎        | 563/4096 [04:43<25:30,  2.31it/s] 14%|█▍        | 564/4096 [04:43<25:31,  2.31it/s] 14%|█▍        | 565/4096 [04:43<25:30,  2.31it/s] 14%|█▍        | 566/4096 [04:44<25:29,  2.31it/s] 14%|█▍        | 567/4096 [04:44<25:27,  2.31it/s] 14%|█▍        | 568/4096 [04:45<25:26,  2.31it/s] 14%|█▍        | 569/4096 [04:45<25:24,  2.31it/s] 14%|█▍        | 570/4096 [04:46<25:24,  2.31it/s] 14%|█▍        | 571/4096 [04:46<25:22,  2.32it/s] 14%|█▍        | 572/4096 [04:46<25:22,  2.32it/s] 14%|█▍        | 573/4096 [04:47<25:22,  2.31it/s] 14%|█▍        | 574/4096 [04:47<25:21,  2.31it/s] 14%|█▍        | 575/4096 [04:48<25:21,  2.31it/s] 14%|█▍        | 576/4096 [04:48<25:21,  2.31it/s] 14%|█▍        | 577/4096 [04:49<25:22,  2.31it/s] 14%|█▍        | 578/4096 [04:49<25:23,  2.31it/s] 14%|█▍        | 579/4096 [04:49<25:23,  2.31it/s] 14%|█▍        | 580/4096 [04:50<25:22,  2.31it/s] 14%|█▍        | 581/4096 [04:50<25:22,  2.31it/s] 14%|█▍        | 582/4096 [04:51<25:21,  2.31it/s] 14%|█▍        | 583/4096 [04:51<25:20,  2.31it/s] 14%|█▍        | 584/4096 [04:52<25:22,  2.31it/s] 14%|█▍        | 585/4096 [04:52<25:22,  2.31it/s] 14%|█▍        | 586/4096 [04:52<25:22,  2.31it/s] 14%|█▍        | 587/4096 [04:53<25:21,  2.31it/s] 14%|█▍        | 588/4096 [04:53<25:19,  2.31it/s] 14%|█▍        | 589/4096 [04:54<25:19,  2.31it/s] 14%|█▍        | 590/4096 [04:54<25:18,  2.31it/s] 14%|█▍        | 591/4096 [04:55<25:17,  2.31it/s] 14%|█▍        | 592/4096 [04:55<25:14,  2.31it/s] 14%|█▍        | 593/4096 [04:55<25:14,  2.31it/s] 15%|█▍        | 594/4096 [04:56<25:13,  2.31it/s] 15%|█▍        | 595/4096 [04:56<25:13,  2.31it/s] 15%|█▍        | 596/4096 [04:57<25:14,  2.31it/s] 15%|█▍        | 597/4096 [04:57<25:14,  2.31it/s] 15%|█▍        | 598/4096 [04:58<25:13,  2.31it/s] 15%|█▍        | 599/4096 [04:58<25:12,  2.31it/s] 15%|█▍        | 600/4096 [04:59<25:11,  2.31it/s] 15%|█▍        | 601/4096 [04:59<25:12,  2.31it/s] 15%|█▍        | 602/4096 [04:59<25:13,  2.31it/s] 15%|█▍        | 603/4096 [05:00<25:11,  2.31it/s] 15%|█▍        | 604/4096 [05:00<25:12,  2.31it/s] 15%|█▍        | 605/4096 [05:01<25:12,  2.31it/s] 15%|█▍        | 606/4096 [05:01<25:13,  2.31it/s] 15%|█▍        | 607/4096 [05:02<25:15,  2.30it/s] 15%|█▍        | 608/4096 [05:02<25:15,  2.30it/s] 15%|█▍        | 609/4096 [05:02<25:15,  2.30it/s] 15%|█▍        | 610/4096 [05:03<25:13,  2.30it/s] 15%|█▍        | 611/4096 [05:03<25:11,  2.31it/s] 15%|█▍        | 612/4096 [05:04<25:11,  2.31it/s] 15%|█▍        | 613/4096 [05:04<25:11,  2.30it/s] 15%|█▍        | 614/4096 [05:05<25:10,  2.30it/s] 15%|█▌        | 615/4096 [05:05<25:09,  2.31it/s] 15%|█▌        | 616/4096 [05:05<25:08,  2.31it/s] 15%|█▌        | 617/4096 [05:06<25:07,  2.31it/s] 15%|█▌        | 618/4096 [05:06<25:09,  2.30it/s] 15%|█▌        | 619/4096 [05:07<25:07,  2.31it/s] 15%|█▌        | 620/4096 [05:07<25:06,  2.31it/s] 15%|█▌        | 621/4096 [05:08<25:06,  2.31it/s] 15%|█▌        | 622/4096 [05:08<25:06,  2.31it/s] 15%|█▌        | 623/4096 [05:09<25:05,  2.31it/s] 15%|█▌        | 624/4096 [05:09<25:07,  2.30it/s] 15%|█▌        | 625/4096 [05:09<25:06,  2.30it/s] 15%|█▌        | 626/4096 [05:10<25:06,  2.30it/s] 15%|█▌        | 627/4096 [05:10<25:05,  2.30it/s] 15%|█▌        | 628/4096 [05:11<25:04,  2.30it/s] 15%|█▌        | 629/4096 [05:11<26:51,  2.15it/s] 15%|█▌        | 630/4096 [05:12<26:19,  2.19it/s] 15%|█▌        | 631/4096 [05:12<25:55,  2.23it/s] 15%|█▌        | 632/4096 [05:13<25:39,  2.25it/s] 15%|█▌        | 633/4096 [05:13<25:27,  2.27it/s] 15%|█▌        | 634/4096 [05:13<25:19,  2.28it/s] 16%|█▌        | 635/4096 [05:14<25:12,  2.29it/s] 16%|█▌        | 636/4096 [05:14<25:07,  2.30it/s] 16%|█▌        | 637/4096 [05:15<25:02,  2.30it/s] 16%|█▌        | 638/4096 [05:15<25:00,  2.30it/s] 16%|█▌        | 639/4096 [05:16<24:58,  2.31it/s] 16%|█▌        | 640/4096 [05:16<20:33,  2.80it/s]loss: 2.4214982986450195
===========================
epoch 5/32 | loss: 2.4214982986450195
---------------------------
example true genres: 
tensor([0, 1, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9321663019693655
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["` ` What is your name, boy?? Come a bit closer. I won't bite, you know''.", "The objective should be to provide a method of getting into print a higher percentage than is now possible of the relevant information in the possession of reporters and editors. Southern California blackoutI would like to see you devote some space in an early issue to the news blackout concerning President Kennedy's activities, so far as Southern California is concerned.", "His head snapped round and he reeled back, crashing into the table where his buddies were sprawling. Tilghman leapt on to him, dragged him upright and hit him again, this time sending him careening against the bar. A bullet gouged into the bar top an inch from Tilghman's stomach as Blue Throat's henchmen started shooting.", "He jumped back, ducked and ran, crouching, down the hill away from the school. He didn't look back and he ran until he was out of sight of the schoolhouse and out of breath ; ; then he slowed to a walk.", 'A stringed orchestra played softly behind the potted palms, and Delphine circulated graciously among her guests, chatting airily of the forthcoming races, the latest fashions from Paris, and Louisiana politics. Suddenly there was a commotion upstairs, a despairing boyish shriek, and the strains of the waltz faltered and died as the musicians and guests gaped at an apparition descending the marble staircase. It was Dandy Brandon, clad only in a bloody loin']
---------------------------
example output paragraph: 
["` ` you you job, I,??, little,. `'' t get you but ',?.", 'of be not the the good of the the the. good quality of the the,. the first material. the public of the. the of The Southern California, of ` be to have the the the of to the editorial economy of the public of press. the. Kennedy s s. and the as the,, not that', "eyes was back the the turnededed back and into the wall. he headleded were in on Herac 'ey, from the the. and the back. he the.. and time time him backfully the wall. hard shotedd the his back.. arm. the..ey, s head. the.... s head wased. to up", "was, and and, went, androuching, and the ground.. the car. He was't see at at he saw to he he sure of the. the boy.. the of the., was, the little.", "few was a, in, the wall,,, and theete was theiltly, the eyes. and with,, the night night, and men,s, the. and and,, The, was a man,, and small of a,,tagouding and and sun of the nightdinginged, the, the wind, the were in the enormousrehensiontion.. sound,. was a ',, and in the a dim of.."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.36it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.45it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.70it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.75it/s][A100%|██████████| 29/29 [00:00<00:00, 65.74it/s]
Mean Perplexity: 767.0602042534158
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 16%|█▌        | 641/4096 [05:24<2:32:59,  2.66s/it] 16%|█▌        | 642/4096 [05:24<1:54:30,  1.99s/it] 16%|█▌        | 643/4096 [05:25<1:27:36,  1.52s/it] 16%|█▌        | 644/4096 [05:25<1:08:46,  1.20s/it] 16%|█▌        | 645/4096 [05:25<55:36,  1.03it/s]   16%|█▌        | 646/4096 [05:26<46:21,  1.24it/s] 16%|█▌        | 647/4096 [05:26<39:55,  1.44it/s] 16%|█▌        | 648/4096 [05:27<35:25,  1.62it/s] 16%|█▌        | 649/4096 [05:27<32:14,  1.78it/s] 16%|█▌        | 650/4096 [05:28<30:01,  1.91it/s] 16%|█▌        | 651/4096 [05:28<28:26,  2.02it/s] 16%|█▌        | 652/4096 [05:29<27:21,  2.10it/s] 16%|█▌        | 653/4096 [05:29<26:35,  2.16it/s] 16%|█▌        | 654/4096 [05:29<26:05,  2.20it/s] 16%|█▌        | 655/4096 [05:30<25:44,  2.23it/s] 16%|█▌        | 656/4096 [05:30<25:25,  2.25it/s] 16%|█▌        | 657/4096 [05:31<25:15,  2.27it/s] 16%|█▌        | 658/4096 [05:31<25:05,  2.28it/s] 16%|█▌        | 659/4096 [05:32<24:58,  2.29it/s] 16%|█▌        | 660/4096 [05:32<24:54,  2.30it/s] 16%|█▌        | 661/4096 [05:32<24:51,  2.30it/s] 16%|█▌        | 662/4096 [05:33<24:49,  2.30it/s] 16%|█▌        | 663/4096 [05:33<24:46,  2.31it/s] 16%|█▌        | 664/4096 [05:34<24:45,  2.31it/s] 16%|█▌        | 665/4096 [05:34<24:47,  2.31it/s] 16%|█▋        | 666/4096 [05:35<24:44,  2.31it/s] 16%|█▋        | 667/4096 [05:35<24:41,  2.31it/s] 16%|█▋        | 668/4096 [05:35<24:42,  2.31it/s] 16%|█▋        | 669/4096 [05:36<24:41,  2.31it/s] 16%|█▋        | 670/4096 [05:36<24:41,  2.31it/s] 16%|█▋        | 671/4096 [05:37<24:41,  2.31it/s] 16%|█▋        | 672/4096 [05:37<24:40,  2.31it/s] 16%|█▋        | 673/4096 [05:38<24:40,  2.31it/s] 16%|█▋        | 674/4096 [05:38<24:40,  2.31it/s] 16%|█▋        | 675/4096 [05:38<24:39,  2.31it/s] 17%|█▋        | 676/4096 [05:39<24:40,  2.31it/s] 17%|█▋        | 677/4096 [05:39<24:40,  2.31it/s] 17%|█▋        | 678/4096 [05:40<24:41,  2.31it/s] 17%|█▋        | 679/4096 [05:40<24:41,  2.31it/s] 17%|█▋        | 680/4096 [05:41<24:41,  2.31it/s] 17%|█▋        | 681/4096 [05:41<24:41,  2.31it/s] 17%|█▋        | 682/4096 [05:41<24:41,  2.30it/s] 17%|█▋        | 683/4096 [05:42<25:14,  2.25it/s] 17%|█▋        | 684/4096 [05:42<25:03,  2.27it/s] 17%|█▋        | 685/4096 [05:43<24:56,  2.28it/s] 17%|█▋        | 686/4096 [05:43<24:52,  2.28it/s] 17%|█▋        | 687/4096 [05:44<24:47,  2.29it/s] 17%|█▋        | 688/4096 [05:44<24:43,  2.30it/s] 17%|█▋        | 689/4096 [05:45<24:39,  2.30it/s] 17%|█▋        | 690/4096 [05:45<24:37,  2.31it/s] 17%|█▋        | 691/4096 [05:45<24:37,  2.31it/s] 17%|█▋        | 692/4096 [05:46<24:35,  2.31it/s] 17%|█▋        | 693/4096 [05:46<24:35,  2.31it/s] 17%|█▋        | 694/4096 [05:47<24:35,  2.31it/s] 17%|█▋        | 695/4096 [05:47<24:35,  2.30it/s] 17%|█▋        | 696/4096 [05:48<24:35,  2.30it/s] 17%|█▋        | 697/4096 [05:48<24:34,  2.30it/s] 17%|█▋        | 698/4096 [05:48<24:35,  2.30it/s] 17%|█▋        | 699/4096 [05:49<24:37,  2.30it/s] 17%|█▋        | 700/4096 [05:49<24:35,  2.30it/s] 17%|█▋        | 701/4096 [05:50<24:34,  2.30it/s] 17%|█▋        | 702/4096 [05:50<24:33,  2.30it/s] 17%|█▋        | 703/4096 [05:51<24:31,  2.31it/s] 17%|█▋        | 704/4096 [05:51<24:30,  2.31it/s] 17%|█▋        | 705/4096 [05:52<24:31,  2.30it/s] 17%|█▋        | 706/4096 [05:52<24:30,  2.30it/s] 17%|█▋        | 707/4096 [05:52<24:31,  2.30it/s] 17%|█▋        | 708/4096 [05:53<24:32,  2.30it/s] 17%|█▋        | 709/4096 [05:53<24:31,  2.30it/s] 17%|█▋        | 710/4096 [05:54<24:30,  2.30it/s] 17%|█▋        | 711/4096 [05:54<24:29,  2.30it/s] 17%|█▋        | 712/4096 [05:55<24:26,  2.31it/s] 17%|█▋        | 713/4096 [05:55<24:24,  2.31it/s] 17%|█▋        | 714/4096 [05:55<24:23,  2.31it/s] 17%|█▋        | 715/4096 [05:56<24:23,  2.31it/s] 17%|█▋        | 716/4096 [05:56<24:24,  2.31it/s] 18%|█▊        | 717/4096 [05:57<24:23,  2.31it/s] 18%|█▊        | 718/4096 [05:57<24:23,  2.31it/s] 18%|█▊        | 719/4096 [05:58<24:22,  2.31it/s] 18%|█▊        | 720/4096 [05:58<24:22,  2.31it/s] 18%|█▊        | 721/4096 [05:58<24:23,  2.31it/s] 18%|█▊        | 722/4096 [05:59<24:20,  2.31it/s] 18%|█▊        | 723/4096 [05:59<24:22,  2.31it/s] 18%|█▊        | 724/4096 [06:00<24:22,  2.31it/s] 18%|█▊        | 725/4096 [06:00<24:20,  2.31it/s] 18%|█▊        | 726/4096 [06:01<24:20,  2.31it/s] 18%|█▊        | 727/4096 [06:01<24:20,  2.31it/s] 18%|█▊        | 728/4096 [06:01<24:20,  2.31it/s] 18%|█▊        | 729/4096 [06:02<24:18,  2.31it/s] 18%|█▊        | 730/4096 [06:02<24:21,  2.30it/s] 18%|█▊        | 731/4096 [06:03<24:20,  2.30it/s] 18%|█▊        | 732/4096 [06:03<24:20,  2.30it/s] 18%|█▊        | 733/4096 [06:04<24:18,  2.31it/s] 18%|█▊        | 734/4096 [06:04<24:17,  2.31it/s] 18%|█▊        | 735/4096 [06:05<24:17,  2.31it/s] 18%|█▊        | 736/4096 [06:05<24:18,  2.30it/s] 18%|█▊        | 737/4096 [06:05<24:17,  2.30it/s] 18%|█▊        | 738/4096 [06:06<24:15,  2.31it/s] 18%|█▊        | 739/4096 [06:06<24:13,  2.31it/s] 18%|█▊        | 740/4096 [06:07<24:12,  2.31it/s] 18%|█▊        | 741/4096 [06:07<24:13,  2.31it/s] 18%|█▊        | 742/4096 [06:08<24:13,  2.31it/s] 18%|█▊        | 743/4096 [06:08<24:12,  2.31it/s] 18%|█▊        | 744/4096 [06:08<24:12,  2.31it/s] 18%|█▊        | 745/4096 [06:09<24:13,  2.31it/s] 18%|█▊        | 746/4096 [06:09<24:13,  2.30it/s] 18%|█▊        | 747/4096 [06:10<24:12,  2.31it/s] 18%|█▊        | 748/4096 [06:10<24:11,  2.31it/s] 18%|█▊        | 749/4096 [06:11<24:13,  2.30it/s] 18%|█▊        | 750/4096 [06:11<24:11,  2.30it/s] 18%|█▊        | 751/4096 [06:11<24:10,  2.31it/s] 18%|█▊        | 752/4096 [06:12<24:09,  2.31it/s] 18%|█▊        | 753/4096 [06:12<24:09,  2.31it/s] 18%|█▊        | 754/4096 [06:13<24:09,  2.31it/s] 18%|█▊        | 755/4096 [06:13<24:10,  2.30it/s] 18%|█▊        | 756/4096 [06:14<24:09,  2.30it/s] 18%|█▊        | 757/4096 [06:14<24:09,  2.30it/s] 19%|█▊        | 758/4096 [06:14<24:08,  2.30it/s] 19%|█▊        | 759/4096 [06:15<24:07,  2.31it/s] 19%|█▊        | 760/4096 [06:15<24:05,  2.31it/s] 19%|█▊        | 761/4096 [06:16<24:05,  2.31it/s] 19%|█▊        | 762/4096 [06:16<24:07,  2.30it/s] 19%|█▊        | 763/4096 [06:17<24:06,  2.30it/s] 19%|█▊        | 764/4096 [06:17<24:06,  2.30it/s] 19%|█▊        | 765/4096 [06:18<24:04,  2.31it/s] 19%|█▊        | 766/4096 [06:18<24:04,  2.31it/s] 19%|█▊        | 767/4096 [06:18<24:04,  2.30it/s] 19%|█▉        | 768/4096 [06:19<19:51,  2.79it/s]loss: 2.8446550369262695
===========================
epoch 6/32 | loss: 2.8446550369262695
---------------------------
example true genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9343544857768052
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["` ` Hell, I jist got on top of - -'' ` ` No, I mean how'd you get her to do it''?? ` ` Hell, I jist ask her''.", "Once again life went its serene way - - soirees, fox hunts, balls and dinners. The excitement over Brandon's bizarre death abated and Madame Lalaurie's stock soared when she resumed her self - imposed chores of visiting the poor and bringing cakes and comfort to destitute patients in the county hospital. Then, on July 2, there occurred another incident which set tongues to wagging at a furious clip.", 'Along each side of the room were six tiered bunks, each one screened off with a curtain. And projecting wickedly through these curtains were the gleaming muzzles of six rifles, all trained on Billy Tilghman. The fighting marshal had walked right into a trap and at any moment six slugs might slam into his hide.', 'The marine was a winehead. His superiors had said that all marines were depraved. The marine slumped forward into a bow like a priest before an idol.', "Within seconds the big barn was blasted into smoking splinters, with every outlaw either dead or injured inside. It was the abrupt end of Blue Throat's dictatorship in Petrie. Though only slightly injured himself the big hoodlum never returned to those parts."]
---------------------------
example output paragraph: 
["` ` `'I'''to a of the - - '? ` `'I'' I'you'to'do the. '.? ` ` `'I 'nk'you? '? `", 'the, was to wayrely, - and muchful, andess, and, as, The men of the was s s,,pod the and,e, was s s ofy, it was the work - -,ores. the the country, elderly them and as theimteble,. the city of., the the,, and was a murder of was the of thegging of the few night of', "the other of the house, a menssonss, and of of, the the large, The the aly, the wall, a first,s. the men. and were by the '.uskey, men was was been out into the corner, fired the moment. -hacks. be them the back.", 'man man a man.. The eyes were been that he thes were avovo. mans in. the thick. a barrel. he enemy.', "the, man man was a up the firehacklopers. and a sound of of,, a in the The was a first of of the,ranock, s s. theron'the the the the, man man was was had to the who of"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.39it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.55it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.04it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.99it/s][A100%|██████████| 29/29 [00:00<00:00, 66.21it/s]
Mean Perplexity: 886.3719631670862
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 19%|█▉        | 769/4096 [06:25<2:05:36,  2.27s/it] 19%|█▉        | 770/4096 [06:26<1:35:14,  1.72s/it] 19%|█▉        | 771/4096 [06:26<1:13:50,  1.33s/it] 19%|█▉        | 772/4096 [06:27<58:52,  1.06s/it]   19%|█▉        | 773/4096 [06:27<48:23,  1.14it/s] 19%|█▉        | 774/4096 [06:27<41:03,  1.35it/s] 19%|█▉        | 775/4096 [06:28<35:56,  1.54it/s] 19%|█▉        | 776/4096 [06:28<32:20,  1.71it/s] 19%|█▉        | 777/4096 [06:29<29:50,  1.85it/s] 19%|█▉        | 778/4096 [06:29<28:05,  1.97it/s] 19%|█▉        | 779/4096 [06:30<26:52,  2.06it/s] 19%|█▉        | 780/4096 [06:30<26:00,  2.13it/s] 19%|█▉        | 781/4096 [06:30<25:22,  2.18it/s] 19%|█▉        | 782/4096 [06:31<24:54,  2.22it/s] 19%|█▉        | 783/4096 [06:31<24:35,  2.24it/s] 19%|█▉        | 784/4096 [06:32<24:23,  2.26it/s] 19%|█▉        | 785/4096 [06:32<24:14,  2.28it/s] 19%|█▉        | 786/4096 [06:33<24:08,  2.29it/s] 19%|█▉        | 787/4096 [06:33<24:04,  2.29it/s] 19%|█▉        | 788/4096 [06:34<24:01,  2.29it/s] 19%|█▉        | 789/4096 [06:34<23:59,  2.30it/s] 19%|█▉        | 790/4096 [06:34<23:56,  2.30it/s] 19%|█▉        | 791/4096 [06:35<23:57,  2.30it/s] 19%|█▉        | 792/4096 [06:35<23:54,  2.30it/s] 19%|█▉        | 793/4096 [06:36<23:53,  2.30it/s] 19%|█▉        | 794/4096 [06:36<23:50,  2.31it/s] 19%|█▉        | 795/4096 [06:37<23:48,  2.31it/s] 19%|█▉        | 796/4096 [06:37<23:48,  2.31it/s] 19%|█▉        | 797/4096 [06:37<23:48,  2.31it/s] 19%|█▉        | 798/4096 [06:38<23:47,  2.31it/s] 20%|█▉        | 799/4096 [06:38<23:46,  2.31it/s] 20%|█▉        | 800/4096 [06:39<23:46,  2.31it/s] 20%|█▉        | 801/4096 [06:39<23:46,  2.31it/s] 20%|█▉        | 802/4096 [06:40<23:47,  2.31it/s] 20%|█▉        | 803/4096 [06:40<23:48,  2.31it/s] 20%|█▉        | 804/4096 [06:40<23:47,  2.31it/s] 20%|█▉        | 805/4096 [06:41<23:49,  2.30it/s] 20%|█▉        | 806/4096 [06:41<23:49,  2.30it/s] 20%|█▉        | 807/4096 [06:42<23:48,  2.30it/s] 20%|█▉        | 808/4096 [06:42<23:46,  2.30it/s] 20%|█▉        | 809/4096 [06:43<23:46,  2.30it/s] 20%|█▉        | 810/4096 [06:43<23:45,  2.31it/s] 20%|█▉        | 811/4096 [06:43<23:46,  2.30it/s] 20%|█▉        | 812/4096 [06:44<23:46,  2.30it/s] 20%|█▉        | 813/4096 [06:44<23:47,  2.30it/s] 20%|█▉        | 814/4096 [06:45<23:44,  2.30it/s] 20%|█▉        | 815/4096 [06:45<23:42,  2.31it/s] 20%|█▉        | 816/4096 [06:46<23:42,  2.31it/s] 20%|█▉        | 817/4096 [06:46<23:43,  2.30it/s] 20%|█▉        | 818/4096 [06:47<23:43,  2.30it/s] 20%|█▉        | 819/4096 [06:47<23:40,  2.31it/s] 20%|██        | 820/4096 [06:47<23:41,  2.31it/s] 20%|██        | 821/4096 [06:48<23:39,  2.31it/s] 20%|██        | 822/4096 [06:48<23:40,  2.31it/s] 20%|██        | 823/4096 [06:49<23:40,  2.30it/s] 20%|██        | 824/4096 [06:49<23:40,  2.30it/s] 20%|██        | 825/4096 [06:50<23:41,  2.30it/s] 20%|██        | 826/4096 [06:50<23:40,  2.30it/s] 20%|██        | 827/4096 [06:50<23:40,  2.30it/s] 20%|██        | 828/4096 [06:51<23:39,  2.30it/s] 20%|██        | 829/4096 [06:51<23:38,  2.30it/s] 20%|██        | 830/4096 [06:52<23:42,  2.30it/s] 20%|██        | 831/4096 [06:52<23:41,  2.30it/s] 20%|██        | 832/4096 [06:53<23:38,  2.30it/s] 20%|██        | 833/4096 [06:53<23:36,  2.30it/s] 20%|██        | 834/4096 [06:53<23:34,  2.31it/s] 20%|██        | 835/4096 [06:54<23:34,  2.31it/s] 20%|██        | 836/4096 [06:54<23:35,  2.30it/s] 20%|██        | 837/4096 [06:55<23:35,  2.30it/s] 20%|██        | 838/4096 [06:55<23:34,  2.30it/s] 20%|██        | 839/4096 [06:56<23:35,  2.30it/s] 21%|██        | 840/4096 [06:56<23:33,  2.30it/s] 21%|██        | 841/4096 [06:57<23:34,  2.30it/s] 21%|██        | 842/4096 [06:57<23:32,  2.30it/s] 21%|██        | 843/4096 [06:57<23:33,  2.30it/s] 21%|██        | 844/4096 [06:58<23:32,  2.30it/s] 21%|██        | 845/4096 [06:58<23:32,  2.30it/s] 21%|██        | 846/4096 [06:59<23:32,  2.30it/s] 21%|██        | 847/4096 [06:59<23:28,  2.31it/s] 21%|██        | 848/4096 [07:00<23:28,  2.31it/s] 21%|██        | 849/4096 [07:00<23:28,  2.31it/s] 21%|██        | 850/4096 [07:00<23:25,  2.31it/s] 21%|██        | 851/4096 [07:01<23:24,  2.31it/s] 21%|██        | 852/4096 [07:01<23:24,  2.31it/s] 21%|██        | 853/4096 [07:02<23:23,  2.31it/s] 21%|██        | 854/4096 [07:02<23:23,  2.31it/s] 21%|██        | 855/4096 [07:03<23:21,  2.31it/s] 21%|██        | 856/4096 [07:03<23:23,  2.31it/s] 21%|██        | 857/4096 [07:03<23:22,  2.31it/s] 21%|██        | 858/4096 [07:04<23:22,  2.31it/s] 21%|██        | 859/4096 [07:04<23:25,  2.30it/s] 21%|██        | 860/4096 [07:05<23:24,  2.30it/s] 21%|██        | 861/4096 [07:05<23:22,  2.31it/s] 21%|██        | 862/4096 [07:06<23:21,  2.31it/s] 21%|██        | 863/4096 [07:06<23:20,  2.31it/s] 21%|██        | 864/4096 [07:06<23:21,  2.31it/s] 21%|██        | 865/4096 [07:07<23:21,  2.31it/s] 21%|██        | 866/4096 [07:07<23:25,  2.30it/s] 21%|██        | 867/4096 [07:08<23:24,  2.30it/s] 21%|██        | 868/4096 [07:08<23:24,  2.30it/s] 21%|██        | 869/4096 [07:09<23:29,  2.29it/s] 21%|██        | 870/4096 [07:09<23:27,  2.29it/s] 21%|██▏       | 871/4096 [07:10<23:24,  2.30it/s] 21%|██▏       | 872/4096 [07:10<23:23,  2.30it/s] 21%|██▏       | 873/4096 [07:10<23:21,  2.30it/s] 21%|██▏       | 874/4096 [07:11<23:20,  2.30it/s] 21%|██▏       | 875/4096 [07:11<23:18,  2.30it/s] 21%|██▏       | 876/4096 [07:12<23:19,  2.30it/s] 21%|██▏       | 877/4096 [07:12<23:17,  2.30it/s] 21%|██▏       | 878/4096 [07:13<23:17,  2.30it/s] 21%|██▏       | 879/4096 [07:13<23:17,  2.30it/s] 21%|██▏       | 880/4096 [07:13<23:22,  2.29it/s] 22%|██▏       | 881/4096 [07:14<23:21,  2.29it/s] 22%|██▏       | 882/4096 [07:14<23:22,  2.29it/s] 22%|██▏       | 883/4096 [07:15<23:20,  2.29it/s] 22%|██▏       | 884/4096 [07:15<23:15,  2.30it/s] 22%|██▏       | 885/4096 [07:16<23:13,  2.30it/s] 22%|██▏       | 886/4096 [07:16<23:11,  2.31it/s] 22%|██▏       | 887/4096 [07:16<23:10,  2.31it/s] 22%|██▏       | 888/4096 [07:17<23:09,  2.31it/s] 22%|██▏       | 889/4096 [07:17<23:06,  2.31it/s] 22%|██▏       | 890/4096 [07:18<23:12,  2.30it/s] 22%|██▏       | 891/4096 [07:18<23:11,  2.30it/s] 22%|██▏       | 892/4096 [07:19<23:08,  2.31it/s] 22%|██▏       | 893/4096 [07:19<23:08,  2.31it/s] 22%|██▏       | 894/4096 [07:20<23:08,  2.31it/s] 22%|██▏       | 895/4096 [07:20<23:08,  2.30it/s] 22%|██▏       | 896/4096 [07:20<19:04,  2.80it/s]loss: 2.608621120452881
===========================
epoch 7/32 | loss: 2.608621120452881
---------------------------
example true genres: 
tensor([1, 1, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 1, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9431072210065645
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['It makes no difference what part of the world is involved, what form of regime, what particular issue. The U. S. cannot take the initiative against the Left. There is even some question whether the U. S. can any longer defend itself against an initiative by the Left.', "he knows that he was never more popular than at the time of the Russo - American ` ` honeymoon'' of 1959. But it seems that pressures against him are coming from somewhere - - in the first place from China, but perhaps also from that ` ` China Lobby'' which, I was assured in Moscow nearly two years ago, exists on the quiet inside the party. To these people, solidarity and unity with China should be the real basis of Russia's future policy.", "His head came up and he said it defiantly. ` ` Well, congratulations''. Feathertop made an elaborate motion with his hand.", "You don't see me stretched out on the deck, do you''?? ` ` You know what they say about two deep dives in one day'', Artie went on, still twirling the snorkle and studying it intently. ` ` I don't think you should go down again''.", 'The marine was alone, for they were impatient people and by now would have vied to knock him from the tree. Down the tree he scrambled and knelt at the edge of foliage. The marine was sprawled some thirty yards away, one arm extended.']
---------------------------
example output paragraph: 
['The is the sense to is of the world is the. but is is thes the is people is The other.... be a risk of the President Party The is no a reason of the President.... be more than the. the enemy. the United Party', "is the the is a told than than the least time. the warr American American ` `ing'' the'The the is that that of the'not to the the - - the middle time. the. the the the the the country ` ` 'bog '. is in think a that the. to years ago ago is to the ground and of world of the people, I, a of the, be expected same issue of the'' own..", "` eyes was up to the turned,.tlylyly ` ` `, Itra''.rryeds a effort effort of a hand.", "`'' t want any'out out the side, but'' ll?? ` `'' you ', you men mens, the '. '? he ` said on on ` thinkinguckinginging bottleluging of a the.. ` You'' t know you'go to on''?", 'He man man a. and the had not.. the the, be beenile his the the. the side. The the hall, was to climbed down the side of the. man was a out way feet away. and of and,']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.57it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.62it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.86it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.30it/s][A100%|██████████| 29/29 [00:00<00:00, 66.15it/s]
Mean Perplexity: 1422.1108248186843
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 22%|██▏       | 897/4096 [07:27<2:01:46,  2.28s/it] 22%|██▏       | 898/4096 [07:27<1:32:09,  1.73s/it] 22%|██▏       | 899/4096 [07:28<1:11:33,  1.34s/it] 22%|██▏       | 900/4096 [07:28<56:59,  1.07s/it]   22%|██▏       | 901/4096 [07:29<46:46,  1.14it/s] 22%|██▏       | 902/4096 [07:29<39:37,  1.34it/s] 22%|██▏       | 903/4096 [07:30<34:38,  1.54it/s] 22%|██▏       | 904/4096 [07:30<31:09,  1.71it/s] 22%|██▏       | 905/4096 [07:30<28:45,  1.85it/s] 22%|██▏       | 906/4096 [07:31<27:02,  1.97it/s] 22%|██▏       | 907/4096 [07:31<25:51,  2.06it/s] 22%|██▏       | 908/4096 [07:32<25:01,  2.12it/s] 22%|██▏       | 909/4096 [07:32<24:26,  2.17it/s] 22%|██▏       | 910/4096 [07:33<24:01,  2.21it/s] 22%|██▏       | 911/4096 [07:33<23:40,  2.24it/s] 22%|██▏       | 912/4096 [07:33<23:24,  2.27it/s] 22%|██▏       | 913/4096 [07:34<23:15,  2.28it/s] 22%|██▏       | 914/4096 [07:34<23:08,  2.29it/s] 22%|██▏       | 915/4096 [07:35<23:05,  2.30it/s] 22%|██▏       | 916/4096 [07:35<23:01,  2.30it/s] 22%|██▏       | 917/4096 [07:36<22:59,  2.30it/s] 22%|██▏       | 918/4096 [07:36<22:58,  2.30it/s] 22%|██▏       | 919/4096 [07:36<22:58,  2.30it/s] 22%|██▏       | 920/4096 [07:37<22:59,  2.30it/s] 22%|██▏       | 921/4096 [07:37<22:59,  2.30it/s] 23%|██▎       | 922/4096 [07:38<23:00,  2.30it/s] 23%|██▎       | 923/4096 [07:38<23:00,  2.30it/s] 23%|██▎       | 924/4096 [07:39<22:58,  2.30it/s] 23%|██▎       | 925/4096 [07:39<22:55,  2.30it/s] 23%|██▎       | 926/4096 [07:40<22:53,  2.31it/s] 23%|██▎       | 927/4096 [07:40<22:53,  2.31it/s] 23%|██▎       | 928/4096 [07:40<22:53,  2.31it/s] 23%|██▎       | 929/4096 [07:41<22:53,  2.31it/s] 23%|██▎       | 930/4096 [07:41<22:53,  2.30it/s] 23%|██▎       | 931/4096 [07:42<22:54,  2.30it/s] 23%|██▎       | 932/4096 [07:42<22:54,  2.30it/s] 23%|██▎       | 933/4096 [07:43<22:56,  2.30it/s] 23%|██▎       | 934/4096 [07:43<22:52,  2.30it/s] 23%|██▎       | 935/4096 [07:43<22:54,  2.30it/s] 23%|██▎       | 936/4096 [07:44<22:54,  2.30it/s] 23%|██▎       | 937/4096 [07:44<22:55,  2.30it/s] 23%|██▎       | 938/4096 [07:45<22:53,  2.30it/s] 23%|██▎       | 939/4096 [07:45<22:52,  2.30it/s] 23%|██▎       | 940/4096 [07:46<22:52,  2.30it/s] 23%|██▎       | 941/4096 [07:46<22:49,  2.30it/s] 23%|██▎       | 942/4096 [07:46<22:48,  2.30it/s] 23%|██▎       | 943/4096 [07:47<22:47,  2.31it/s] 23%|██▎       | 944/4096 [07:47<22:47,  2.31it/s] 23%|██▎       | 945/4096 [07:48<22:48,  2.30it/s] 23%|██▎       | 946/4096 [07:48<22:46,  2.31it/s] 23%|██▎       | 947/4096 [07:49<22:45,  2.31it/s] 23%|██▎       | 948/4096 [07:49<22:45,  2.31it/s] 23%|██▎       | 949/4096 [07:49<22:45,  2.30it/s] 23%|██▎       | 950/4096 [07:50<22:45,  2.30it/s] 23%|██▎       | 951/4096 [07:50<22:44,  2.30it/s] 23%|██▎       | 952/4096 [07:51<22:44,  2.30it/s] 23%|██▎       | 953/4096 [07:51<22:43,  2.30it/s] 23%|██▎       | 954/4096 [07:52<22:43,  2.30it/s] 23%|██▎       | 955/4096 [07:52<22:41,  2.31it/s] 23%|██▎       | 956/4096 [07:53<22:42,  2.31it/s] 23%|██▎       | 957/4096 [07:53<22:42,  2.30it/s] 23%|██▎       | 958/4096 [07:53<22:41,  2.31it/s] 23%|██▎       | 959/4096 [07:54<22:38,  2.31it/s] 23%|██▎       | 960/4096 [07:54<22:38,  2.31it/s] 23%|██▎       | 961/4096 [07:55<22:38,  2.31it/s] 23%|██▎       | 962/4096 [07:55<22:39,  2.31it/s] 24%|██▎       | 963/4096 [07:56<22:38,  2.31it/s] 24%|██▎       | 964/4096 [07:56<22:37,  2.31it/s] 24%|██▎       | 965/4096 [07:56<22:35,  2.31it/s] 24%|██▎       | 966/4096 [07:57<22:36,  2.31it/s] 24%|██▎       | 967/4096 [07:57<22:37,  2.31it/s] 24%|██▎       | 968/4096 [07:58<22:37,  2.30it/s] 24%|██▎       | 969/4096 [07:58<22:37,  2.30it/s] 24%|██▎       | 970/4096 [07:59<22:37,  2.30it/s] 24%|██▎       | 971/4096 [07:59<22:39,  2.30it/s] 24%|██▎       | 972/4096 [07:59<22:37,  2.30it/s] 24%|██▍       | 973/4096 [08:00<22:35,  2.30it/s] 24%|██▍       | 974/4096 [08:00<22:32,  2.31it/s] 24%|██▍       | 975/4096 [08:01<22:32,  2.31it/s] 24%|██▍       | 976/4096 [08:01<22:32,  2.31it/s] 24%|██▍       | 977/4096 [08:02<22:33,  2.30it/s] 24%|██▍       | 978/4096 [08:02<22:32,  2.30it/s] 24%|██▍       | 979/4096 [08:03<22:36,  2.30it/s] 24%|██▍       | 980/4096 [08:03<22:39,  2.29it/s] 24%|██▍       | 981/4096 [08:03<22:39,  2.29it/s] 24%|██▍       | 982/4096 [08:04<22:38,  2.29it/s] 24%|██▍       | 983/4096 [08:04<22:35,  2.30it/s] 24%|██▍       | 984/4096 [08:05<22:34,  2.30it/s] 24%|██▍       | 985/4096 [08:05<22:32,  2.30it/s] 24%|██▍       | 986/4096 [08:06<22:31,  2.30it/s] 24%|██▍       | 987/4096 [08:06<22:29,  2.30it/s] 24%|██▍       | 988/4096 [08:06<22:30,  2.30it/s] 24%|██▍       | 989/4096 [08:07<22:30,  2.30it/s] 24%|██▍       | 990/4096 [08:07<22:31,  2.30it/s] 24%|██▍       | 991/4096 [08:08<22:31,  2.30it/s] 24%|██▍       | 992/4096 [08:08<22:29,  2.30it/s] 24%|██▍       | 993/4096 [08:09<22:27,  2.30it/s] 24%|██▍       | 994/4096 [08:09<22:24,  2.31it/s] 24%|██▍       | 995/4096 [08:09<22:23,  2.31it/s] 24%|██▍       | 996/4096 [08:10<22:24,  2.31it/s] 24%|██▍       | 997/4096 [08:10<22:26,  2.30it/s] 24%|██▍       | 998/4096 [08:11<22:25,  2.30it/s] 24%|██▍       | 999/4096 [08:11<22:25,  2.30it/s] 24%|██▍       | 1000/4096 [08:12<22:25,  2.30it/s] 24%|██▍       | 1001/4096 [08:12<22:23,  2.30it/s] 24%|██▍       | 1002/4096 [08:13<22:23,  2.30it/s] 24%|██▍       | 1003/4096 [08:13<22:23,  2.30it/s] 25%|██▍       | 1004/4096 [08:13<22:21,  2.30it/s] 25%|██▍       | 1005/4096 [08:14<22:19,  2.31it/s] 25%|██▍       | 1006/4096 [08:14<22:19,  2.31it/s] 25%|██▍       | 1007/4096 [08:15<22:19,  2.31it/s] 25%|██▍       | 1008/4096 [08:15<22:19,  2.30it/s] 25%|██▍       | 1009/4096 [08:16<22:20,  2.30it/s] 25%|██▍       | 1010/4096 [08:16<22:24,  2.29it/s] 25%|██▍       | 1011/4096 [08:16<22:22,  2.30it/s] 25%|██▍       | 1012/4096 [08:17<22:22,  2.30it/s] 25%|██▍       | 1013/4096 [08:17<22:21,  2.30it/s] 25%|██▍       | 1014/4096 [08:18<22:21,  2.30it/s] 25%|██▍       | 1015/4096 [08:18<22:22,  2.30it/s] 25%|██▍       | 1016/4096 [08:19<22:21,  2.30it/s] 25%|██▍       | 1017/4096 [08:19<22:21,  2.29it/s] 25%|██▍       | 1018/4096 [08:19<22:20,  2.30it/s] 25%|██▍       | 1019/4096 [08:20<22:21,  2.29it/s] 25%|██▍       | 1020/4096 [08:20<22:17,  2.30it/s] 25%|██▍       | 1021/4096 [08:21<22:17,  2.30it/s] 25%|██▍       | 1022/4096 [08:21<22:14,  2.30it/s] 25%|██▍       | 1023/4096 [08:22<22:12,  2.31it/s] 25%|██▌       | 1024/4096 [08:22<18:19,  2.79it/s]loss: 2.455937385559082
===========================
epoch 8/32 | loss: 2.455937385559082
---------------------------
example true genres: 
tensor([1, 0, 1, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9212253829321663
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['A 60 mm. mortar and a 57 mm. recoilless rifle owned by Lauchli were brought along. The mortar was equipped with dummy shells and the recoilless rifle was deactivated. After a tortuous drive in an open truck and a World War 2, army jeep down soggy trails, the band arrived at a small clearing squeezed between a long, low ridge and a creek - filled gully.', "Feathertop aimed a finger at him. ` ` Oh, come on, Cappy'', the girl chided. ` ` He's okay.", "I answered the routine question about my itinerary, rather coolly. Chiang spoke again, this time at greater length. ` ` The President says'', the translator came in, ` ` that the reason he asked you where you were going is because he hoped you would be visiting other areas in Southeast Asia, and that everywhere you went, you would seek the answer to your question.", "` ` You came well equipped to die''. Some odor made him lean over the man. He sniffed and recognized it.", "I know this from my talks with him''. ` ` Well, let's let him make up his own mind, OK''?? Waddell said."]
---------------------------
example output paragraph: 
['The few -. Thes a rifle - rifle Theconed rifle rifle a thederrff in in in The rifles a with anged rifles. a riflecon rifle rifle. ater. the minute theile time, the aircraft area, the truck War., the waseeps the much and were the truck was at the time field of by the truck road narrow,. the few. filled valleyully.', "`ather's at pistol, the. ` ` You, you,, you'' ', he man saidoppy him ` ` You said s going,", "The am the telephone of. the own wasating. but thanly, I ` of, and time time the length. ` I man of that ', he President of to to and ` ` he President is is him is he are going to to he is to would not able the places of the Asia. and not he else have to and would not out same for the name.", "He ` `'to to to go. '. He ofs him a to the saddle. was the said the.", "`'you'the own. the. '. ` ` `, I's be'' a'own own'but''.? `ritedle said."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.47it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.25it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.19it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.91it/s][A100%|██████████| 29/29 [00:00<00:00, 66.04it/s]
Mean Perplexity: 928.6724438656863
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 25%|██▌       | 1025/4096 [08:29<1:57:25,  2.29s/it] 25%|██▌       | 1026/4096 [08:29<1:28:52,  1.74s/it] 25%|██▌       | 1027/4096 [08:29<1:08:51,  1.35s/it] 25%|██▌       | 1028/4096 [08:30<54:49,  1.07s/it]   25%|██▌       | 1029/4096 [08:30<45:11,  1.13it/s] 25%|██▌       | 1030/4096 [08:31<38:17,  1.33it/s] 25%|██▌       | 1031/4096 [08:31<33:26,  1.53it/s] 25%|██▌       | 1032/4096 [08:32<30:04,  1.70it/s] 25%|██▌       | 1033/4096 [08:32<27:41,  1.84it/s] 25%|██▌       | 1034/4096 [08:33<26:01,  1.96it/s] 25%|██▌       | 1035/4096 [08:33<24:51,  2.05it/s] 25%|██▌       | 1036/4096 [08:33<24:02,  2.12it/s] 25%|██▌       | 1037/4096 [08:34<23:27,  2.17it/s] 25%|██▌       | 1038/4096 [08:34<23:04,  2.21it/s] 25%|██▌       | 1039/4096 [08:35<22:47,  2.23it/s] 25%|██▌       | 1040/4096 [08:35<22:36,  2.25it/s] 25%|██▌       | 1041/4096 [08:36<22:26,  2.27it/s] 25%|██▌       | 1042/4096 [08:36<22:20,  2.28it/s] 25%|██▌       | 1043/4096 [08:36<22:16,  2.28it/s] 25%|██▌       | 1044/4096 [08:37<22:14,  2.29it/s] 26%|██▌       | 1045/4096 [08:37<22:11,  2.29it/s] 26%|██▌       | 1046/4096 [08:38<22:07,  2.30it/s] 26%|██▌       | 1047/4096 [08:38<22:05,  2.30it/s] 26%|██▌       | 1048/4096 [08:39<22:05,  2.30it/s] 26%|██▌       | 1049/4096 [08:39<22:05,  2.30it/s] 26%|██▌       | 1050/4096 [08:40<22:03,  2.30it/s] 26%|██▌       | 1051/4096 [08:40<22:01,  2.30it/s] 26%|██▌       | 1052/4096 [08:40<21:59,  2.31it/s] 26%|██▌       | 1053/4096 [08:41<22:01,  2.30it/s] 26%|██▌       | 1054/4096 [08:41<22:00,  2.30it/s] 26%|██▌       | 1055/4096 [08:42<22:02,  2.30it/s] 26%|██▌       | 1056/4096 [08:42<22:02,  2.30it/s] 26%|██▌       | 1057/4096 [08:43<22:00,  2.30it/s] 26%|██▌       | 1058/4096 [08:43<22:00,  2.30it/s] 26%|██▌       | 1059/4096 [08:43<21:59,  2.30it/s] 26%|██▌       | 1060/4096 [08:44<21:58,  2.30it/s] 26%|██▌       | 1061/4096 [08:44<21:56,  2.30it/s] 26%|██▌       | 1062/4096 [08:45<21:55,  2.31it/s] 26%|██▌       | 1063/4096 [08:45<21:56,  2.30it/s] 26%|██▌       | 1064/4096 [08:46<21:54,  2.31it/s] 26%|██▌       | 1065/4096 [08:46<21:53,  2.31it/s] 26%|██▌       | 1066/4096 [08:46<21:53,  2.31it/s] 26%|██▌       | 1067/4096 [08:47<21:50,  2.31it/s] 26%|██▌       | 1068/4096 [08:47<21:51,  2.31it/s] 26%|██▌       | 1069/4096 [08:48<21:50,  2.31it/s] 26%|██▌       | 1070/4096 [08:48<21:51,  2.31it/s] 26%|██▌       | 1071/4096 [08:49<21:52,  2.31it/s] 26%|██▌       | 1072/4096 [08:49<21:49,  2.31it/s] 26%|██▌       | 1073/4096 [08:49<21:50,  2.31it/s] 26%|██▌       | 1074/4096 [08:50<21:49,  2.31it/s] 26%|██▌       | 1075/4096 [08:50<21:49,  2.31it/s] 26%|██▋       | 1076/4096 [08:51<21:49,  2.31it/s] 26%|██▋       | 1077/4096 [08:51<21:49,  2.30it/s] 26%|██▋       | 1078/4096 [08:52<21:49,  2.31it/s] 26%|██▋       | 1079/4096 [08:52<21:49,  2.30it/s] 26%|██▋       | 1080/4096 [08:53<21:49,  2.30it/s] 26%|██▋       | 1081/4096 [08:53<21:48,  2.30it/s] 26%|██▋       | 1082/4096 [08:53<21:47,  2.31it/s] 26%|██▋       | 1083/4096 [08:54<21:44,  2.31it/s] 26%|██▋       | 1084/4096 [08:54<21:46,  2.31it/s] 26%|██▋       | 1085/4096 [08:55<21:45,  2.31it/s] 27%|██▋       | 1086/4096 [08:55<21:44,  2.31it/s] 27%|██▋       | 1087/4096 [08:56<21:43,  2.31it/s] 27%|██▋       | 1088/4096 [08:56<21:44,  2.31it/s] 27%|██▋       | 1089/4096 [08:56<21:42,  2.31it/s] 27%|██▋       | 1090/4096 [08:57<21:41,  2.31it/s] 27%|██▋       | 1091/4096 [08:57<21:42,  2.31it/s] 27%|██▋       | 1092/4096 [08:58<21:41,  2.31it/s] 27%|██▋       | 1093/4096 [08:58<21:40,  2.31it/s] 27%|██▋       | 1094/4096 [08:59<21:43,  2.30it/s] 27%|██▋       | 1095/4096 [08:59<21:44,  2.30it/s] 27%|██▋       | 1096/4096 [08:59<21:41,  2.30it/s] 27%|██▋       | 1097/4096 [09:00<21:42,  2.30it/s] 27%|██▋       | 1098/4096 [09:00<21:41,  2.30it/s] 27%|██▋       | 1099/4096 [09:01<21:39,  2.31it/s] 27%|██▋       | 1100/4096 [09:01<21:39,  2.31it/s] 27%|██▋       | 1101/4096 [09:02<21:40,  2.30it/s] 27%|██▋       | 1102/4096 [09:02<21:39,  2.30it/s] 27%|██▋       | 1103/4096 [09:02<21:40,  2.30it/s] 27%|██▋       | 1104/4096 [09:03<21:40,  2.30it/s] 27%|██▋       | 1105/4096 [09:03<21:38,  2.30it/s] 27%|██▋       | 1106/4096 [09:04<21:41,  2.30it/s] 27%|██▋       | 1107/4096 [09:04<21:39,  2.30it/s] 27%|██▋       | 1108/4096 [09:05<21:39,  2.30it/s] 27%|██▋       | 1109/4096 [09:05<21:42,  2.29it/s] 27%|██▋       | 1110/4096 [09:06<21:39,  2.30it/s] 27%|██▋       | 1111/4096 [09:06<21:36,  2.30it/s] 27%|██▋       | 1112/4096 [09:06<21:35,  2.30it/s] 27%|██▋       | 1113/4096 [09:07<21:32,  2.31it/s] 27%|██▋       | 1114/4096 [09:07<21:32,  2.31it/s] 27%|██▋       | 1115/4096 [09:08<21:34,  2.30it/s] 27%|██▋       | 1116/4096 [09:08<21:38,  2.30it/s] 27%|██▋       | 1117/4096 [09:09<21:38,  2.29it/s] 27%|██▋       | 1118/4096 [09:09<21:39,  2.29it/s] 27%|██▋       | 1119/4096 [09:09<21:36,  2.30it/s] 27%|██▋       | 1120/4096 [09:10<21:33,  2.30it/s] 27%|██▋       | 1121/4096 [09:10<21:31,  2.30it/s] 27%|██▋       | 1122/4096 [09:11<21:30,  2.30it/s] 27%|██▋       | 1123/4096 [09:11<21:29,  2.31it/s] 27%|██▋       | 1124/4096 [09:12<21:26,  2.31it/s] 27%|██▋       | 1125/4096 [09:12<21:27,  2.31it/s] 27%|██▋       | 1126/4096 [09:12<21:29,  2.30it/s] 28%|██▊       | 1127/4096 [09:13<21:30,  2.30it/s] 28%|██▊       | 1128/4096 [09:13<21:29,  2.30it/s] 28%|██▊       | 1129/4096 [09:14<21:26,  2.31it/s] 28%|██▊       | 1130/4096 [09:14<21:27,  2.30it/s] 28%|██▊       | 1131/4096 [09:15<21:26,  2.31it/s] 28%|██▊       | 1132/4096 [09:15<21:24,  2.31it/s] 28%|██▊       | 1133/4096 [09:16<21:23,  2.31it/s] 28%|██▊       | 1134/4096 [09:16<21:24,  2.31it/s] 28%|██▊       | 1135/4096 [09:16<21:24,  2.31it/s] 28%|██▊       | 1136/4096 [09:17<21:24,  2.30it/s] 28%|██▊       | 1137/4096 [09:17<21:21,  2.31it/s] 28%|██▊       | 1138/4096 [09:18<21:21,  2.31it/s] 28%|██▊       | 1139/4096 [09:18<21:20,  2.31it/s] 28%|██▊       | 1140/4096 [09:19<21:20,  2.31it/s] 28%|██▊       | 1141/4096 [09:19<21:19,  2.31it/s] 28%|██▊       | 1142/4096 [09:19<21:19,  2.31it/s] 28%|██▊       | 1143/4096 [09:20<21:19,  2.31it/s] 28%|██▊       | 1144/4096 [09:20<21:19,  2.31it/s] 28%|██▊       | 1145/4096 [09:21<21:20,  2.31it/s] 28%|██▊       | 1146/4096 [09:21<21:19,  2.30it/s] 28%|██▊       | 1147/4096 [09:22<21:19,  2.31it/s] 28%|██▊       | 1148/4096 [09:22<21:19,  2.30it/s] 28%|██▊       | 1149/4096 [09:22<21:20,  2.30it/s] 28%|██▊       | 1150/4096 [09:23<21:20,  2.30it/s] 28%|██▊       | 1151/4096 [09:23<21:18,  2.30it/s] 28%|██▊       | 1152/4096 [09:24<17:32,  2.80it/s]loss: 2.137118101119995
===========================
epoch 9/32 | loss: 2.137118101119995
---------------------------
example true genres: 
tensor([0, 1, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9190371991247265
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["Since then, and since the pure grain had gotten him divorced from every decent - - and even indecent - - group from Greenwich Village to the Embarcadero, he had become a sucker - rolling freight - jumper. ` ` There ain't nothin'faster, or lonelier, or more direct than a cannonball freight when you wanna go someplace'', Feathertop would say. ` ` The accommodations may not be the poshest", "The critics'campaign finally inspired the first major U. S. exhibit of Schiele's works. The show has been to Boston and Manhattan, will in time reach Pittsburgh and Minneapolis. Last week it opened at the J. B. Speed Museum in Louisville, at the very moment that a second Schiele exhibit was being made ready at the Felix Landau gallery in Los Angeles.", "I don't get it why this time I should pull such a stupid trick''. ` ` Well, I get it'', Artie said, still on the ladder. ` ` You are a big muscle - bound ape and you got this idea about setting a record.", 'Fear and relief mingled in his churning emotions. He pressed his palms together and addressed himself to the patron saint of divers in a hurried and anxious whisper. ` ` Blessed Saint Nicholas, I thank thee for getting me out of that mess and sending me up instead of down when I was bewildered.', "Such delicate beauty, such fine flesh. It will rip and shred easily for Madame''. ` ` Be quiet, Devol!!"]
---------------------------
example output paragraph: 
["The the, the the the firstst was been to to, the man man sized - the the thecent - - -, the,, the otherberbarve,, he had been a popularer of - iner -, ` We'' t any much'' than Mr evenr, but even than than you man,.er the hit'to way to '. helderss be that ` ` mans'be be in sameonyer of", 'The American were s was was the public two public. S.. in thehuffff s s. The first shows been shown be, the, and be the for the. the. The year, was in the first. K. Cey. Chicago. the the time last. the new,huffr is opened held. for the same Felixer.. the Angeles.', "`'' t want to'' time'' have it a thing thing. '. ` ` I, I'to'', he said said, ` looking the way. ` You'going fool man man - man man'you'a one of it up fire on", "He of the,gling with the mindores thoughts. He was his lips against against he him. the manizing. the. the deep manner gentle,. ` I, Mary'he know you for you to to of the mountain of I me to to of a there I was ahold.", '` things creatures, and a,, ` is beple fhroud me, a. s. ` Iss, Ieryn!']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.62it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.06it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.36it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.35it/s][A100%|██████████| 29/29 [00:00<00:00, 66.36it/s]
Mean Perplexity: 993.5991321121315
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 28%|██▊       | 1153/4096 [09:30<1:51:52,  2.28s/it] 28%|██▊       | 1154/4096 [09:31<1:24:39,  1.73s/it] 28%|██▊       | 1155/4096 [09:31<1:05:50,  1.34s/it] 28%|██▊       | 1156/4096 [09:32<52:52,  1.08s/it]   28%|██▊       | 1157/4096 [09:32<43:22,  1.13it/s] 28%|██▊       | 1158/4096 [09:32<36:42,  1.33it/s] 28%|██▊       | 1159/4096 [09:33<32:02,  1.53it/s] 28%|██▊       | 1160/4096 [09:33<28:46,  1.70it/s] 28%|██▊       | 1161/4096 [09:34<26:29,  1.85it/s] 28%|██▊       | 1162/4096 [09:34<24:54,  1.96it/s] 28%|██▊       | 1163/4096 [09:35<23:46,  2.06it/s] 28%|██▊       | 1164/4096 [09:35<23:00,  2.12it/s] 28%|██▊       | 1165/4096 [09:36<22:29,  2.17it/s] 28%|██▊       | 1166/4096 [09:36<22:04,  2.21it/s] 28%|██▊       | 1167/4096 [09:36<21:47,  2.24it/s] 29%|██▊       | 1168/4096 [09:37<21:34,  2.26it/s] 29%|██▊       | 1169/4096 [09:37<21:27,  2.27it/s] 29%|██▊       | 1170/4096 [09:38<21:29,  2.27it/s] 29%|██▊       | 1171/4096 [09:38<21:29,  2.27it/s] 29%|██▊       | 1172/4096 [09:39<21:23,  2.28it/s] 29%|██▊       | 1173/4096 [09:39<21:19,  2.28it/s] 29%|██▊       | 1174/4096 [09:39<21:13,  2.29it/s] 29%|██▊       | 1175/4096 [09:40<21:12,  2.30it/s] 29%|██▊       | 1176/4096 [09:40<21:08,  2.30it/s] 29%|██▊       | 1177/4096 [09:41<21:04,  2.31it/s] 29%|██▉       | 1178/4096 [09:41<21:04,  2.31it/s] 29%|██▉       | 1179/4096 [09:42<21:02,  2.31it/s] 29%|██▉       | 1180/4096 [09:42<21:01,  2.31it/s] 29%|██▉       | 1181/4096 [09:42<21:02,  2.31it/s] 29%|██▉       | 1182/4096 [09:43<21:02,  2.31it/s] 29%|██▉       | 1183/4096 [09:43<21:02,  2.31it/s] 29%|██▉       | 1184/4096 [09:44<21:02,  2.31it/s] 29%|██▉       | 1185/4096 [09:44<21:01,  2.31it/s] 29%|██▉       | 1186/4096 [09:45<20:58,  2.31it/s] 29%|██▉       | 1187/4096 [09:45<20:58,  2.31it/s] 29%|██▉       | 1188/4096 [09:45<20:58,  2.31it/s] 29%|██▉       | 1189/4096 [09:46<20:59,  2.31it/s] 29%|██▉       | 1190/4096 [09:46<20:59,  2.31it/s] 29%|██▉       | 1191/4096 [09:47<20:56,  2.31it/s] 29%|██▉       | 1192/4096 [09:47<20:57,  2.31it/s] 29%|██▉       | 1193/4096 [09:48<20:57,  2.31it/s] 29%|██▉       | 1194/4096 [09:48<20:55,  2.31it/s] 29%|██▉       | 1195/4096 [09:49<20:55,  2.31it/s] 29%|██▉       | 1196/4096 [09:49<20:56,  2.31it/s] 29%|██▉       | 1197/4096 [09:49<20:56,  2.31it/s] 29%|██▉       | 1198/4096 [09:50<20:57,  2.30it/s] 29%|██▉       | 1199/4096 [09:50<20:58,  2.30it/s] 29%|██▉       | 1200/4096 [09:51<20:57,  2.30it/s] 29%|██▉       | 1201/4096 [09:51<20:55,  2.31it/s] 29%|██▉       | 1202/4096 [09:52<20:56,  2.30it/s] 29%|██▉       | 1203/4096 [09:52<20:55,  2.30it/s] 29%|██▉       | 1204/4096 [09:52<20:53,  2.31it/s] 29%|██▉       | 1205/4096 [09:53<20:52,  2.31it/s] 29%|██▉       | 1206/4096 [09:53<20:51,  2.31it/s] 29%|██▉       | 1207/4096 [09:54<20:52,  2.31it/s] 29%|██▉       | 1208/4096 [09:54<20:53,  2.30it/s] 30%|██▉       | 1209/4096 [09:55<20:52,  2.30it/s] 30%|██▉       | 1210/4096 [09:55<20:52,  2.30it/s] 30%|██▉       | 1211/4096 [09:55<20:51,  2.30it/s] 30%|██▉       | 1212/4096 [09:56<20:52,  2.30it/s] 30%|██▉       | 1213/4096 [09:56<20:50,  2.31it/s] 30%|██▉       | 1214/4096 [09:57<20:50,  2.30it/s] 30%|██▉       | 1215/4096 [09:57<20:50,  2.30it/s] 30%|██▉       | 1216/4096 [09:58<20:50,  2.30it/s] 30%|██▉       | 1217/4096 [09:58<20:50,  2.30it/s] 30%|██▉       | 1218/4096 [09:59<20:49,  2.30it/s] 30%|██▉       | 1219/4096 [09:59<20:49,  2.30it/s] 30%|██▉       | 1220/4096 [09:59<20:49,  2.30it/s] 30%|██▉       | 1221/4096 [10:00<20:49,  2.30it/s] 30%|██▉       | 1222/4096 [10:00<20:48,  2.30it/s] 30%|██▉       | 1223/4096 [10:01<20:49,  2.30it/s] 30%|██▉       | 1224/4096 [10:01<20:47,  2.30it/s] 30%|██▉       | 1225/4096 [10:02<20:46,  2.30it/s] 30%|██▉       | 1226/4096 [10:02<20:45,  2.31it/s] 30%|██▉       | 1227/4096 [10:02<20:42,  2.31it/s] 30%|██▉       | 1228/4096 [10:03<20:40,  2.31it/s] 30%|███       | 1229/4096 [10:03<20:41,  2.31it/s] 30%|███       | 1230/4096 [10:04<20:41,  2.31it/s] 30%|███       | 1231/4096 [10:04<20:41,  2.31it/s] 30%|███       | 1232/4096 [10:05<20:42,  2.30it/s] 30%|███       | 1233/4096 [10:05<20:41,  2.31it/s] 30%|███       | 1234/4096 [10:05<20:41,  2.31it/s] 30%|███       | 1235/4096 [10:06<20:40,  2.31it/s] 30%|███       | 1236/4096 [10:06<20:40,  2.31it/s] 30%|███       | 1237/4096 [10:07<20:42,  2.30it/s] 30%|███       | 1238/4096 [10:07<20:41,  2.30it/s] 30%|███       | 1239/4096 [10:08<20:40,  2.30it/s] 30%|███       | 1240/4096 [10:08<20:39,  2.30it/s] 30%|███       | 1241/4096 [10:08<20:39,  2.30it/s] 30%|███       | 1242/4096 [10:09<20:38,  2.30it/s] 30%|███       | 1243/4096 [10:09<20:38,  2.30it/s] 30%|███       | 1244/4096 [10:10<20:38,  2.30it/s] 30%|███       | 1245/4096 [10:10<20:37,  2.30it/s] 30%|███       | 1246/4096 [10:11<20:37,  2.30it/s] 30%|███       | 1247/4096 [10:11<20:38,  2.30it/s] 30%|███       | 1248/4096 [10:12<20:37,  2.30it/s] 30%|███       | 1249/4096 [10:12<20:38,  2.30it/s] 31%|███       | 1250/4096 [10:12<20:37,  2.30it/s] 31%|███       | 1251/4096 [10:13<20:36,  2.30it/s] 31%|███       | 1252/4096 [10:13<20:37,  2.30it/s] 31%|███       | 1253/4096 [10:14<20:35,  2.30it/s] 31%|███       | 1254/4096 [10:14<20:36,  2.30it/s] 31%|███       | 1255/4096 [10:15<20:35,  2.30it/s] 31%|███       | 1256/4096 [10:15<20:33,  2.30it/s] 31%|███       | 1257/4096 [10:15<20:31,  2.31it/s] 31%|███       | 1258/4096 [10:16<20:28,  2.31it/s] 31%|███       | 1259/4096 [10:16<20:27,  2.31it/s] 31%|███       | 1260/4096 [10:17<20:27,  2.31it/s] 31%|███       | 1261/4096 [10:17<20:27,  2.31it/s] 31%|███       | 1262/4096 [10:18<20:25,  2.31it/s] 31%|███       | 1263/4096 [10:18<20:26,  2.31it/s] 31%|███       | 1264/4096 [10:18<20:23,  2.32it/s] 31%|███       | 1265/4096 [10:19<20:22,  2.31it/s] 31%|███       | 1266/4096 [10:19<20:21,  2.32it/s] 31%|███       | 1267/4096 [10:20<20:21,  2.32it/s] 31%|███       | 1268/4096 [10:20<20:20,  2.32it/s] 31%|███       | 1269/4096 [10:21<20:19,  2.32it/s] 31%|███       | 1270/4096 [10:21<20:22,  2.31it/s] 31%|███       | 1271/4096 [10:21<20:21,  2.31it/s] 31%|███       | 1272/4096 [10:22<20:23,  2.31it/s] 31%|███       | 1273/4096 [10:22<20:23,  2.31it/s] 31%|███       | 1274/4096 [10:23<20:23,  2.31it/s] 31%|███       | 1275/4096 [10:23<20:23,  2.31it/s] 31%|███       | 1276/4096 [10:24<20:23,  2.31it/s] 31%|███       | 1277/4096 [10:24<20:23,  2.30it/s] 31%|███       | 1278/4096 [10:25<20:23,  2.30it/s] 31%|███       | 1279/4096 [10:25<20:20,  2.31it/s] 31%|███▏      | 1280/4096 [10:25<16:45,  2.80it/s]loss: 2.2318761348724365
===========================
epoch 10/32 | loss: 2.2318761348724365
---------------------------
example true genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.949671772428884
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['Heat, in the sunlight, pressed in like an invisible crowd. He squatted by the head, gently placing the rifle on the ground. With a snakestrike motion he grasped the hair, and, twisting, pulled the marine over on his back.', 'It was our hope to educate him and to give him his freedom when the right time came, for he was a bright and friendly youth who seemed worthy of our interest. After I paid Monsieur Prieur for Dandy, I brought him home, but he was ill at ease and ran away the same night. How he returned in such a ghastly condition, or why, I cannot say.', "But there was no definite agreement about business arrangements''. ` ` Well, damn'', Waddell said. There was the end of his front - page feature story, with byline.", "` ` Aristide!! I want you to find Monsieur Prieur at once and give him this money for the boy's purchase. There's $ 600 in gold in this chamois sack.", 'Hand grenades. The bobbing head was a poor target, so Matsuo shot him in the upper trunk. The marine spun, clapping a hand high on his chest, and dived forward.']
---------------------------
example output paragraph: 
['Heing the the dark, he the the a egg pin. He wasuitted on the door of and, the gun in the side. He a momentkinng,, dropped the gun of and pulled he it he it triggers the the back.', "He was a own that be him. he be him to life to he man of,, and he was a man man brave man. was to to the life. He the had him,.tain, the ', he had him a to and he was not. the. he to from house way. much was to the a auisetly way. and he he not had understand.", "` the was a one indication on the.. '. ` ` `, I it '. heriterig said. ` was no same of the voice - pocket editorial in. but a the of", "The ` I, you,! You am to to see me.randtain'the, I me a to to the money. s sake. I is s a 1, a - the boxapiry.", "Heling, He manulge of was a pistol,. but heine'at. a side half of He man was around andlattered his pistol.. his shoulder. and hed into."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.29it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.52it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.59it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.01it/s][A100%|██████████| 29/29 [00:00<00:00, 65.93it/s]
Mean Perplexity: 1003.2598969774642
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 31%|███▏      | 1281/4096 [10:32<1:48:12,  2.31s/it] 31%|███▏      | 1282/4096 [10:32<1:21:48,  1.74s/it] 31%|███▏      | 1283/4096 [10:33<1:03:22,  1.35s/it] 31%|███▏      | 1284/4096 [10:33<50:27,  1.08s/it]   31%|███▏      | 1285/4096 [10:34<41:23,  1.13it/s] 31%|███▏      | 1286/4096 [10:34<35:07,  1.33it/s] 31%|███▏      | 1287/4096 [10:35<30:37,  1.53it/s] 31%|███▏      | 1288/4096 [10:35<27:29,  1.70it/s] 31%|███▏      | 1289/4096 [10:35<25:16,  1.85it/s] 31%|███▏      | 1290/4096 [10:36<23:46,  1.97it/s] 32%|███▏      | 1291/4096 [10:36<22:40,  2.06it/s] 32%|███▏      | 1292/4096 [10:37<21:54,  2.13it/s] 32%|███▏      | 1293/4096 [10:37<21:27,  2.18it/s] 32%|███▏      | 1294/4096 [10:38<21:04,  2.22it/s] 32%|███▏      | 1295/4096 [10:38<20:51,  2.24it/s] 32%|███▏      | 1296/4096 [10:38<20:38,  2.26it/s] 32%|███▏      | 1297/4096 [10:39<20:31,  2.27it/s] 32%|███▏      | 1298/4096 [10:39<20:25,  2.28it/s] 32%|███▏      | 1299/4096 [10:40<20:20,  2.29it/s] 32%|███▏      | 1300/4096 [10:40<20:17,  2.30it/s] 32%|███▏      | 1301/4096 [10:41<20:14,  2.30it/s] 32%|███▏      | 1302/4096 [10:41<20:11,  2.31it/s] 32%|███▏      | 1303/4096 [10:42<20:10,  2.31it/s] 32%|███▏      | 1304/4096 [10:42<20:10,  2.31it/s] 32%|███▏      | 1305/4096 [10:42<20:08,  2.31it/s] 32%|███▏      | 1306/4096 [10:43<20:06,  2.31it/s] 32%|███▏      | 1307/4096 [10:43<20:05,  2.31it/s] 32%|███▏      | 1308/4096 [10:44<20:05,  2.31it/s] 32%|███▏      | 1309/4096 [10:44<20:04,  2.31it/s] 32%|███▏      | 1310/4096 [10:45<20:04,  2.31it/s] 32%|███▏      | 1311/4096 [10:45<20:04,  2.31it/s] 32%|███▏      | 1312/4096 [10:45<20:03,  2.31it/s] 32%|███▏      | 1313/4096 [10:46<20:02,  2.31it/s] 32%|███▏      | 1314/4096 [10:46<20:02,  2.31it/s] 32%|███▏      | 1315/4096 [10:47<20:02,  2.31it/s] 32%|███▏      | 1316/4096 [10:47<20:02,  2.31it/s] 32%|███▏      | 1317/4096 [10:48<20:01,  2.31it/s] 32%|███▏      | 1318/4096 [10:48<20:00,  2.31it/s] 32%|███▏      | 1319/4096 [10:48<20:01,  2.31it/s] 32%|███▏      | 1320/4096 [10:49<20:00,  2.31it/s] 32%|███▏      | 1321/4096 [10:49<20:00,  2.31it/s] 32%|███▏      | 1322/4096 [10:50<19:59,  2.31it/s] 32%|███▏      | 1323/4096 [10:50<19:59,  2.31it/s] 32%|███▏      | 1324/4096 [10:51<19:59,  2.31it/s] 32%|███▏      | 1325/4096 [10:51<19:57,  2.31it/s] 32%|███▏      | 1326/4096 [10:51<19:58,  2.31it/s] 32%|███▏      | 1327/4096 [10:52<19:59,  2.31it/s] 32%|███▏      | 1328/4096 [10:52<19:58,  2.31it/s] 32%|███▏      | 1329/4096 [10:53<19:57,  2.31it/s] 32%|███▏      | 1330/4096 [10:53<19:57,  2.31it/s] 32%|███▏      | 1331/4096 [10:54<19:58,  2.31it/s] 33%|███▎      | 1332/4096 [10:54<19:57,  2.31it/s] 33%|███▎      | 1333/4096 [10:55<19:57,  2.31it/s] 33%|███▎      | 1334/4096 [10:55<19:57,  2.31it/s] 33%|███▎      | 1335/4096 [10:55<19:56,  2.31it/s] 33%|███▎      | 1336/4096 [10:56<19:57,  2.30it/s] 33%|███▎      | 1337/4096 [10:56<19:56,  2.31it/s] 33%|███▎      | 1338/4096 [10:57<19:53,  2.31it/s] 33%|███▎      | 1339/4096 [10:57<19:53,  2.31it/s] 33%|███▎      | 1340/4096 [10:58<19:52,  2.31it/s] 33%|███▎      | 1341/4096 [10:58<19:51,  2.31it/s] 33%|███▎      | 1342/4096 [10:58<19:50,  2.31it/s] 33%|███▎      | 1343/4096 [10:59<19:49,  2.31it/s] 33%|███▎      | 1344/4096 [10:59<19:49,  2.31it/s] 33%|███▎      | 1345/4096 [11:00<19:50,  2.31it/s] 33%|███▎      | 1346/4096 [11:00<19:50,  2.31it/s] 33%|███▎      | 1347/4096 [11:01<19:49,  2.31it/s] 33%|███▎      | 1348/4096 [11:01<19:48,  2.31it/s] 33%|███▎      | 1349/4096 [11:01<19:48,  2.31it/s] 33%|███▎      | 1350/4096 [11:02<19:50,  2.31it/s] 33%|███▎      | 1351/4096 [11:02<19:49,  2.31it/s] 33%|███▎      | 1352/4096 [11:03<19:47,  2.31it/s] 33%|███▎      | 1353/4096 [11:03<19:47,  2.31it/s] 33%|███▎      | 1354/4096 [11:04<19:46,  2.31it/s] 33%|███▎      | 1355/4096 [11:04<19:46,  2.31it/s] 33%|███▎      | 1356/4096 [11:04<19:45,  2.31it/s] 33%|███▎      | 1357/4096 [11:05<19:45,  2.31it/s] 33%|███▎      | 1358/4096 [11:05<19:45,  2.31it/s] 33%|███▎      | 1359/4096 [11:06<19:45,  2.31it/s] 33%|███▎      | 1360/4096 [11:06<19:47,  2.30it/s] 33%|███▎      | 1361/4096 [11:07<19:46,  2.31it/s] 33%|███▎      | 1362/4096 [11:07<19:45,  2.31it/s] 33%|███▎      | 1363/4096 [11:07<19:43,  2.31it/s] 33%|███▎      | 1364/4096 [11:08<19:43,  2.31it/s] 33%|███▎      | 1365/4096 [11:08<19:42,  2.31it/s] 33%|███▎      | 1366/4096 [11:09<19:42,  2.31it/s] 33%|███▎      | 1367/4096 [11:09<19:42,  2.31it/s] 33%|███▎      | 1368/4096 [11:10<19:41,  2.31it/s] 33%|███▎      | 1369/4096 [11:10<19:39,  2.31it/s] 33%|███▎      | 1370/4096 [11:11<19:39,  2.31it/s] 33%|███▎      | 1371/4096 [11:11<19:39,  2.31it/s] 33%|███▎      | 1372/4096 [11:11<19:39,  2.31it/s] 34%|███▎      | 1373/4096 [11:12<19:38,  2.31it/s] 34%|███▎      | 1374/4096 [11:12<19:38,  2.31it/s] 34%|███▎      | 1375/4096 [11:13<19:38,  2.31it/s] 34%|███▎      | 1376/4096 [11:13<19:41,  2.30it/s] 34%|███▎      | 1377/4096 [11:14<19:42,  2.30it/s] 34%|███▎      | 1378/4096 [11:14<19:41,  2.30it/s] 34%|███▎      | 1379/4096 [11:14<19:44,  2.29it/s] 34%|███▎      | 1380/4096 [11:15<19:41,  2.30it/s] 34%|███▎      | 1381/4096 [11:15<19:39,  2.30it/s] 34%|███▎      | 1382/4096 [11:16<19:36,  2.31it/s] 34%|███▍      | 1383/4096 [11:16<19:37,  2.30it/s] 34%|███▍      | 1384/4096 [11:17<19:37,  2.30it/s] 34%|███▍      | 1385/4096 [11:17<19:37,  2.30it/s] 34%|███▍      | 1386/4096 [11:17<19:35,  2.31it/s] 34%|███▍      | 1387/4096 [11:18<19:35,  2.31it/s] 34%|███▍      | 1388/4096 [11:18<19:36,  2.30it/s] 34%|███▍      | 1389/4096 [11:19<19:36,  2.30it/s] 34%|███▍      | 1390/4096 [11:19<19:33,  2.31it/s] 34%|███▍      | 1391/4096 [11:20<19:32,  2.31it/s] 34%|███▍      | 1392/4096 [11:20<19:33,  2.30it/s] 34%|███▍      | 1393/4096 [11:21<19:31,  2.31it/s] 34%|███▍      | 1394/4096 [11:21<19:32,  2.31it/s] 34%|███▍      | 1395/4096 [11:21<19:30,  2.31it/s] 34%|███▍      | 1396/4096 [11:22<19:30,  2.31it/s] 34%|███▍      | 1397/4096 [11:22<19:30,  2.31it/s] 34%|███▍      | 1398/4096 [11:23<19:31,  2.30it/s] 34%|███▍      | 1399/4096 [11:23<19:28,  2.31it/s] 34%|███▍      | 1400/4096 [11:24<19:27,  2.31it/s] 34%|███▍      | 1401/4096 [11:24<19:26,  2.31it/s] 34%|███▍      | 1402/4096 [11:24<19:26,  2.31it/s] 34%|███▍      | 1403/4096 [11:25<19:27,  2.31it/s] 34%|███▍      | 1404/4096 [11:25<19:29,  2.30it/s] 34%|███▍      | 1405/4096 [11:26<19:29,  2.30it/s] 34%|███▍      | 1406/4096 [11:26<19:26,  2.31it/s] 34%|███▍      | 1407/4096 [11:27<19:26,  2.30it/s] 34%|███▍      | 1408/4096 [11:27<16:01,  2.80it/s]loss: 3.8829915523529053
===========================
epoch 11/32 | loss: 3.8829915523529053
---------------------------
example true genres: 
tensor([0, 1, 1, 0, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 1, 0, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9452954048140044
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["But there is no use causing him to worry at this time''. The German's words worked on the newspaperman like a reprieve from an odious duty. He took a big swig of his drink.", "We don't need this type of protection any more. The public is now armed with sophistication and numerous competing media. Besides, there are no longer enough corruptible journalists about.", "The objective should be to provide a method of getting into print a higher percentage than is now possible of the relevant information in the possession of reporters and editors. Southern California blackoutI would like to see you devote some space in an early issue to the news blackout concerning President Kennedy's activities, so far as Southern California is concerned.", "These two were going to be easy pickins. They couldn't have much dough, but then none of the freight - bums Feathertop rolled had much. And besides, the chick had a little something the others didn't have.", 'The military in Taiwan believe that the Communists have made two mistakes, which, together, may prove fatal. The first was the commune program, which will ensure agricultural poverty for years. The family is largely broken up ; ;']
---------------------------
example output paragraph: 
["He he was a way of him to be. him time. '. He man man s voice were out him man.. a manconroachr the unxen man. was a chance breathlug of his beer from", "The are't have to time of war. more than We people can a a with a muchstical. the weapons groups., the is are more longer ofed or who the", "The editor of be met be a good of a the the. new level of the the.. the public information. the United of the. the of The Illinois University schoolsersnc be to have the a the time to the editorial article of the editorial of market. the Kennedy's s. and that as theers has concerned.", "He were men the to be a.ing. They were't afford been time for but they they were them otherers wagonsales wereatherss up a of the, the bigs been few bit like way'' t like.", 'The President military the has that the United are been a years. and are in with are be to. The United is the first of of which is be the growth. the. The Communist has a in by and ;']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.89it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.94it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.64it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 67.04it/s][A100%|██████████| 29/29 [00:00<00:00, 67.05it/s]
Mean Perplexity: 1084.525939156764
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 34%|███▍      | 1409/4096 [11:34<1:41:53,  2.28s/it] 34%|███▍      | 1410/4096 [11:34<1:17:10,  1.72s/it] 34%|███▍      | 1411/4096 [11:34<59:52,  1.34s/it]   34%|███▍      | 1412/4096 [11:35<47:41,  1.07s/it] 34%|███▍      | 1413/4096 [11:35<39:09,  1.14it/s] 35%|███▍      | 1414/4096 [11:36<33:11,  1.35it/s] 35%|███▍      | 1415/4096 [11:36<29:00,  1.54it/s] 35%|███▍      | 1416/4096 [11:37<26:07,  1.71it/s] 35%|███▍      | 1417/4096 [11:37<24:34,  1.82it/s] 35%|███▍      | 1418/4096 [11:37<22:59,  1.94it/s] 35%|███▍      | 1419/4096 [11:38<21:51,  2.04it/s] 35%|███▍      | 1420/4096 [11:38<21:03,  2.12it/s] 35%|███▍      | 1421/4096 [11:39<20:29,  2.18it/s] 35%|███▍      | 1422/4096 [11:39<20:07,  2.21it/s] 35%|███▍      | 1423/4096 [11:40<19:50,  2.25it/s] 35%|███▍      | 1424/4096 [11:40<19:37,  2.27it/s] 35%|███▍      | 1425/4096 [11:40<19:30,  2.28it/s] 35%|███▍      | 1426/4096 [11:41<19:26,  2.29it/s] 35%|███▍      | 1427/4096 [11:41<19:23,  2.29it/s] 35%|███▍      | 1428/4096 [11:42<19:19,  2.30it/s] 35%|███▍      | 1429/4096 [11:42<19:18,  2.30it/s] 35%|███▍      | 1430/4096 [11:43<19:18,  2.30it/s] 35%|███▍      | 1431/4096 [11:43<19:16,  2.30it/s] 35%|███▍      | 1432/4096 [11:43<19:13,  2.31it/s] 35%|███▍      | 1433/4096 [11:44<19:13,  2.31it/s] 35%|███▌      | 1434/4096 [11:44<19:10,  2.31it/s] 35%|███▌      | 1435/4096 [11:45<19:10,  2.31it/s] 35%|███▌      | 1436/4096 [11:45<19:14,  2.30it/s] 35%|███▌      | 1437/4096 [11:46<19:14,  2.30it/s] 35%|███▌      | 1438/4096 [11:46<19:12,  2.31it/s] 35%|███▌      | 1439/4096 [11:47<19:11,  2.31it/s] 35%|███▌      | 1440/4096 [11:47<19:10,  2.31it/s] 35%|███▌      | 1441/4096 [11:47<19:09,  2.31it/s] 35%|███▌      | 1442/4096 [11:48<19:07,  2.31it/s] 35%|███▌      | 1443/4096 [11:48<19:05,  2.32it/s] 35%|███▌      | 1444/4096 [11:49<19:08,  2.31it/s] 35%|███▌      | 1445/4096 [11:49<19:08,  2.31it/s] 35%|███▌      | 1446/4096 [11:50<19:07,  2.31it/s] 35%|███▌      | 1447/4096 [11:50<19:08,  2.31it/s] 35%|███▌      | 1448/4096 [11:50<19:08,  2.31it/s] 35%|███▌      | 1449/4096 [11:51<19:06,  2.31it/s] 35%|███▌      | 1450/4096 [11:51<19:07,  2.31it/s] 35%|███▌      | 1451/4096 [11:52<19:06,  2.31it/s] 35%|███▌      | 1452/4096 [11:52<19:05,  2.31it/s] 35%|███▌      | 1453/4096 [11:53<19:04,  2.31it/s] 35%|███▌      | 1454/4096 [11:53<19:04,  2.31it/s] 36%|███▌      | 1455/4096 [11:53<19:03,  2.31it/s] 36%|███▌      | 1456/4096 [11:54<19:02,  2.31it/s] 36%|███▌      | 1457/4096 [11:54<19:02,  2.31it/s] 36%|███▌      | 1458/4096 [11:55<19:00,  2.31it/s] 36%|███▌      | 1459/4096 [11:55<19:01,  2.31it/s] 36%|███▌      | 1460/4096 [11:56<19:00,  2.31it/s] 36%|███▌      | 1461/4096 [11:56<19:01,  2.31it/s] 36%|███▌      | 1462/4096 [11:56<19:02,  2.30it/s] 36%|███▌      | 1463/4096 [11:57<19:00,  2.31it/s] 36%|███▌      | 1464/4096 [11:57<18:58,  2.31it/s] 36%|███▌      | 1465/4096 [11:58<18:57,  2.31it/s] 36%|███▌      | 1466/4096 [11:58<18:58,  2.31it/s] 36%|███▌      | 1467/4096 [11:59<18:57,  2.31it/s] 36%|███▌      | 1468/4096 [11:59<18:57,  2.31it/s] 36%|███▌      | 1469/4096 [12:00<18:57,  2.31it/s] 36%|███▌      | 1470/4096 [12:00<18:56,  2.31it/s] 36%|███▌      | 1471/4096 [12:00<18:56,  2.31it/s] 36%|███▌      | 1472/4096 [12:01<18:56,  2.31it/s] 36%|███▌      | 1473/4096 [12:01<18:57,  2.31it/s] 36%|███▌      | 1474/4096 [12:02<18:58,  2.30it/s] 36%|███▌      | 1475/4096 [12:02<18:56,  2.31it/s] 36%|███▌      | 1476/4096 [12:03<18:55,  2.31it/s] 36%|███▌      | 1477/4096 [12:03<18:55,  2.31it/s] 36%|███▌      | 1478/4096 [12:03<18:55,  2.31it/s] 36%|███▌      | 1479/4096 [12:04<18:54,  2.31it/s] 36%|███▌      | 1480/4096 [12:04<18:53,  2.31it/s] 36%|███▌      | 1481/4096 [12:05<18:54,  2.30it/s] 36%|███▌      | 1482/4096 [12:05<18:54,  2.30it/s] 36%|███▌      | 1483/4096 [12:06<18:53,  2.30it/s] 36%|███▌      | 1484/4096 [12:06<18:52,  2.31it/s] 36%|███▋      | 1485/4096 [12:06<18:52,  2.31it/s] 36%|███▋      | 1486/4096 [12:07<18:52,  2.31it/s] 36%|███▋      | 1487/4096 [12:07<18:51,  2.31it/s] 36%|███▋      | 1488/4096 [12:08<18:47,  2.31it/s] 36%|███▋      | 1489/4096 [12:08<18:47,  2.31it/s] 36%|███▋      | 1490/4096 [12:09<18:47,  2.31it/s] 36%|███▋      | 1491/4096 [12:09<18:47,  2.31it/s] 36%|███▋      | 1492/4096 [12:09<18:47,  2.31it/s] 36%|███▋      | 1493/4096 [12:10<18:46,  2.31it/s] 36%|███▋      | 1494/4096 [12:10<18:50,  2.30it/s] 36%|███▋      | 1495/4096 [12:11<18:50,  2.30it/s] 37%|███▋      | 1496/4096 [12:11<18:48,  2.30it/s] 37%|███▋      | 1497/4096 [12:12<18:48,  2.30it/s] 37%|███▋      | 1498/4096 [12:12<18:46,  2.31it/s] 37%|███▋      | 1499/4096 [12:13<18:46,  2.30it/s] 37%|███▋      | 1500/4096 [12:13<18:46,  2.30it/s] 37%|███▋      | 1501/4096 [12:13<18:44,  2.31it/s] 37%|███▋      | 1502/4096 [12:14<18:45,  2.31it/s] 37%|███▋      | 1503/4096 [12:14<18:43,  2.31it/s] 37%|███▋      | 1504/4096 [12:15<18:42,  2.31it/s] 37%|███▋      | 1505/4096 [12:15<18:41,  2.31it/s] 37%|███▋      | 1506/4096 [12:16<18:40,  2.31it/s] 37%|███▋      | 1507/4096 [12:16<18:39,  2.31it/s] 37%|███▋      | 1508/4096 [12:16<18:40,  2.31it/s] 37%|███▋      | 1509/4096 [12:17<18:41,  2.31it/s] 37%|███▋      | 1510/4096 [12:17<18:40,  2.31it/s] 37%|███▋      | 1511/4096 [12:18<18:40,  2.31it/s] 37%|███▋      | 1512/4096 [12:18<18:41,  2.30it/s] 37%|███▋      | 1513/4096 [12:19<18:41,  2.30it/s] 37%|███▋      | 1514/4096 [12:19<18:41,  2.30it/s] 37%|███▋      | 1515/4096 [12:19<18:40,  2.30it/s] 37%|███▋      | 1516/4096 [12:20<18:39,  2.30it/s] 37%|███▋      | 1517/4096 [12:20<18:38,  2.30it/s] 37%|███▋      | 1518/4096 [12:21<18:38,  2.30it/s] 37%|███▋      | 1519/4096 [12:21<18:39,  2.30it/s] 37%|███▋      | 1520/4096 [12:22<18:37,  2.30it/s] 37%|███▋      | 1521/4096 [12:22<18:37,  2.30it/s] 37%|███▋      | 1522/4096 [12:22<18:36,  2.31it/s] 37%|███▋      | 1523/4096 [12:23<18:35,  2.31it/s] 37%|███▋      | 1524/4096 [12:23<18:35,  2.31it/s] 37%|███▋      | 1525/4096 [12:24<18:33,  2.31it/s] 37%|███▋      | 1526/4096 [12:24<18:32,  2.31it/s] 37%|███▋      | 1527/4096 [12:25<18:33,  2.31it/s] 37%|███▋      | 1528/4096 [12:25<18:32,  2.31it/s] 37%|███▋      | 1529/4096 [12:26<18:32,  2.31it/s] 37%|███▋      | 1530/4096 [12:26<18:33,  2.31it/s] 37%|███▋      | 1531/4096 [12:26<18:33,  2.30it/s] 37%|███▋      | 1532/4096 [12:27<18:33,  2.30it/s] 37%|███▋      | 1533/4096 [12:27<18:32,  2.30it/s] 37%|███▋      | 1534/4096 [12:28<18:31,  2.30it/s] 37%|███▋      | 1535/4096 [12:28<18:30,  2.31it/s] 38%|███▊      | 1536/4096 [12:28<15:15,  2.80it/s]loss: 1.598706841468811
===========================
epoch 12/32 | loss: 1.598706841468811
---------------------------
example true genres: 
tensor([0, 0, 1, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 1, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.936542669584245
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['The drummer flogged the gourd with frantic intensity as the dancers began the calinda, a sensual gyration which had long been a favorite of voodoo practitioners and their disciples in the Louisiana slave compounds. The dance was of Haitian origin. The white girl with the penetrating green eyes sipped the lemonade handed to her by a handsome man of about 30, who had coppery skin and beetling eyebrows.', "` ` You intend to speak with Mr. Roy''?? ` ` What else''?? Waddell asked.", "No matter that the Katanga operation is strategically insane in terms of Western interests in Africa. ( Even granted that the Congo should be unified, you don't protect Western security by first removing the pro - Western weight from the power equilibrium. )", 'The unabashed sexuality of so many of his paintings was not the only thing that kept the public at bay : his view of the world was one of almost unrelieved tragedy, and it was too much even for morbid - minded Vienna. He was obsessed by disease and poverty, by the melancholy of old age and the tyranny of lust. The children he painted were almost always in rags, his portraits were often ruthless to the point of ugliness,', 'A signal?? Matsuo lifted his rifle, easing the sling under his left upper arm for steadiness. Fresh on his mind were events of the past day when his whole regiment was destroyed in the hills.']
---------------------------
example output paragraph: 
['The man wasogged the manrillad of thely. he men were to dancedenceas, and manitylam of of was been been known popular of theacs. the own. the wild countryside market. The man was a a style, dance man was a g eyes eyes and the waters in her her. the small man who a ten, 000 was been hair hair. tant the.', "` ` `'to do of me. K''?? ` ` What'' '??hiskydy asked.", "The, how the situationangaanga has a important. the of the domination. the. The The the the the United is be a by the can't want the interests. the - the threatto Western powers of the West of.", 'The manbearableshed, was the - people us life, the quite only thing that he the imagination imagination the. the own of the world, the of the nothingconentved,. and the was not much. to theushbidly - men. The was a with the and the, and the fact of the men. the illravenny of the, fact of had with not nothing forgotten thes, and own were the distorted. destroy young that the.ness. and', 'He few was? Heee had his rifle from andasing the riflelug of the shoulder shoulder arm. alugy. Hely his feet, the that the past year. he father body was in. the open.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.54it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.83it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.76it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.93it/s][A100%|██████████| 29/29 [00:00<00:00, 65.98it/s]
Mean Perplexity: 1086.7071488962654
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 38%|███▊      | 1537/4096 [12:35<1:38:08,  2.30s/it] 38%|███▊      | 1538/4096 [12:36<1:14:16,  1.74s/it] 38%|███▊      | 1539/4096 [12:36<57:30,  1.35s/it]   38%|███▊      | 1540/4096 [12:36<45:48,  1.08s/it] 38%|███▊      | 1541/4096 [12:37<37:34,  1.13it/s] 38%|███▊      | 1542/4096 [12:37<31:47,  1.34it/s] 38%|███▊      | 1543/4096 [12:38<27:54,  1.52it/s] 38%|███▊      | 1544/4096 [12:38<25:02,  1.70it/s] 38%|███▊      | 1545/4096 [12:39<23:01,  1.85it/s] 38%|███▊      | 1546/4096 [12:39<21:36,  1.97it/s] 38%|███▊      | 1547/4096 [12:39<20:37,  2.06it/s] 38%|███▊      | 1548/4096 [12:40<19:56,  2.13it/s] 38%|███▊      | 1549/4096 [12:40<19:28,  2.18it/s] 38%|███▊      | 1550/4096 [12:41<19:08,  2.22it/s] 38%|███▊      | 1551/4096 [12:41<18:55,  2.24it/s] 38%|███▊      | 1552/4096 [12:42<18:45,  2.26it/s] 38%|███▊      | 1553/4096 [12:42<18:39,  2.27it/s] 38%|███▊      | 1554/4096 [12:43<18:34,  2.28it/s] 38%|███▊      | 1555/4096 [12:43<18:30,  2.29it/s] 38%|███▊      | 1556/4096 [12:43<18:28,  2.29it/s] 38%|███▊      | 1557/4096 [12:44<18:26,  2.29it/s] 38%|███▊      | 1558/4096 [12:44<18:24,  2.30it/s] 38%|███▊      | 1559/4096 [12:45<18:21,  2.30it/s] 38%|███▊      | 1560/4096 [12:45<18:20,  2.30it/s] 38%|███▊      | 1561/4096 [12:46<18:20,  2.30it/s] 38%|███▊      | 1562/4096 [12:46<18:17,  2.31it/s] 38%|███▊      | 1563/4096 [12:46<18:17,  2.31it/s] 38%|███▊      | 1564/4096 [12:47<18:17,  2.31it/s] 38%|███▊      | 1565/4096 [12:47<18:16,  2.31it/s] 38%|███▊      | 1566/4096 [12:48<18:14,  2.31it/s] 38%|███▊      | 1567/4096 [12:48<18:15,  2.31it/s] 38%|███▊      | 1568/4096 [12:49<18:15,  2.31it/s] 38%|███▊      | 1569/4096 [12:49<18:15,  2.31it/s] 38%|███▊      | 1570/4096 [12:49<18:16,  2.30it/s] 38%|███▊      | 1571/4096 [12:50<18:15,  2.30it/s] 38%|███▊      | 1572/4096 [12:50<18:15,  2.30it/s] 38%|███▊      | 1573/4096 [12:51<18:14,  2.30it/s] 38%|███▊      | 1574/4096 [12:51<18:14,  2.30it/s] 38%|███▊      | 1575/4096 [12:52<18:14,  2.30it/s] 38%|███▊      | 1576/4096 [12:52<18:15,  2.30it/s] 39%|███▊      | 1577/4096 [12:53<18:14,  2.30it/s] 39%|███▊      | 1578/4096 [12:53<18:13,  2.30it/s] 39%|███▊      | 1579/4096 [12:53<18:12,  2.30it/s] 39%|███▊      | 1580/4096 [12:54<18:12,  2.30it/s] 39%|███▊      | 1581/4096 [12:54<18:10,  2.31it/s] 39%|███▊      | 1582/4096 [12:55<18:11,  2.30it/s] 39%|███▊      | 1583/4096 [12:55<18:12,  2.30it/s] 39%|███▊      | 1584/4096 [12:56<18:12,  2.30it/s] 39%|███▊      | 1585/4096 [12:56<18:11,  2.30it/s] 39%|███▊      | 1586/4096 [12:56<18:09,  2.30it/s] 39%|███▊      | 1587/4096 [12:57<18:08,  2.31it/s] 39%|███▉      | 1588/4096 [12:57<18:09,  2.30it/s] 39%|███▉      | 1589/4096 [12:58<18:09,  2.30it/s] 39%|███▉      | 1590/4096 [12:58<18:08,  2.30it/s] 39%|███▉      | 1591/4096 [12:59<18:09,  2.30it/s] 39%|███▉      | 1592/4096 [12:59<18:07,  2.30it/s] 39%|███▉      | 1593/4096 [12:59<18:09,  2.30it/s] 39%|███▉      | 1594/4096 [13:00<18:09,  2.30it/s] 39%|███▉      | 1595/4096 [13:00<18:08,  2.30it/s] 39%|███▉      | 1596/4096 [13:01<18:07,  2.30it/s] 39%|███▉      | 1597/4096 [13:01<18:05,  2.30it/s] 39%|███▉      | 1598/4096 [13:02<18:05,  2.30it/s] 39%|███▉      | 1599/4096 [13:02<18:04,  2.30it/s] 39%|███▉      | 1600/4096 [13:02<18:02,  2.31it/s] 39%|███▉      | 1601/4096 [13:03<18:01,  2.31it/s] 39%|███▉      | 1602/4096 [13:03<18:01,  2.31it/s] 39%|███▉      | 1603/4096 [13:04<18:00,  2.31it/s] 39%|███▉      | 1604/4096 [13:04<18:00,  2.31it/s] 39%|███▉      | 1605/4096 [13:05<17:57,  2.31it/s] 39%|███▉      | 1606/4096 [13:05<17:58,  2.31it/s] 39%|███▉      | 1607/4096 [13:06<17:58,  2.31it/s] 39%|███▉      | 1608/4096 [13:06<17:57,  2.31it/s] 39%|███▉      | 1609/4096 [13:06<17:56,  2.31it/s] 39%|███▉      | 1610/4096 [13:07<17:57,  2.31it/s] 39%|███▉      | 1611/4096 [13:07<17:58,  2.30it/s] 39%|███▉      | 1612/4096 [13:08<17:57,  2.31it/s] 39%|███▉      | 1613/4096 [13:08<17:58,  2.30it/s] 39%|███▉      | 1614/4096 [13:09<17:57,  2.30it/s] 39%|███▉      | 1615/4096 [13:09<17:57,  2.30it/s] 39%|███▉      | 1616/4096 [13:09<17:56,  2.30it/s] 39%|███▉      | 1617/4096 [13:10<17:57,  2.30it/s] 40%|███▉      | 1618/4096 [13:10<17:56,  2.30it/s] 40%|███▉      | 1619/4096 [13:11<17:56,  2.30it/s] 40%|███▉      | 1620/4096 [13:11<17:55,  2.30it/s] 40%|███▉      | 1621/4096 [13:12<17:55,  2.30it/s] 40%|███▉      | 1622/4096 [13:12<17:54,  2.30it/s] 40%|███▉      | 1623/4096 [13:12<17:55,  2.30it/s] 40%|███▉      | 1624/4096 [13:13<17:56,  2.30it/s] 40%|███▉      | 1625/4096 [13:13<17:56,  2.29it/s] 40%|███▉      | 1626/4096 [13:14<17:56,  2.29it/s] 40%|███▉      | 1627/4096 [13:14<17:55,  2.30it/s] 40%|███▉      | 1628/4096 [13:15<17:52,  2.30it/s] 40%|███▉      | 1629/4096 [13:15<17:51,  2.30it/s] 40%|███▉      | 1630/4096 [13:16<17:50,  2.30it/s] 40%|███▉      | 1631/4096 [13:16<17:50,  2.30it/s] 40%|███▉      | 1632/4096 [13:16<17:51,  2.30it/s] 40%|███▉      | 1633/4096 [13:17<17:51,  2.30it/s] 40%|███▉      | 1634/4096 [13:17<17:49,  2.30it/s] 40%|███▉      | 1635/4096 [13:18<17:48,  2.30it/s] 40%|███▉      | 1636/4096 [13:18<17:46,  2.31it/s] 40%|███▉      | 1637/4096 [13:19<17:46,  2.31it/s] 40%|███▉      | 1638/4096 [13:19<17:45,  2.31it/s] 40%|████      | 1639/4096 [13:19<17:45,  2.31it/s] 40%|████      | 1640/4096 [13:20<17:45,  2.30it/s] 40%|████      | 1641/4096 [13:20<17:43,  2.31it/s] 40%|████      | 1642/4096 [13:21<17:42,  2.31it/s] 40%|████      | 1643/4096 [13:21<17:41,  2.31it/s] 40%|████      | 1644/4096 [13:22<17:40,  2.31it/s] 40%|████      | 1645/4096 [13:22<17:42,  2.31it/s] 40%|████      | 1646/4096 [13:22<17:42,  2.31it/s] 40%|████      | 1647/4096 [13:23<17:40,  2.31it/s] 40%|████      | 1648/4096 [13:23<17:42,  2.30it/s] 40%|████      | 1649/4096 [13:24<17:41,  2.30it/s] 40%|████      | 1650/4096 [13:24<17:41,  2.30it/s] 40%|████      | 1651/4096 [13:25<17:40,  2.30it/s] 40%|████      | 1652/4096 [13:25<17:41,  2.30it/s] 40%|████      | 1653/4096 [13:25<17:40,  2.30it/s] 40%|████      | 1654/4096 [13:26<17:42,  2.30it/s] 40%|████      | 1655/4096 [13:26<17:43,  2.30it/s] 40%|████      | 1656/4096 [13:27<17:41,  2.30it/s] 40%|████      | 1657/4096 [13:27<17:39,  2.30it/s] 40%|████      | 1658/4096 [13:28<17:39,  2.30it/s] 41%|████      | 1659/4096 [13:28<17:38,  2.30it/s] 41%|████      | 1660/4096 [13:29<17:40,  2.30it/s] 41%|████      | 1661/4096 [13:29<17:41,  2.29it/s] 41%|████      | 1662/4096 [13:29<17:41,  2.29it/s] 41%|████      | 1663/4096 [13:30<17:40,  2.29it/s] 41%|████      | 1664/4096 [13:30<14:33,  2.78it/s]loss: 2.905125141143799
===========================
epoch 13/32 | loss: 2.905125141143799
---------------------------
example true genres: 
tensor([0, 0, 1, 1, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 1, 1, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9321663019693655
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["He was in his early forties, rather short and very compactly built, and with a manner that was reserved and stiff despite his efforts to adapt himself to American ways. His open face seemed to promise a sort of innocence, until one looked into his eyes, which had no warmth in them but only alert intelligence. Waddell had heard that he had been a commando in Rommel's Afrika Corps, and he said to himself : I'd hate to", 'Then it would be a choice between starvation and suicide. Whichever the way, he would rot in this vast choking green, his wife never to receive an urn of his ashes. He sighed and leaned for a moment against the trunk.', "Referring to Britain, he says, ` ` We see a nation that traditionally values sovereignty above all else willing to give up its economy, placing this authority in Continental hands''. Since the goal of our international planners is a World Government, this Atlantic Community would mark a giant step in that direction for, once American economic autonomy is absorbed, a larger grouping is a question of time. Frankly, it is being very cleverly done for, in a sense, they have", "These never ceased to suggest that if, in the eyes of Marx and Lenin ` ` full communism'' was still a very distant ideal, the establishment of a Communist society had now, under Khrushchev, become an ` ` immediate and tangible reality''. It seems that Khrushchev himself took a very special pride in having made a world - shaking contribution to Marxist doctrine with his Draft Program ( a large part of his twelve - hour speech at the recent Congress was", 'But he painted some of the boldest and most original pictures of his time, and even after nearly half a century, the tense, tormented world he put on canvas has lost none of its fascination. The devil himself. The son of a railway stationmaster, Schiele lived most of his childhood in the drowsy Danubian town of Tulln, 14 miles northwest of Vienna.']
---------------------------
example output paragraph: 
['He was not the mind stagesies, and than, very much.,. and he a limp of he a for un, his height. be to. the politics. He eyes eyes was to be nothing good of peace. but he of up his eyes, he he been doubt, his. his aness.hiskydy was been the the was been in manant, theomomer, s employ.roner, who he had, himself, ` am m heard to be', 'He, was be a burden. the and a. The was was fate, he had have out the place cavern place, the heart. saw see a answerrgen. meat own. He had, then against the moment, the wall,', "Thequestring the the, the is, that ` ` are the good of is has the, the other. to do up the own to and the one on the Europe. '. The the United of the nation union is, to priority Organization of it is is is be the new leap in the the of the and the citizens development, to by the new role of to priority of the. has has who is a considered difficult,,. the but the way of are no", "The were met to be that the the the the end of the, the, ` ` of, ', the not matter difficult possibility of the Soviet of the new party in been been and thehrushchev'was a independent ` ` threat vitalgible, of '. The was that thehrushchev'was the position difficult role in the his a certain of one attitude to the ideals. the ownage of the new scale of the own - year -, the Soviet Congress. in", 'The the was the of the art lines scenes beautiful ofs of the own. and the the the a the century, he firsts uned, of had up on. been its of the beauty. He man is is man of the man company,, whohuefr in years all life days the citywarhen countrytownllo,. Berlinsca,, was - from of the.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.52it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.22it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.41it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.27it/s][A100%|██████████| 29/29 [00:00<00:00, 66.34it/s]
Mean Perplexity: 1108.305123427243
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 41%|████      | 1665/4096 [13:37<1:33:58,  2.32s/it] 41%|████      | 1666/4096 [13:37<1:10:59,  1.75s/it] 41%|████      | 1667/4096 [13:38<54:55,  1.36s/it]   41%|████      | 1668/4096 [13:38<43:41,  1.08s/it] 41%|████      | 1669/4096 [13:39<35:49,  1.13it/s] 41%|████      | 1670/4096 [13:39<30:26,  1.33it/s] 41%|████      | 1671/4096 [13:40<26:32,  1.52it/s] 41%|████      | 1672/4096 [13:40<23:48,  1.70it/s] 41%|████      | 1673/4096 [13:40<21:53,  1.84it/s] 41%|████      | 1674/4096 [13:41<20:32,  1.97it/s] 41%|████      | 1675/4096 [13:41<19:36,  2.06it/s] 41%|████      | 1676/4096 [13:42<18:57,  2.13it/s] 41%|████      | 1677/4096 [13:42<18:29,  2.18it/s] 41%|████      | 1678/4096 [13:43<18:11,  2.21it/s] 41%|████      | 1679/4096 [13:43<17:58,  2.24it/s] 41%|████      | 1680/4096 [13:43<17:48,  2.26it/s] 41%|████      | 1681/4096 [13:44<17:41,  2.28it/s] 41%|████      | 1682/4096 [13:44<17:35,  2.29it/s] 41%|████      | 1683/4096 [13:45<17:32,  2.29it/s] 41%|████      | 1684/4096 [13:45<17:30,  2.30it/s] 41%|████      | 1685/4096 [13:46<17:30,  2.30it/s] 41%|████      | 1686/4096 [13:46<17:30,  2.29it/s] 41%|████      | 1687/4096 [13:46<17:29,  2.29it/s] 41%|████      | 1688/4096 [13:47<17:27,  2.30it/s] 41%|████      | 1689/4096 [13:47<17:27,  2.30it/s] 41%|████▏     | 1690/4096 [13:48<17:26,  2.30it/s] 41%|████▏     | 1691/4096 [13:48<17:25,  2.30it/s] 41%|████▏     | 1692/4096 [13:49<17:24,  2.30it/s] 41%|████▏     | 1693/4096 [13:49<17:22,  2.30it/s] 41%|████▏     | 1694/4096 [13:50<17:22,  2.30it/s] 41%|████▏     | 1695/4096 [13:50<17:22,  2.30it/s] 41%|████▏     | 1696/4096 [13:50<17:23,  2.30it/s] 41%|████▏     | 1697/4096 [13:51<17:25,  2.30it/s] 41%|████▏     | 1698/4096 [13:51<17:23,  2.30it/s] 41%|████▏     | 1699/4096 [13:52<17:22,  2.30it/s] 42%|████▏     | 1700/4096 [13:52<17:20,  2.30it/s] 42%|████▏     | 1701/4096 [13:53<17:20,  2.30it/s] 42%|████▏     | 1702/4096 [13:53<17:19,  2.30it/s] 42%|████▏     | 1703/4096 [13:53<17:18,  2.30it/s] 42%|████▏     | 1704/4096 [13:54<17:16,  2.31it/s] 42%|████▏     | 1705/4096 [13:54<17:17,  2.31it/s] 42%|████▏     | 1706/4096 [13:55<17:15,  2.31it/s] 42%|████▏     | 1707/4096 [13:55<17:16,  2.30it/s] 42%|████▏     | 1708/4096 [13:56<17:17,  2.30it/s] 42%|████▏     | 1709/4096 [13:56<17:16,  2.30it/s] 42%|████▏     | 1710/4096 [13:56<17:15,  2.30it/s] 42%|████▏     | 1711/4096 [13:57<17:14,  2.31it/s] 42%|████▏     | 1712/4096 [13:57<17:13,  2.31it/s] 42%|████▏     | 1713/4096 [13:58<17:14,  2.30it/s] 42%|████▏     | 1714/4096 [13:58<17:14,  2.30it/s] 42%|████▏     | 1715/4096 [13:59<17:14,  2.30it/s] 42%|████▏     | 1716/4096 [13:59<17:12,  2.30it/s] 42%|████▏     | 1717/4096 [13:59<17:13,  2.30it/s] 42%|████▏     | 1718/4096 [14:00<17:15,  2.30it/s] 42%|████▏     | 1719/4096 [14:00<17:16,  2.29it/s] 42%|████▏     | 1720/4096 [14:01<17:15,  2.29it/s] 42%|████▏     | 1721/4096 [14:01<17:12,  2.30it/s] 42%|████▏     | 1722/4096 [14:02<17:13,  2.30it/s] 42%|████▏     | 1723/4096 [14:02<17:12,  2.30it/s] 42%|████▏     | 1724/4096 [14:03<17:11,  2.30it/s] 42%|████▏     | 1725/4096 [14:03<17:11,  2.30it/s] 42%|████▏     | 1726/4096 [14:03<17:11,  2.30it/s] 42%|████▏     | 1727/4096 [14:04<17:11,  2.30it/s] 42%|████▏     | 1728/4096 [14:04<17:11,  2.30it/s] 42%|████▏     | 1729/4096 [14:05<17:10,  2.30it/s] 42%|████▏     | 1730/4096 [14:05<17:09,  2.30it/s] 42%|████▏     | 1731/4096 [14:06<17:08,  2.30it/s] 42%|████▏     | 1732/4096 [14:06<17:07,  2.30it/s] 42%|████▏     | 1733/4096 [14:06<17:07,  2.30it/s] 42%|████▏     | 1734/4096 [14:07<17:06,  2.30it/s] 42%|████▏     | 1735/4096 [14:07<17:05,  2.30it/s] 42%|████▏     | 1736/4096 [14:08<17:04,  2.30it/s] 42%|████▏     | 1737/4096 [14:08<17:04,  2.30it/s] 42%|████▏     | 1738/4096 [14:09<17:05,  2.30it/s] 42%|████▏     | 1739/4096 [14:09<17:05,  2.30it/s] 42%|████▏     | 1740/4096 [14:10<17:05,  2.30it/s] 43%|████▎     | 1741/4096 [14:10<17:04,  2.30it/s] 43%|████▎     | 1742/4096 [14:10<17:03,  2.30it/s] 43%|████▎     | 1743/4096 [14:11<17:04,  2.30it/s] 43%|████▎     | 1744/4096 [14:11<17:03,  2.30it/s] 43%|████▎     | 1745/4096 [14:12<17:04,  2.30it/s] 43%|████▎     | 1746/4096 [14:12<17:03,  2.30it/s] 43%|████▎     | 1747/4096 [14:13<17:03,  2.30it/s] 43%|████▎     | 1748/4096 [14:13<17:03,  2.29it/s] 43%|████▎     | 1749/4096 [14:13<17:01,  2.30it/s] 43%|████▎     | 1750/4096 [14:14<17:00,  2.30it/s] 43%|████▎     | 1751/4096 [14:14<17:00,  2.30it/s] 43%|████▎     | 1752/4096 [14:15<17:00,  2.30it/s] 43%|████▎     | 1753/4096 [14:15<16:59,  2.30it/s] 43%|████▎     | 1754/4096 [14:16<16:58,  2.30it/s] 43%|████▎     | 1755/4096 [14:16<16:57,  2.30it/s] 43%|████▎     | 1756/4096 [14:16<16:56,  2.30it/s] 43%|████▎     | 1757/4096 [14:17<16:56,  2.30it/s] 43%|████▎     | 1758/4096 [14:17<16:55,  2.30it/s] 43%|████▎     | 1759/4096 [14:18<16:55,  2.30it/s] 43%|████▎     | 1760/4096 [14:18<16:54,  2.30it/s] 43%|████▎     | 1761/4096 [14:19<16:53,  2.30it/s] 43%|████▎     | 1762/4096 [14:19<16:53,  2.30it/s] 43%|████▎     | 1763/4096 [14:20<16:53,  2.30it/s] 43%|████▎     | 1764/4096 [14:20<16:53,  2.30it/s] 43%|████▎     | 1765/4096 [14:20<16:53,  2.30it/s] 43%|████▎     | 1766/4096 [14:21<16:54,  2.30it/s] 43%|████▎     | 1767/4096 [14:21<16:55,  2.29it/s] 43%|████▎     | 1768/4096 [14:22<16:53,  2.30it/s] 43%|████▎     | 1769/4096 [14:22<16:53,  2.30it/s] 43%|████▎     | 1770/4096 [14:23<16:52,  2.30it/s] 43%|████▎     | 1771/4096 [14:23<16:51,  2.30it/s] 43%|████▎     | 1772/4096 [14:23<16:52,  2.30it/s] 43%|████▎     | 1773/4096 [14:24<16:50,  2.30it/s] 43%|████▎     | 1774/4096 [14:24<16:48,  2.30it/s] 43%|████▎     | 1775/4096 [14:25<16:48,  2.30it/s] 43%|████▎     | 1776/4096 [14:25<16:48,  2.30it/s] 43%|████▎     | 1777/4096 [14:26<16:47,  2.30it/s] 43%|████▎     | 1778/4096 [14:26<16:48,  2.30it/s] 43%|████▎     | 1779/4096 [14:26<16:48,  2.30it/s] 43%|████▎     | 1780/4096 [14:27<16:48,  2.30it/s] 43%|████▎     | 1781/4096 [14:27<16:46,  2.30it/s] 44%|████▎     | 1782/4096 [14:28<16:46,  2.30it/s] 44%|████▎     | 1783/4096 [14:28<16:45,  2.30it/s] 44%|████▎     | 1784/4096 [14:29<16:45,  2.30it/s] 44%|████▎     | 1785/4096 [14:29<16:45,  2.30it/s] 44%|████▎     | 1786/4096 [14:30<16:43,  2.30it/s] 44%|████▎     | 1787/4096 [14:30<16:43,  2.30it/s] 44%|████▎     | 1788/4096 [14:30<16:43,  2.30it/s] 44%|████▎     | 1789/4096 [14:31<16:43,  2.30it/s] 44%|████▎     | 1790/4096 [14:31<16:42,  2.30it/s] 44%|████▎     | 1791/4096 [14:32<16:41,  2.30it/s] 44%|████▍     | 1792/4096 [14:32<13:44,  2.79it/s]loss: 2.4055330753326416
===========================
epoch 14/32 | loss: 2.4055330753326416
---------------------------
example true genres: 
tensor([0, 1, 1, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 1, 1, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.925601750547046
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["` ` I'd druther stay here and watch the girls'', Charles grinned. ` ` Maybe some of'em will fall down and we'll see up their dress''. ` ` Maybe'', Jack said idly, watching for Miss Langford.", "The designation of five Catholic theologians to attend the World Council of Churches assembly in New Delhi as ` ` official'' observers reverses the Church's earlier stand. The public appeal by the new Vatican Secretary of State, Cardinal Cicognani, for renewed efforts toward Eastern and Western reunion was still another remarkable act. Nor can one forget Pope John's unprecedented meeting with the Archbishop of Canterbury.", "Can any Christian fail to respond to these words?? The budget deficitthe administration's official budget review, which estimates a 6. 9 billion dollar deficit for the current fiscal year, isn't making anyone happy.", "Cried one professor after a few months of Student Schiele's tantrums and rebellion : ` ` The devil himself must have defecated you into my classroom''!! For a while his work was influenced deeply by the French impressionists, and by the patterned, mosaic - like paintings of Gustav Klimt, then the dean of Austrian art. Gradually Schiele evolved a somber style of his own - - and he had few inhibition", 'Fear and relief mingled in his churning emotions. He pressed his palms together and addressed himself to the patron saint of divers in a hurried and anxious whisper. ` ` Blessed Saint Nicholas, I thank thee for getting me out of that mess and sending me up instead of down when I was bewildered.']
---------------------------
example output paragraph: 
["` ` You'll like likeck'''wait me horses'''he said. ` ` I youthin me em'be'on fall'll be'' horses''. ` ` I you ', he said.ly. ` him a Bwell '", "The editor of the of priestss, the the conference Congress of the in of the York. a ` ` meeting '.. the the statement in s s statement on The United opinion to the United President Council General State, Mr Josephurtreno, has the scrutiny to the Europe European Europe in to not matter step of the it be that Bon V s letter visit of the United of Jerusalem,", "The the other Christian to prove to the demands.? President cuts is ` is s budget budget is is which has the great million 1 million dollars.. the new budget year. is't it it think.", "Theurt the of, another few minutes, time 'umefr s deathningum, thes ` ` I editor'' be beenerted the in a own, '.! a time, part was a by by the c Revolution of of the the the influence of of thes headed, of the Vhrski, and the first of the Austria schooluiually,huefr into new -ly of the own own - - in was been words thes", "Heing the,gling with his heartores thoughts. He felt his lips against together he him to the manizing. theity the deep manner weary,. ` I Jesus Mary'he know you for you him to of the mountain of I him to to of a here I was ahold."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.85it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.97it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.01it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.23it/s][A100%|██████████| 29/29 [00:00<00:00, 66.24it/s]
Mean Perplexity: 1158.33447439248
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 44%|████▍     | 1793/4096 [14:39<1:26:46,  2.26s/it] 44%|████▍     | 1794/4096 [14:39<1:06:14,  1.73s/it] 44%|████▍     | 1795/4096 [14:39<51:21,  1.34s/it]   44%|████▍     | 1796/4096 [14:40<40:54,  1.07s/it] 44%|████▍     | 1797/4096 [14:40<33:36,  1.14it/s] 44%|████▍     | 1798/4096 [14:41<28:31,  1.34it/s] 44%|████▍     | 1799/4096 [14:41<24:55,  1.54it/s] 44%|████▍     | 1800/4096 [14:42<22:26,  1.71it/s] 44%|████▍     | 1801/4096 [14:42<20:40,  1.85it/s] 44%|████▍     | 1802/4096 [14:43<19:27,  1.97it/s] 44%|████▍     | 1803/4096 [14:43<18:33,  2.06it/s] 44%|████▍     | 1804/4096 [14:43<17:56,  2.13it/s] 44%|████▍     | 1805/4096 [14:44<17:32,  2.18it/s] 44%|████▍     | 1806/4096 [14:44<17:14,  2.21it/s] 44%|████▍     | 1807/4096 [14:45<17:02,  2.24it/s] 44%|████▍     | 1808/4096 [14:45<16:53,  2.26it/s] 44%|████▍     | 1809/4096 [14:46<16:48,  2.27it/s] 44%|████▍     | 1810/4096 [14:46<16:44,  2.28it/s] 44%|████▍     | 1811/4096 [14:46<16:40,  2.28it/s] 44%|████▍     | 1812/4096 [14:47<16:37,  2.29it/s] 44%|████▍     | 1813/4096 [14:47<16:35,  2.29it/s] 44%|████▍     | 1814/4096 [14:48<16:32,  2.30it/s] 44%|████▍     | 1815/4096 [14:48<16:31,  2.30it/s] 44%|████▍     | 1816/4096 [14:49<16:30,  2.30it/s] 44%|████▍     | 1817/4096 [14:49<16:28,  2.30it/s] 44%|████▍     | 1818/4096 [14:49<16:27,  2.31it/s] 44%|████▍     | 1819/4096 [14:50<16:27,  2.31it/s] 44%|████▍     | 1820/4096 [14:50<16:26,  2.31it/s] 44%|████▍     | 1821/4096 [14:51<16:26,  2.31it/s] 44%|████▍     | 1822/4096 [14:51<16:25,  2.31it/s] 45%|████▍     | 1823/4096 [14:52<16:25,  2.31it/s] 45%|████▍     | 1824/4096 [14:52<16:24,  2.31it/s] 45%|████▍     | 1825/4096 [14:52<16:24,  2.31it/s] 45%|████▍     | 1826/4096 [14:53<16:23,  2.31it/s] 45%|████▍     | 1827/4096 [14:53<16:24,  2.31it/s] 45%|████▍     | 1828/4096 [14:54<16:22,  2.31it/s] 45%|████▍     | 1829/4096 [14:54<16:22,  2.31it/s] 45%|████▍     | 1830/4096 [14:55<16:22,  2.31it/s] 45%|████▍     | 1831/4096 [14:55<16:22,  2.31it/s] 45%|████▍     | 1832/4096 [14:56<16:23,  2.30it/s] 45%|████▍     | 1833/4096 [14:56<16:23,  2.30it/s] 45%|████▍     | 1834/4096 [14:56<16:20,  2.31it/s] 45%|████▍     | 1835/4096 [14:57<16:21,  2.30it/s] 45%|████▍     | 1836/4096 [14:57<16:20,  2.30it/s] 45%|████▍     | 1837/4096 [14:58<16:20,  2.30it/s] 45%|████▍     | 1838/4096 [14:58<16:20,  2.30it/s] 45%|████▍     | 1839/4096 [14:59<16:20,  2.30it/s] 45%|████▍     | 1840/4096 [14:59<16:19,  2.30it/s] 45%|████▍     | 1841/4096 [14:59<16:18,  2.31it/s] 45%|████▍     | 1842/4096 [15:00<16:18,  2.30it/s] 45%|████▍     | 1843/4096 [15:00<16:19,  2.30it/s] 45%|████▌     | 1844/4096 [15:01<16:17,  2.30it/s] 45%|████▌     | 1845/4096 [15:01<16:16,  2.30it/s] 45%|████▌     | 1846/4096 [15:02<16:20,  2.30it/s] 45%|████▌     | 1847/4096 [15:02<16:18,  2.30it/s] 45%|████▌     | 1848/4096 [15:02<16:17,  2.30it/s] 45%|████▌     | 1849/4096 [15:03<16:16,  2.30it/s] 45%|████▌     | 1850/4096 [15:03<16:16,  2.30it/s] 45%|████▌     | 1851/4096 [15:04<16:17,  2.30it/s] 45%|████▌     | 1852/4096 [15:04<16:17,  2.30it/s] 45%|████▌     | 1853/4096 [15:05<16:14,  2.30it/s] 45%|████▌     | 1854/4096 [15:05<16:14,  2.30it/s] 45%|████▌     | 1855/4096 [15:06<16:12,  2.30it/s] 45%|████▌     | 1856/4096 [15:06<16:11,  2.31it/s] 45%|████▌     | 1857/4096 [15:06<16:10,  2.31it/s] 45%|████▌     | 1858/4096 [15:07<16:10,  2.31it/s] 45%|████▌     | 1859/4096 [15:07<16:09,  2.31it/s] 45%|████▌     | 1860/4096 [15:08<16:08,  2.31it/s] 45%|████▌     | 1861/4096 [15:08<16:08,  2.31it/s] 45%|████▌     | 1862/4096 [15:09<16:06,  2.31it/s] 45%|████▌     | 1863/4096 [15:09<16:07,  2.31it/s] 46%|████▌     | 1864/4096 [15:09<16:07,  2.31it/s] 46%|████▌     | 1865/4096 [15:10<16:06,  2.31it/s] 46%|████▌     | 1866/4096 [15:10<16:05,  2.31it/s] 46%|████▌     | 1867/4096 [15:11<16:05,  2.31it/s] 46%|████▌     | 1868/4096 [15:11<16:04,  2.31it/s] 46%|████▌     | 1869/4096 [15:12<16:05,  2.31it/s] 46%|████▌     | 1870/4096 [15:12<16:05,  2.30it/s] 46%|████▌     | 1871/4096 [15:12<16:04,  2.31it/s] 46%|████▌     | 1872/4096 [15:13<16:03,  2.31it/s] 46%|████▌     | 1873/4096 [15:13<16:04,  2.30it/s] 46%|████▌     | 1874/4096 [15:14<16:03,  2.31it/s] 46%|████▌     | 1875/4096 [15:14<16:04,  2.30it/s] 46%|████▌     | 1876/4096 [15:15<16:03,  2.30it/s] 46%|████▌     | 1877/4096 [15:15<16:03,  2.30it/s] 46%|████▌     | 1878/4096 [15:15<16:03,  2.30it/s] 46%|████▌     | 1879/4096 [15:16<16:03,  2.30it/s] 46%|████▌     | 1880/4096 [15:16<16:03,  2.30it/s] 46%|████▌     | 1881/4096 [15:17<16:01,  2.30it/s] 46%|████▌     | 1882/4096 [15:17<16:01,  2.30it/s] 46%|████▌     | 1883/4096 [15:18<16:01,  2.30it/s] 46%|████▌     | 1884/4096 [15:18<16:01,  2.30it/s] 46%|████▌     | 1885/4096 [15:19<16:01,  2.30it/s] 46%|████▌     | 1886/4096 [15:19<15:59,  2.30it/s] 46%|████▌     | 1887/4096 [15:19<15:58,  2.30it/s] 46%|████▌     | 1888/4096 [15:20<15:55,  2.31it/s] 46%|████▌     | 1889/4096 [15:20<15:55,  2.31it/s] 46%|████▌     | 1890/4096 [15:21<15:54,  2.31it/s] 46%|████▌     | 1891/4096 [15:21<15:54,  2.31it/s] 46%|████▌     | 1892/4096 [15:22<15:53,  2.31it/s] 46%|████▌     | 1893/4096 [15:22<15:54,  2.31it/s] 46%|████▌     | 1894/4096 [15:22<15:53,  2.31it/s] 46%|████▋     | 1895/4096 [15:23<15:52,  2.31it/s] 46%|████▋     | 1896/4096 [15:23<15:51,  2.31it/s] 46%|████▋     | 1897/4096 [15:24<15:51,  2.31it/s] 46%|████▋     | 1898/4096 [15:24<15:51,  2.31it/s] 46%|████▋     | 1899/4096 [15:25<15:51,  2.31it/s] 46%|████▋     | 1900/4096 [15:25<15:50,  2.31it/s] 46%|████▋     | 1901/4096 [15:25<15:50,  2.31it/s] 46%|████▋     | 1902/4096 [15:26<15:50,  2.31it/s] 46%|████▋     | 1903/4096 [15:26<15:49,  2.31it/s] 46%|████▋     | 1904/4096 [15:27<15:50,  2.31it/s] 47%|████▋     | 1905/4096 [15:27<15:49,  2.31it/s] 47%|████▋     | 1906/4096 [15:28<15:48,  2.31it/s] 47%|████▋     | 1907/4096 [15:28<15:48,  2.31it/s] 47%|████▋     | 1908/4096 [15:28<15:48,  2.31it/s] 47%|████▋     | 1909/4096 [15:29<15:48,  2.31it/s] 47%|████▋     | 1910/4096 [15:29<15:46,  2.31it/s] 47%|████▋     | 1911/4096 [15:30<15:47,  2.31it/s] 47%|████▋     | 1912/4096 [15:30<15:47,  2.31it/s] 47%|████▋     | 1913/4096 [15:31<15:45,  2.31it/s] 47%|████▋     | 1914/4096 [15:31<15:46,  2.31it/s] 47%|████▋     | 1915/4096 [15:32<15:46,  2.30it/s] 47%|████▋     | 1916/4096 [15:32<15:44,  2.31it/s] 47%|████▋     | 1917/4096 [15:32<15:47,  2.30it/s] 47%|████▋     | 1918/4096 [15:33<15:46,  2.30it/s] 47%|████▋     | 1919/4096 [15:33<15:45,  2.30it/s] 47%|████▋     | 1920/4096 [15:33<12:59,  2.79it/s]loss: 3.274014472961426
===========================
epoch 15/32 | loss: 3.274014472961426
---------------------------
example true genres: 
tensor([1, 0, 1, 1, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 1, 1, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9452954048140044
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['The unabashed sexuality of so many of his paintings was not the only thing that kept the public at bay : his view of the world was one of almost unrelieved tragedy, and it was too much even for morbid - minded Vienna. He was obsessed by disease and poverty, by the melancholy of old age and the tyranny of lust. The children he painted were almost always in rags, his portraits were often ruthless to the point of ugliness,', "He was big, and filthy, and his toes stuck out of the flapping tops of his shoes. He held the black plastic kazoo lightly. ` ` Come sit'', said Feathertop, motioning them toward him.", "Can any Christian fail to respond to these words?? The budget deficitthe administration's official budget review, which estimates a 6. 9 billion dollar deficit for the current fiscal year, isn't making anyone happy.", "To whom will the generals stay loyal?? There is little doubt if they had a secret ballot, they would vote for food for their family, in place of ideological purity out on the farm. It is another question whether ` ` they'' - - or a single general, off in a corner of China, secure for a few ( galvanizing??", "And if the foreigners fighting in the Katanga Army are mercenaries then Lafayette and Von Steuben were mercenaries too, as were also the members of the Lafayette Escadrille in the early part of World War 1, and of Chennault's Flying Tigers in the early days of World War 2. Modern postal sloganIt doesn't take a Gore Vidal to tell you what's wrong with Cherokee Textile's slogan ( Pitney - Bowes Objects"]
---------------------------
example output paragraph: 
['The manbearableshed, of the - men the life, the only only thing that he the imagination from the. the own of the world, the of the allconiablyved and. and the was not much of to aortbidly - men. The was not with the and the, and the fact of the men. the illravenny of the, fact of had with not certainly incapable thes, and own were the distorted. the point that thetopianess. and', "He was a, and he, and sweating eyes, out out his coatper, of his shorts. He was his gun coat bagmmb from, He ` You on down ', he Gregather,s anding the toward the.", "The the other Christian to do to the demands.? President budget is year is s budget budget is is which has the good million 5 million dollars.. the next budget year. is't it it think.", 'The the the not President, in,? The is no hope that the are to choice secret, the will not for the, the own. and the of the propaganda, of the cold. is not possibility that the ` the are re? - is the nuclear choice who the the the war of the, is the the nuclear days -aszed )?', "The the the American are in the Westanga,, not,, has thekienn, not for to the the the the United of the United Legionstrpy Corps the United 1930s of the War I. the the course Tg, s s Corps were the Congo 1930s of the War I, The American codess ` `'t be place bitniet'be the about the s the. thes and and'`s (ch, -er,.s '"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.93it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.94it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.04it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.22it/s][A100%|██████████| 29/29 [00:00<00:00, 66.21it/s]
Mean Perplexity: 1085.4409838787053
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 47%|████▋     | 1921/4096 [15:40<1:23:29,  2.30s/it] 47%|████▋     | 1922/4096 [15:41<1:03:05,  1.74s/it] 47%|████▋     | 1923/4096 [15:41<48:51,  1.35s/it]   47%|████▋     | 1924/4096 [15:42<38:52,  1.07s/it] 47%|████▋     | 1925/4096 [15:42<31:54,  1.13it/s] 47%|████▋     | 1926/4096 [15:42<27:01,  1.34it/s] 47%|████▋     | 1927/4096 [15:43<23:35,  1.53it/s] 47%|████▋     | 1928/4096 [15:43<21:14,  1.70it/s] 47%|████▋     | 1929/4096 [15:44<19:33,  1.85it/s] 47%|████▋     | 1930/4096 [15:44<18:22,  1.96it/s] 47%|████▋     | 1931/4096 [15:45<17:33,  2.06it/s] 47%|████▋     | 1932/4096 [15:45<16:57,  2.13it/s] 47%|████▋     | 1933/4096 [15:45<16:33,  2.18it/s] 47%|████▋     | 1934/4096 [15:46<16:15,  2.22it/s] 47%|████▋     | 1935/4096 [15:46<16:05,  2.24it/s] 47%|████▋     | 1936/4096 [15:47<15:56,  2.26it/s] 47%|████▋     | 1937/4096 [15:47<15:50,  2.27it/s] 47%|████▋     | 1938/4096 [15:48<15:46,  2.28it/s] 47%|████▋     | 1939/4096 [15:48<15:43,  2.29it/s] 47%|████▋     | 1940/4096 [15:49<15:39,  2.30it/s] 47%|████▋     | 1941/4096 [15:49<15:35,  2.30it/s] 47%|████▋     | 1942/4096 [15:49<15:34,  2.30it/s] 47%|████▋     | 1943/4096 [15:50<15:32,  2.31it/s] 47%|████▋     | 1944/4096 [15:50<15:31,  2.31it/s] 47%|████▋     | 1945/4096 [15:51<15:30,  2.31it/s] 48%|████▊     | 1946/4096 [15:51<15:30,  2.31it/s] 48%|████▊     | 1947/4096 [15:52<15:31,  2.31it/s] 48%|████▊     | 1948/4096 [15:52<15:32,  2.30it/s] 48%|████▊     | 1949/4096 [15:52<15:30,  2.31it/s] 48%|████▊     | 1950/4096 [15:53<15:29,  2.31it/s] 48%|████▊     | 1951/4096 [15:53<15:30,  2.31it/s] 48%|████▊     | 1952/4096 [15:54<15:28,  2.31it/s] 48%|████▊     | 1953/4096 [15:54<15:28,  2.31it/s] 48%|████▊     | 1954/4096 [15:55<15:29,  2.30it/s] 48%|████▊     | 1955/4096 [15:55<15:29,  2.30it/s] 48%|████▊     | 1956/4096 [15:55<15:28,  2.31it/s] 48%|████▊     | 1957/4096 [15:56<15:28,  2.30it/s] 48%|████▊     | 1958/4096 [15:56<15:27,  2.30it/s] 48%|████▊     | 1959/4096 [15:57<15:26,  2.31it/s] 48%|████▊     | 1960/4096 [15:57<15:27,  2.30it/s] 48%|████▊     | 1961/4096 [15:58<15:25,  2.31it/s] 48%|████▊     | 1962/4096 [15:58<15:25,  2.31it/s] 48%|████▊     | 1963/4096 [15:58<15:25,  2.30it/s] 48%|████▊     | 1964/4096 [15:59<15:24,  2.31it/s] 48%|████▊     | 1965/4096 [15:59<15:23,  2.31it/s] 48%|████▊     | 1966/4096 [16:00<15:23,  2.31it/s] 48%|████▊     | 1967/4096 [16:00<15:23,  2.31it/s] 48%|████▊     | 1968/4096 [16:01<15:22,  2.31it/s] 48%|████▊     | 1969/4096 [16:01<15:21,  2.31it/s] 48%|████▊     | 1970/4096 [16:02<15:21,  2.31it/s] 48%|████▊     | 1971/4096 [16:02<15:20,  2.31it/s] 48%|████▊     | 1972/4096 [16:02<15:19,  2.31it/s] 48%|████▊     | 1973/4096 [16:03<15:19,  2.31it/s] 48%|████▊     | 1974/4096 [16:03<15:19,  2.31it/s] 48%|████▊     | 1975/4096 [16:04<15:18,  2.31it/s] 48%|████▊     | 1976/4096 [16:04<15:19,  2.30it/s] 48%|████▊     | 1977/4096 [16:05<15:18,  2.31it/s] 48%|████▊     | 1978/4096 [16:05<15:17,  2.31it/s] 48%|████▊     | 1979/4096 [16:05<15:17,  2.31it/s] 48%|████▊     | 1980/4096 [16:06<15:17,  2.31it/s] 48%|████▊     | 1981/4096 [16:06<15:16,  2.31it/s] 48%|████▊     | 1982/4096 [16:07<15:16,  2.31it/s] 48%|████▊     | 1983/4096 [16:07<15:16,  2.31it/s] 48%|████▊     | 1984/4096 [16:08<15:16,  2.30it/s] 48%|████▊     | 1985/4096 [16:08<15:16,  2.30it/s] 48%|████▊     | 1986/4096 [16:08<15:14,  2.31it/s] 49%|████▊     | 1987/4096 [16:09<15:13,  2.31it/s] 49%|████▊     | 1988/4096 [16:09<15:13,  2.31it/s] 49%|████▊     | 1989/4096 [16:10<15:12,  2.31it/s] 49%|████▊     | 1990/4096 [16:10<15:13,  2.31it/s] 49%|████▊     | 1991/4096 [16:11<15:12,  2.31it/s] 49%|████▊     | 1992/4096 [16:11<15:11,  2.31it/s] 49%|████▊     | 1993/4096 [16:12<15:11,  2.31it/s] 49%|████▊     | 1994/4096 [16:12<15:09,  2.31it/s] 49%|████▊     | 1995/4096 [16:12<15:09,  2.31it/s] 49%|████▊     | 1996/4096 [16:13<15:08,  2.31it/s] 49%|████▉     | 1997/4096 [16:13<15:08,  2.31it/s] 49%|████▉     | 1998/4096 [16:14<15:08,  2.31it/s] 49%|████▉     | 1999/4096 [16:14<15:08,  2.31it/s] 49%|████▉     | 2000/4096 [16:15<15:07,  2.31it/s] 49%|████▉     | 2001/4096 [16:15<15:06,  2.31it/s] 49%|████▉     | 2002/4096 [16:15<15:06,  2.31it/s] 49%|████▉     | 2003/4096 [16:16<15:05,  2.31it/s] 49%|████▉     | 2004/4096 [16:16<15:04,  2.31it/s] 49%|████▉     | 2005/4096 [16:17<15:05,  2.31it/s] 49%|████▉     | 2006/4096 [16:17<15:04,  2.31it/s] 49%|████▉     | 2007/4096 [16:18<15:04,  2.31it/s] 49%|████▉     | 2008/4096 [16:18<15:04,  2.31it/s] 49%|████▉     | 2009/4096 [16:18<15:04,  2.31it/s] 49%|████▉     | 2010/4096 [16:19<15:03,  2.31it/s] 49%|████▉     | 2011/4096 [16:19<15:03,  2.31it/s] 49%|████▉     | 2012/4096 [16:20<15:03,  2.31it/s] 49%|████▉     | 2013/4096 [16:20<15:02,  2.31it/s] 49%|████▉     | 2014/4096 [16:21<15:00,  2.31it/s] 49%|████▉     | 2015/4096 [16:21<15:01,  2.31it/s] 49%|████▉     | 2016/4096 [16:21<15:00,  2.31it/s] 49%|████▉     | 2017/4096 [16:22<15:01,  2.31it/s] 49%|████▉     | 2018/4096 [16:22<15:00,  2.31it/s] 49%|████▉     | 2019/4096 [16:23<15:00,  2.31it/s] 49%|████▉     | 2020/4096 [16:23<14:59,  2.31it/s] 49%|████▉     | 2021/4096 [16:24<14:59,  2.31it/s] 49%|████▉     | 2022/4096 [16:24<14:59,  2.31it/s] 49%|████▉     | 2023/4096 [16:24<14:57,  2.31it/s] 49%|████▉     | 2024/4096 [16:25<14:58,  2.31it/s] 49%|████▉     | 2025/4096 [16:25<14:57,  2.31it/s] 49%|████▉     | 2026/4096 [16:26<14:56,  2.31it/s] 49%|████▉     | 2027/4096 [16:26<14:56,  2.31it/s] 50%|████▉     | 2028/4096 [16:27<14:54,  2.31it/s] 50%|████▉     | 2029/4096 [16:27<14:55,  2.31it/s] 50%|████▉     | 2030/4096 [16:28<14:56,  2.31it/s] 50%|████▉     | 2031/4096 [16:28<14:55,  2.31it/s] 50%|████▉     | 2032/4096 [16:28<14:54,  2.31it/s] 50%|████▉     | 2033/4096 [16:29<14:53,  2.31it/s] 50%|████▉     | 2034/4096 [16:29<14:54,  2.31it/s] 50%|████▉     | 2035/4096 [16:30<14:54,  2.31it/s] 50%|████▉     | 2036/4096 [16:30<14:53,  2.31it/s] 50%|████▉     | 2037/4096 [16:31<14:52,  2.31it/s] 50%|████▉     | 2038/4096 [16:31<14:52,  2.31it/s] 50%|████▉     | 2039/4096 [16:31<14:53,  2.30it/s] 50%|████▉     | 2040/4096 [16:32<14:52,  2.30it/s] 50%|████▉     | 2041/4096 [16:32<14:51,  2.31it/s] 50%|████▉     | 2042/4096 [16:33<14:49,  2.31it/s] 50%|████▉     | 2043/4096 [16:33<14:50,  2.31it/s] 50%|████▉     | 2044/4096 [16:34<14:48,  2.31it/s] 50%|████▉     | 2045/4096 [16:34<14:49,  2.31it/s] 50%|████▉     | 2046/4096 [16:34<14:48,  2.31it/s] 50%|████▉     | 2047/4096 [16:35<14:48,  2.31it/s] 50%|█████     | 2048/4096 [16:35<12:12,  2.80it/s]loss: 3.290846824645996
===========================
epoch 16/32 | loss: 3.290846824645996
---------------------------
example true genres: 
tensor([0, 1, 1, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 1, 1, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9343544857768052
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["Taking aim at the man's face, Matsuo squeezed the trigger up to the point of discharge, and then he changed his mind. He wanted the arrogant marine to know fear, and so he aimed above the head. The shot reverberated in diminishing whiplashes of sound.", "The objective should be to provide a method of getting into print a higher percentage than is now possible of the relevant information in the possession of reporters and editors. Southern California blackoutI would like to see you devote some space in an early issue to the news blackout concerning President Kennedy's activities, so far as Southern California is concerned.", 'as a result buyers, the public, must be satisfied with second - rate teachers. But this is not the real problem ; ; the rub arises from the fact that teachers are usually paid on the basis of time served rather than quality.', "It is extremely doubtful that the handful of Albanians who call themselves Communists could have done this without the direct approval of their Chinese friends. The big question is whether, in the name of a restored Chinese - Soviet solidarity, the Chinese will choose to persuade the Albanians to present their humble apologies to Khrushchev - - or get rid of Enver Hoxa. These seem about the only two ways in which the ` ` unhappy incident'' can now be closed.", "Strange. At last he reached for the knife. Even the bone handle scorched, and he retrieved the marine's handkerchief to wrap it."]
---------------------------
example output paragraph: 
["He the of the gun's face. heee'the trigger and and the trigger of his. and he, hit the aim to He was to gun,, see what. but he he could his the man of man wasvgedated into the successionshinglys. the.", "The editor of be met be a good of communication the the from new rate of the the.. the public information. the United of the. the of The Illinois University andersnc be to have the in to time to the editorial area of the editor of market. the Kennedy's s. and that as theers has concerned concerned", 'The the few of are the owners can and have informed by the choice hand price. The the is not a way reason of ; publicbish from the lack that the are not not by a job of the.. than than.', "The is not important that the British of Americans are are themselves Communists, not been no right the possibility action of the own government. The United problem is that whether in fact end of the Communist country government speaking government group will United will not to accept them Communistss to join themselves own demandspologies for thehrushchev. - for not out of thegelgedjou are to to fact thing thing to which the United ` the country has '. be be the to", 'Hely He the, had the the bottle. He the cold was wastoched the the he had the guns s coat. hide it.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.49it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.07it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.28it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.21it/s][A100%|██████████| 29/29 [00:00<00:00, 66.22it/s]
Mean Perplexity: 1181.5420679639153
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 50%|█████     | 2049/4096 [16:42<1:18:08,  2.29s/it] 50%|█████     | 2050/4096 [16:42<59:05,  1.73s/it]   50%|█████     | 2051/4096 [16:43<45:47,  1.34s/it] 50%|█████     | 2052/4096 [16:43<36:28,  1.07s/it] 50%|█████     | 2053/4096 [16:44<29:57,  1.14it/s] 50%|█████     | 2054/4096 [16:44<25:23,  1.34it/s] 50%|█████     | 2055/4096 [16:44<22:11,  1.53it/s] 50%|█████     | 2056/4096 [16:45<19:57,  1.70it/s] 50%|█████     | 2057/4096 [16:45<18:22,  1.85it/s] 50%|█████     | 2058/4096 [16:46<17:15,  1.97it/s] 50%|█████     | 2059/4096 [16:46<16:28,  2.06it/s] 50%|█████     | 2060/4096 [16:47<15:54,  2.13it/s] 50%|█████     | 2061/4096 [16:47<15:33,  2.18it/s] 50%|█████     | 2062/4096 [16:48<15:15,  2.22it/s] 50%|█████     | 2063/4096 [16:48<15:05,  2.25it/s] 50%|█████     | 2064/4096 [16:48<14:56,  2.27it/s] 50%|█████     | 2065/4096 [16:49<14:52,  2.28it/s] 50%|█████     | 2066/4096 [16:49<14:48,  2.28it/s] 50%|█████     | 2067/4096 [16:50<14:45,  2.29it/s] 50%|█████     | 2068/4096 [16:50<14:42,  2.30it/s] 51%|█████     | 2069/4096 [16:51<14:41,  2.30it/s] 51%|█████     | 2070/4096 [16:51<14:39,  2.30it/s] 51%|█████     | 2071/4096 [16:51<14:37,  2.31it/s] 51%|█████     | 2072/4096 [16:52<14:37,  2.31it/s] 51%|█████     | 2073/4096 [16:52<14:35,  2.31it/s] 51%|█████     | 2074/4096 [16:53<14:35,  2.31it/s] 51%|█████     | 2075/4096 [16:53<14:35,  2.31it/s] 51%|█████     | 2076/4096 [16:54<14:34,  2.31it/s] 51%|█████     | 2077/4096 [16:54<14:35,  2.31it/s] 51%|█████     | 2078/4096 [16:54<14:33,  2.31it/s] 51%|█████     | 2079/4096 [16:55<14:33,  2.31it/s] 51%|█████     | 2080/4096 [16:55<14:32,  2.31it/s] 51%|█████     | 2081/4096 [16:56<14:32,  2.31it/s] 51%|█████     | 2082/4096 [16:56<14:32,  2.31it/s] 51%|█████     | 2083/4096 [16:57<14:31,  2.31it/s] 51%|█████     | 2084/4096 [16:57<14:31,  2.31it/s] 51%|█████     | 2085/4096 [16:57<14:29,  2.31it/s] 51%|█████     | 2086/4096 [16:58<14:29,  2.31it/s] 51%|█████     | 2087/4096 [16:58<14:30,  2.31it/s] 51%|█████     | 2088/4096 [16:59<14:28,  2.31it/s] 51%|█████     | 2089/4096 [16:59<14:29,  2.31it/s] 51%|█████     | 2090/4096 [17:00<14:28,  2.31it/s] 51%|█████     | 2091/4096 [17:00<14:27,  2.31it/s] 51%|█████     | 2092/4096 [17:00<14:26,  2.31it/s] 51%|█████     | 2093/4096 [17:01<14:26,  2.31it/s] 51%|█████     | 2094/4096 [17:01<14:25,  2.31it/s] 51%|█████     | 2095/4096 [17:02<14:25,  2.31it/s] 51%|█████     | 2096/4096 [17:02<14:25,  2.31it/s] 51%|█████     | 2097/4096 [17:03<14:24,  2.31it/s] 51%|█████     | 2098/4096 [17:03<14:25,  2.31it/s] 51%|█████     | 2099/4096 [17:04<14:24,  2.31it/s] 51%|█████▏    | 2100/4096 [17:04<14:23,  2.31it/s] 51%|█████▏    | 2101/4096 [17:04<14:24,  2.31it/s] 51%|█████▏    | 2102/4096 [17:05<14:24,  2.31it/s] 51%|█████▏    | 2103/4096 [17:05<14:23,  2.31it/s] 51%|█████▏    | 2104/4096 [17:06<14:23,  2.31it/s] 51%|█████▏    | 2105/4096 [17:06<14:22,  2.31it/s] 51%|█████▏    | 2106/4096 [17:07<14:21,  2.31it/s] 51%|█████▏    | 2107/4096 [17:07<14:21,  2.31it/s] 51%|█████▏    | 2108/4096 [17:07<14:21,  2.31it/s] 51%|█████▏    | 2109/4096 [17:08<14:20,  2.31it/s] 52%|█████▏    | 2110/4096 [17:08<14:21,  2.30it/s] 52%|█████▏    | 2111/4096 [17:09<14:20,  2.31it/s] 52%|█████▏    | 2112/4096 [17:09<14:18,  2.31it/s] 52%|█████▏    | 2113/4096 [17:10<14:19,  2.31it/s] 52%|█████▏    | 2114/4096 [17:10<14:19,  2.31it/s] 52%|█████▏    | 2115/4096 [17:10<14:19,  2.30it/s] 52%|█████▏    | 2116/4096 [17:11<14:19,  2.30it/s] 52%|█████▏    | 2117/4096 [17:11<14:19,  2.30it/s] 52%|█████▏    | 2118/4096 [17:12<14:17,  2.31it/s] 52%|█████▏    | 2119/4096 [17:12<14:15,  2.31it/s] 52%|█████▏    | 2120/4096 [17:13<14:14,  2.31it/s] 52%|█████▏    | 2121/4096 [17:13<14:13,  2.31it/s] 52%|█████▏    | 2122/4096 [17:13<14:13,  2.31it/s] 52%|█████▏    | 2123/4096 [17:14<14:14,  2.31it/s] 52%|█████▏    | 2124/4096 [17:14<14:13,  2.31it/s] 52%|█████▏    | 2125/4096 [17:15<14:13,  2.31it/s] 52%|█████▏    | 2126/4096 [17:15<14:13,  2.31it/s] 52%|█████▏    | 2127/4096 [17:16<14:11,  2.31it/s] 52%|█████▏    | 2128/4096 [17:16<14:11,  2.31it/s] 52%|█████▏    | 2129/4096 [17:17<14:11,  2.31it/s] 52%|█████▏    | 2130/4096 [17:17<14:10,  2.31it/s] 52%|█████▏    | 2131/4096 [17:17<14:09,  2.31it/s] 52%|█████▏    | 2132/4096 [17:18<14:08,  2.32it/s] 52%|█████▏    | 2133/4096 [17:18<14:07,  2.32it/s] 52%|█████▏    | 2134/4096 [17:19<14:08,  2.31it/s] 52%|█████▏    | 2135/4096 [17:19<14:07,  2.31it/s] 52%|█████▏    | 2136/4096 [17:20<14:09,  2.31it/s] 52%|█████▏    | 2137/4096 [17:20<14:10,  2.30it/s] 52%|█████▏    | 2138/4096 [17:20<14:10,  2.30it/s] 52%|█████▏    | 2139/4096 [17:21<14:09,  2.30it/s] 52%|█████▏    | 2140/4096 [17:21<14:09,  2.30it/s] 52%|█████▏    | 2141/4096 [17:22<14:08,  2.30it/s] 52%|█████▏    | 2142/4096 [17:22<14:06,  2.31it/s] 52%|█████▏    | 2143/4096 [17:23<14:05,  2.31it/s] 52%|█████▏    | 2144/4096 [17:23<14:05,  2.31it/s] 52%|█████▏    | 2145/4096 [17:23<14:04,  2.31it/s] 52%|█████▏    | 2146/4096 [17:24<14:04,  2.31it/s] 52%|█████▏    | 2147/4096 [17:24<14:03,  2.31it/s] 52%|█████▏    | 2148/4096 [17:25<14:03,  2.31it/s] 52%|█████▏    | 2149/4096 [17:25<14:03,  2.31it/s] 52%|█████▏    | 2150/4096 [17:26<14:03,  2.31it/s] 53%|█████▎    | 2151/4096 [17:26<14:02,  2.31it/s] 53%|█████▎    | 2152/4096 [17:26<14:02,  2.31it/s] 53%|█████▎    | 2153/4096 [17:27<14:00,  2.31it/s] 53%|█████▎    | 2154/4096 [17:27<14:00,  2.31it/s] 53%|█████▎    | 2155/4096 [17:28<13:59,  2.31it/s] 53%|█████▎    | 2156/4096 [17:28<13:58,  2.31it/s] 53%|█████▎    | 2157/4096 [17:29<13:59,  2.31it/s] 53%|█████▎    | 2158/4096 [17:29<13:58,  2.31it/s] 53%|█████▎    | 2159/4096 [17:30<13:58,  2.31it/s] 53%|█████▎    | 2160/4096 [17:30<13:58,  2.31it/s] 53%|█████▎    | 2161/4096 [17:30<13:57,  2.31it/s] 53%|█████▎    | 2162/4096 [17:31<13:57,  2.31it/s] 53%|█████▎    | 2163/4096 [17:31<13:56,  2.31it/s] 53%|█████▎    | 2164/4096 [17:32<13:57,  2.31it/s] 53%|█████▎    | 2165/4096 [17:32<13:58,  2.30it/s] 53%|█████▎    | 2166/4096 [17:33<13:57,  2.30it/s] 53%|█████▎    | 2167/4096 [17:33<13:57,  2.30it/s] 53%|█████▎    | 2168/4096 [17:33<13:57,  2.30it/s] 53%|█████▎    | 2169/4096 [17:34<13:56,  2.30it/s] 53%|█████▎    | 2170/4096 [17:34<13:55,  2.31it/s] 53%|█████▎    | 2171/4096 [17:35<13:54,  2.31it/s] 53%|█████▎    | 2172/4096 [17:35<13:52,  2.31it/s] 53%|█████▎    | 2173/4096 [17:36<13:51,  2.31it/s] 53%|█████▎    | 2174/4096 [17:36<13:52,  2.31it/s] 53%|█████▎    | 2175/4096 [17:36<13:52,  2.31it/s] 53%|█████▎    | 2176/4096 [17:37<11:25,  2.80it/s]loss: 2.1651663780212402
===========================
epoch 17/32 | loss: 2.1651663780212402
---------------------------
example true genres: 
tensor([0, 1, 0, 1, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 0, 1, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9234135667396062
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["` ` Yeah, seems so, don't it'', the boy laughed, hugging her close. ` ` Ah - ah''!! Feathertop interrupted, standing up, brushing the pig offal from his dirty pants.", "Off - Broadway, where production is still comparatively cheap, is proving itself only slightly more original. Laudably enough, it is offering classics and off - beat imports, but last week only one U. S. original was on the boards, Robert D. Hock's stunning Civil War work, Borak. The real trouble seems to be the failing imagination of U. S. playwrights.", "He was big, and filthy, and his toes stuck out of the flapping tops of his shoes. He held the black plastic kazoo lightly. ` ` Come sit'', said Feathertop, motioning them toward him.", "Editor's note : Reprints of ` ` Confrontation'' will be included among the material to be distributed to members of the Peace Corps. A Peace Corps official described the editorial as ` ` precisely the message we need to communicate to the men and women who will soon be Peace Corps volunteers''. Improper Bostonian??", 'For, after leaving the Army in 1956, I spent five years in Graduate School first at Boston College and then at the University of Toronto. This time, added to that which I had already spent in school prior to my induction in 1954, makes a total of twenty - two ( 22 ) years of education. The possibility of recall into the Army is part of the price that a modern American has to pay for the enviable heritage of liberty which he enjoys.']
---------------------------
example output paragraph: 
["` ` You, I you much I't you'', he girl said. ` her close. ` ` You, -, ',!atherss, ` up in and the hair, the. his coat coat.", 'The the end, the the has being going slow. the the to. a less than than Thegh,,, the is not a to other - putting, of and it year,, week. S.., released the radio of and K. Kck, s s novel War Memorial. andutse New problem is to be the fact of of the. S..s.', "He had a, and he, and sweating eyes, out out his saddleper, of his shirt. He was his gun coat bagbb from, He ` You on down ', he Gregather,s anding the toward the.", "The : s letter : `laceted : the ` `gratulationsing'', be a in the recommendations in help considered in the of the United Corps. The letter Corps Corps official the letter : : ` ` ` first of'to do. the public who the of are be be able Corps members. '. 'rovingly editorial editorial?", 'The the the the, war, the, he was the years of the School, in the University. then in Yale University of Chicago. In year was in to the of was had been been in the, to the death in the, was me sense difference one five five years ( ) years in teaching. The first of the, the college is that of the program of I person American citizen been be. the servicegravingted education of the. is was.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.22it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.42it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.54it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.96it/s][A100%|██████████| 29/29 [00:00<00:00, 65.89it/s]
Mean Perplexity: 1155.9759076241592
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 53%|█████▎    | 2177/4096 [17:43<1:13:21,  2.29s/it] 53%|█████▎    | 2178/4096 [17:44<55:27,  1.73s/it]   53%|█████▎    | 2179/4096 [17:44<43:07,  1.35s/it] 53%|█████▎    | 2180/4096 [17:45<34:18,  1.07s/it] 53%|█████▎    | 2181/4096 [17:45<28:08,  1.13it/s] 53%|█████▎    | 2182/4096 [17:46<23:50,  1.34it/s] 53%|█████▎    | 2183/4096 [17:46<20:48,  1.53it/s] 53%|█████▎    | 2184/4096 [17:46<18:41,  1.70it/s] 53%|█████▎    | 2185/4096 [17:47<17:13,  1.85it/s] 53%|█████▎    | 2186/4096 [17:47<16:09,  1.97it/s] 53%|█████▎    | 2187/4096 [17:48<15:27,  2.06it/s] 53%|█████▎    | 2188/4096 [17:48<14:56,  2.13it/s] 53%|█████▎    | 2189/4096 [17:49<14:33,  2.18it/s] 53%|█████▎    | 2190/4096 [17:49<14:18,  2.22it/s] 53%|█████▎    | 2191/4096 [17:50<14:07,  2.25it/s] 54%|█████▎    | 2192/4096 [17:50<14:00,  2.27it/s] 54%|█████▎    | 2193/4096 [17:50<13:55,  2.28it/s] 54%|█████▎    | 2194/4096 [17:51<13:50,  2.29it/s] 54%|█████▎    | 2195/4096 [17:51<13:48,  2.29it/s] 54%|█████▎    | 2196/4096 [17:52<13:46,  2.30it/s] 54%|█████▎    | 2197/4096 [17:52<13:44,  2.30it/s] 54%|█████▎    | 2198/4096 [17:53<13:43,  2.30it/s] 54%|█████▎    | 2199/4096 [17:53<13:42,  2.31it/s] 54%|█████▎    | 2200/4096 [17:53<13:41,  2.31it/s] 54%|█████▎    | 2201/4096 [17:54<13:41,  2.31it/s] 54%|█████▍    | 2202/4096 [17:54<13:40,  2.31it/s] 54%|█████▍    | 2203/4096 [17:55<13:39,  2.31it/s] 54%|█████▍    | 2204/4096 [17:55<13:38,  2.31it/s] 54%|█████▍    | 2205/4096 [17:56<13:37,  2.31it/s] 54%|█████▍    | 2206/4096 [17:56<13:37,  2.31it/s] 54%|█████▍    | 2207/4096 [17:56<13:37,  2.31it/s] 54%|█████▍    | 2208/4096 [17:57<13:38,  2.31it/s] 54%|█████▍    | 2209/4096 [17:57<13:37,  2.31it/s] 54%|█████▍    | 2210/4096 [17:58<13:37,  2.31it/s] 54%|█████▍    | 2211/4096 [17:58<13:35,  2.31it/s] 54%|█████▍    | 2212/4096 [17:59<13:35,  2.31it/s] 54%|█████▍    | 2213/4096 [17:59<13:34,  2.31it/s] 54%|█████▍    | 2214/4096 [17:59<13:33,  2.31it/s] 54%|█████▍    | 2215/4096 [18:00<13:32,  2.31it/s] 54%|█████▍    | 2216/4096 [18:00<13:32,  2.32it/s] 54%|█████▍    | 2217/4096 [18:01<13:31,  2.32it/s] 54%|█████▍    | 2218/4096 [18:01<13:31,  2.31it/s] 54%|█████▍    | 2219/4096 [18:02<13:31,  2.31it/s] 54%|█████▍    | 2220/4096 [18:02<13:31,  2.31it/s] 54%|█████▍    | 2221/4096 [18:02<13:32,  2.31it/s] 54%|█████▍    | 2222/4096 [18:03<13:31,  2.31it/s] 54%|█████▍    | 2223/4096 [18:03<13:30,  2.31it/s] 54%|█████▍    | 2224/4096 [18:04<13:30,  2.31it/s] 54%|█████▍    | 2225/4096 [18:04<13:29,  2.31it/s] 54%|█████▍    | 2226/4096 [18:05<13:30,  2.31it/s] 54%|█████▍    | 2227/4096 [18:05<13:29,  2.31it/s] 54%|█████▍    | 2228/4096 [18:06<13:29,  2.31it/s] 54%|█████▍    | 2229/4096 [18:06<13:27,  2.31it/s] 54%|█████▍    | 2230/4096 [18:06<13:27,  2.31it/s] 54%|█████▍    | 2231/4096 [18:07<13:25,  2.32it/s] 54%|█████▍    | 2232/4096 [18:07<13:25,  2.31it/s] 55%|█████▍    | 2233/4096 [18:08<13:24,  2.32it/s] 55%|█████▍    | 2234/4096 [18:08<13:24,  2.31it/s] 55%|█████▍    | 2235/4096 [18:09<13:25,  2.31it/s] 55%|█████▍    | 2236/4096 [18:09<13:25,  2.31it/s] 55%|█████▍    | 2237/4096 [18:09<13:25,  2.31it/s] 55%|█████▍    | 2238/4096 [18:10<13:25,  2.31it/s] 55%|█████▍    | 2239/4096 [18:10<13:25,  2.31it/s] 55%|█████▍    | 2240/4096 [18:11<13:25,  2.30it/s] 55%|█████▍    | 2241/4096 [18:11<13:25,  2.30it/s] 55%|█████▍    | 2242/4096 [18:12<13:24,  2.31it/s] 55%|█████▍    | 2243/4096 [18:12<13:22,  2.31it/s] 55%|█████▍    | 2244/4096 [18:12<13:22,  2.31it/s] 55%|█████▍    | 2245/4096 [18:13<13:21,  2.31it/s] 55%|█████▍    | 2246/4096 [18:13<13:21,  2.31it/s] 55%|█████▍    | 2247/4096 [18:14<13:20,  2.31it/s] 55%|█████▍    | 2248/4096 [18:14<13:20,  2.31it/s] 55%|█████▍    | 2249/4096 [18:15<13:20,  2.31it/s] 55%|█████▍    | 2250/4096 [18:15<13:20,  2.31it/s] 55%|█████▍    | 2251/4096 [18:15<13:19,  2.31it/s] 55%|█████▍    | 2252/4096 [18:16<13:18,  2.31it/s] 55%|█████▌    | 2253/4096 [18:16<13:19,  2.31it/s] 55%|█████▌    | 2254/4096 [18:17<13:18,  2.31it/s] 55%|█████▌    | 2255/4096 [18:17<13:17,  2.31it/s] 55%|█████▌    | 2256/4096 [18:18<13:17,  2.31it/s] 55%|█████▌    | 2257/4096 [18:18<13:16,  2.31it/s] 55%|█████▌    | 2258/4096 [18:19<13:15,  2.31it/s] 55%|█████▌    | 2259/4096 [18:19<13:14,  2.31it/s] 55%|█████▌    | 2260/4096 [18:19<13:13,  2.31it/s] 55%|█████▌    | 2261/4096 [18:20<13:13,  2.31it/s] 55%|█████▌    | 2262/4096 [18:20<13:13,  2.31it/s] 55%|█████▌    | 2263/4096 [18:21<13:13,  2.31it/s] 55%|█████▌    | 2264/4096 [18:21<13:13,  2.31it/s] 55%|█████▌    | 2265/4096 [18:22<13:13,  2.31it/s] 55%|█████▌    | 2266/4096 [18:22<13:14,  2.30it/s] 55%|█████▌    | 2267/4096 [18:22<13:12,  2.31it/s] 55%|█████▌    | 2268/4096 [18:23<13:13,  2.30it/s] 55%|█████▌    | 2269/4096 [18:23<13:12,  2.31it/s] 55%|█████▌    | 2270/4096 [18:24<13:11,  2.31it/s] 55%|█████▌    | 2271/4096 [18:24<13:10,  2.31it/s] 55%|█████▌    | 2272/4096 [18:25<13:09,  2.31it/s] 55%|█████▌    | 2273/4096 [18:25<13:09,  2.31it/s] 56%|█████▌    | 2274/4096 [18:25<13:09,  2.31it/s] 56%|█████▌    | 2275/4096 [18:26<13:08,  2.31it/s] 56%|█████▌    | 2276/4096 [18:26<13:08,  2.31it/s] 56%|█████▌    | 2277/4096 [18:27<13:08,  2.31it/s] 56%|█████▌    | 2278/4096 [18:27<13:07,  2.31it/s] 56%|█████▌    | 2279/4096 [18:28<13:06,  2.31it/s] 56%|█████▌    | 2280/4096 [18:28<13:06,  2.31it/s] 56%|█████▌    | 2281/4096 [18:28<13:05,  2.31it/s] 56%|█████▌    | 2282/4096 [18:29<13:04,  2.31it/s] 56%|█████▌    | 2283/4096 [18:29<13:03,  2.31it/s] 56%|█████▌    | 2284/4096 [18:30<13:02,  2.32it/s] 56%|█████▌    | 2285/4096 [18:30<13:01,  2.32it/s] 56%|█████▌    | 2286/4096 [18:31<13:01,  2.32it/s] 56%|█████▌    | 2287/4096 [18:31<13:00,  2.32it/s] 56%|█████▌    | 2288/4096 [18:31<13:01,  2.31it/s] 56%|█████▌    | 2289/4096 [18:32<13:01,  2.31it/s] 56%|█████▌    | 2290/4096 [18:32<13:01,  2.31it/s] 56%|█████▌    | 2291/4096 [18:33<13:01,  2.31it/s] 56%|█████▌    | 2292/4096 [18:33<13:01,  2.31it/s] 56%|█████▌    | 2293/4096 [18:34<13:02,  2.31it/s] 56%|█████▌    | 2294/4096 [18:34<13:03,  2.30it/s] 56%|█████▌    | 2295/4096 [18:35<13:02,  2.30it/s] 56%|█████▌    | 2296/4096 [18:35<13:02,  2.30it/s] 56%|█████▌    | 2297/4096 [18:35<13:00,  2.31it/s] 56%|█████▌    | 2298/4096 [18:36<12:59,  2.31it/s] 56%|█████▌    | 2299/4096 [18:36<12:59,  2.31it/s] 56%|█████▌    | 2300/4096 [18:37<12:58,  2.31it/s] 56%|█████▌    | 2301/4096 [18:37<12:58,  2.31it/s] 56%|█████▌    | 2302/4096 [18:38<12:56,  2.31it/s] 56%|█████▌    | 2303/4096 [18:38<12:56,  2.31it/s] 56%|█████▋    | 2304/4096 [18:38<10:39,  2.80it/s]loss: 2.1814279556274414
===========================
epoch 18/32 | loss: 2.1814279556274414
---------------------------
example true genres: 
tensor([1, 0, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9299781181619255
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["To think that we can merely relinquish our economic autonomy without giving up our political or legal autonomy is wishful thinking. If it is not enough that all of our internationalist One Worlders are advocating that we join this market, I refer you to an article in the New York Times'magazine section ( Nov. 12, 1961 ), by Mr. Eric Johnston, entitled ` ` We Must Join The Common Market''. He says : ` ` It has swept aside", "` ` Rob's not going to give up as easy as all that''. He was a florid, puffy man in his early sixties, very natty in his yachting cap, striped jacket and white flannels. He went to Key West every fall and winter and was the only man in town who did not know that his title of ` ` Commodore'' was never used without irony.", "He started out the door. ` ` One moment''!! Herr Schaffner said.", 'He capped the bottle and replaced it. After all, he had less reason to desire it than the marine. Before much longer the marine quieted down.', 'It ended when he tumbled ; ; but jumping right up, he staggered in no particular direction. He wore no head cover of any kind and, more odd, had no visible weapon.']
---------------------------
example output paragraph: 
["The the of the are not beapcih the rights rights, the up to own rights other obligations. a to to. The the is not to, the the us citizens obligations obligations - Organization'willing to the can in one in it think to to the article in the Communist York Times, '.. (. 1 ) 1960 ) ) which which. Kennedy K, editor to ` `''in with Worldplace''. wrote : ` ` The is no the the", "He ` I's son a to get him a much, he of''. He was a manirtd, andffed, who a shorts daysties, and muchgging, his owns,. and his and a shirtossl. He was out theport, day, then was the a only man to the to was not like what he father was a `'' '. to a. a.", "` said to of door. ` ` I thing'',!ehwaffff said.", 'He was his pistol. started the. He a, he had finishedened to do.. he other. He he of he man wased,.', 'He was with he was over ; he he off at, he was. a way direction. He was a hatless, his clothing of he he than, he been purpose movement.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.69it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.78it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.44it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.25it/s][A100%|██████████| 29/29 [00:00<00:00, 66.43it/s]
Mean Perplexity: 1129.6805524095664
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 56%|█████▋    | 2305/4096 [18:45<1:08:06,  2.28s/it] 56%|█████▋    | 2306/4096 [18:45<51:30,  1.73s/it]   56%|█████▋    | 2307/4096 [18:46<40:00,  1.34s/it] 56%|█████▋    | 2308/4096 [18:46<31:51,  1.07s/it] 56%|█████▋    | 2309/4096 [18:47<26:10,  1.14it/s] 56%|█████▋    | 2310/4096 [18:47<22:09,  1.34it/s] 56%|█████▋    | 2311/4096 [18:48<19:22,  1.54it/s] 56%|█████▋    | 2312/4096 [18:48<17:24,  1.71it/s] 56%|█████▋    | 2313/4096 [18:48<16:02,  1.85it/s] 56%|█████▋    | 2314/4096 [18:49<15:06,  1.97it/s] 57%|█████▋    | 2315/4096 [18:49<14:25,  2.06it/s] 57%|█████▋    | 2316/4096 [18:50<13:59,  2.12it/s] 57%|█████▋    | 2317/4096 [18:50<13:38,  2.17it/s] 57%|█████▋    | 2318/4096 [18:51<13:23,  2.21it/s] 57%|█████▋    | 2319/4096 [18:51<13:12,  2.24it/s] 57%|█████▋    | 2320/4096 [18:51<13:05,  2.26it/s] 57%|█████▋    | 2321/4096 [18:52<13:00,  2.27it/s] 57%|█████▋    | 2322/4096 [18:52<12:57,  2.28it/s] 57%|█████▋    | 2323/4096 [18:53<12:54,  2.29it/s] 57%|█████▋    | 2324/4096 [18:53<12:52,  2.29it/s] 57%|█████▋    | 2325/4096 [18:54<12:52,  2.29it/s] 57%|█████▋    | 2326/4096 [18:54<12:50,  2.30it/s] 57%|█████▋    | 2327/4096 [18:55<12:48,  2.30it/s] 57%|█████▋    | 2328/4096 [18:55<12:46,  2.31it/s] 57%|█████▋    | 2329/4096 [18:55<12:45,  2.31it/s] 57%|█████▋    | 2330/4096 [18:56<12:44,  2.31it/s] 57%|█████▋    | 2331/4096 [18:56<12:44,  2.31it/s] 57%|█████▋    | 2332/4096 [18:57<12:43,  2.31it/s] 57%|█████▋    | 2333/4096 [18:57<12:41,  2.32it/s] 57%|█████▋    | 2334/4096 [18:58<12:41,  2.31it/s] 57%|█████▋    | 2335/4096 [18:58<12:42,  2.31it/s] 57%|█████▋    | 2336/4096 [18:58<12:42,  2.31it/s] 57%|█████▋    | 2337/4096 [18:59<12:40,  2.31it/s] 57%|█████▋    | 2338/4096 [18:59<12:41,  2.31it/s] 57%|█████▋    | 2339/4096 [19:00<12:41,  2.31it/s] 57%|█████▋    | 2340/4096 [19:00<12:40,  2.31it/s] 57%|█████▋    | 2341/4096 [19:01<12:40,  2.31it/s] 57%|█████▋    | 2342/4096 [19:01<12:40,  2.31it/s] 57%|█████▋    | 2343/4096 [19:01<12:41,  2.30it/s] 57%|█████▋    | 2344/4096 [19:02<12:39,  2.31it/s] 57%|█████▋    | 2345/4096 [19:02<12:38,  2.31it/s] 57%|█████▋    | 2346/4096 [19:03<12:37,  2.31it/s] 57%|█████▋    | 2347/4096 [19:03<12:36,  2.31it/s] 57%|█████▋    | 2348/4096 [19:04<12:36,  2.31it/s] 57%|█████▋    | 2349/4096 [19:04<12:35,  2.31it/s] 57%|█████▋    | 2350/4096 [19:04<12:34,  2.31it/s] 57%|█████▋    | 2351/4096 [19:05<12:35,  2.31it/s] 57%|█████▋    | 2352/4096 [19:05<12:35,  2.31it/s] 57%|█████▋    | 2353/4096 [19:06<12:35,  2.31it/s] 57%|█████▋    | 2354/4096 [19:06<12:34,  2.31it/s] 57%|█████▋    | 2355/4096 [19:07<12:34,  2.31it/s] 58%|█████▊    | 2356/4096 [19:07<12:33,  2.31it/s] 58%|█████▊    | 2357/4096 [19:07<12:34,  2.30it/s] 58%|█████▊    | 2358/4096 [19:08<12:34,  2.30it/s] 58%|█████▊    | 2359/4096 [19:08<12:33,  2.31it/s] 58%|█████▊    | 2360/4096 [19:09<12:31,  2.31it/s] 58%|█████▊    | 2361/4096 [19:09<12:31,  2.31it/s] 58%|█████▊    | 2362/4096 [19:10<12:30,  2.31it/s] 58%|█████▊    | 2363/4096 [19:10<12:30,  2.31it/s] 58%|█████▊    | 2364/4096 [19:11<12:30,  2.31it/s] 58%|█████▊    | 2365/4096 [19:11<12:30,  2.31it/s] 58%|█████▊    | 2366/4096 [19:11<12:30,  2.31it/s] 58%|█████▊    | 2367/4096 [19:12<12:30,  2.30it/s] 58%|█████▊    | 2368/4096 [19:12<12:28,  2.31it/s] 58%|█████▊    | 2369/4096 [19:13<12:29,  2.31it/s] 58%|█████▊    | 2370/4096 [19:13<12:27,  2.31it/s] 58%|█████▊    | 2371/4096 [19:14<12:27,  2.31it/s] 58%|█████▊    | 2372/4096 [19:14<12:27,  2.31it/s] 58%|█████▊    | 2373/4096 [19:14<12:26,  2.31it/s] 58%|█████▊    | 2374/4096 [19:15<12:26,  2.31it/s] 58%|█████▊    | 2375/4096 [19:15<12:25,  2.31it/s] 58%|█████▊    | 2376/4096 [19:16<12:25,  2.31it/s] 58%|█████▊    | 2377/4096 [19:16<12:25,  2.31it/s] 58%|█████▊    | 2378/4096 [19:17<12:22,  2.31it/s] 58%|█████▊    | 2379/4096 [19:17<12:22,  2.31it/s] 58%|█████▊    | 2380/4096 [19:17<12:22,  2.31it/s] 58%|█████▊    | 2381/4096 [19:18<12:22,  2.31it/s] 58%|█████▊    | 2382/4096 [19:18<12:22,  2.31it/s] 58%|█████▊    | 2383/4096 [19:19<12:21,  2.31it/s] 58%|█████▊    | 2384/4096 [19:19<12:21,  2.31it/s] 58%|█████▊    | 2385/4096 [19:20<12:21,  2.31it/s] 58%|█████▊    | 2386/4096 [19:20<12:21,  2.31it/s] 58%|█████▊    | 2387/4096 [19:20<12:21,  2.31it/s] 58%|█████▊    | 2388/4096 [19:21<12:22,  2.30it/s] 58%|█████▊    | 2389/4096 [19:21<12:20,  2.30it/s] 58%|█████▊    | 2390/4096 [19:22<12:18,  2.31it/s] 58%|█████▊    | 2391/4096 [19:22<12:18,  2.31it/s] 58%|█████▊    | 2392/4096 [19:23<12:18,  2.31it/s] 58%|█████▊    | 2393/4096 [19:23<12:18,  2.31it/s] 58%|█████▊    | 2394/4096 [19:24<12:18,  2.30it/s] 58%|█████▊    | 2395/4096 [19:24<12:18,  2.30it/s] 58%|█████▊    | 2396/4096 [19:24<12:16,  2.31it/s] 59%|█████▊    | 2397/4096 [19:25<12:15,  2.31it/s] 59%|█████▊    | 2398/4096 [19:25<12:15,  2.31it/s] 59%|█████▊    | 2399/4096 [19:26<12:15,  2.31it/s] 59%|█████▊    | 2400/4096 [19:26<12:15,  2.31it/s] 59%|█████▊    | 2401/4096 [19:27<12:15,  2.30it/s] 59%|█████▊    | 2402/4096 [19:27<12:14,  2.30it/s] 59%|█████▊    | 2403/4096 [19:27<12:13,  2.31it/s] 59%|█████▊    | 2404/4096 [19:28<12:12,  2.31it/s] 59%|█████▊    | 2405/4096 [19:28<12:11,  2.31it/s] 59%|█████▊    | 2406/4096 [19:29<12:11,  2.31it/s] 59%|█████▉    | 2407/4096 [19:29<12:10,  2.31it/s] 59%|█████▉    | 2408/4096 [19:30<12:10,  2.31it/s] 59%|█████▉    | 2409/4096 [19:30<12:10,  2.31it/s] 59%|█████▉    | 2410/4096 [19:30<12:09,  2.31it/s] 59%|█████▉    | 2411/4096 [19:31<12:09,  2.31it/s] 59%|█████▉    | 2412/4096 [19:31<12:09,  2.31it/s] 59%|█████▉    | 2413/4096 [19:32<12:08,  2.31it/s] 59%|█████▉    | 2414/4096 [19:32<12:07,  2.31it/s] 59%|█████▉    | 2415/4096 [19:33<12:08,  2.31it/s] 59%|█████▉    | 2416/4096 [19:33<12:07,  2.31it/s] 59%|█████▉    | 2417/4096 [19:33<12:06,  2.31it/s] 59%|█████▉    | 2418/4096 [19:34<12:06,  2.31it/s] 59%|█████▉    | 2419/4096 [19:34<12:05,  2.31it/s] 59%|█████▉    | 2420/4096 [19:35<12:06,  2.31it/s] 59%|█████▉    | 2421/4096 [19:35<12:06,  2.30it/s] 59%|█████▉    | 2422/4096 [19:36<12:05,  2.31it/s] 59%|█████▉    | 2423/4096 [19:36<12:05,  2.31it/s] 59%|█████▉    | 2424/4096 [19:37<12:05,  2.31it/s] 59%|█████▉    | 2425/4096 [19:37<12:05,  2.30it/s] 59%|█████▉    | 2426/4096 [19:37<12:05,  2.30it/s] 59%|█████▉    | 2427/4096 [19:38<12:02,  2.31it/s] 59%|█████▉    | 2428/4096 [19:38<12:02,  2.31it/s] 59%|█████▉    | 2429/4096 [19:39<12:01,  2.31it/s] 59%|█████▉    | 2430/4096 [19:39<12:00,  2.31it/s] 59%|█████▉    | 2431/4096 [19:40<12:00,  2.31it/s] 59%|█████▉    | 2432/4096 [19:40<09:52,  2.81it/s]loss: 2.1820461750030518
===========================
epoch 19/32 | loss: 2.1820461750030518
---------------------------
example true genres: 
tensor([1, 0, 0, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 0, 1, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9234135667396062
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["for he kept on threatening that he would ` ` pull the ears'' of those responsible for agricultural production. And, as we know, the Virgin Lands are not producing as much as Khrushchev had hoped. One cannot but wonder whether these doubts about the success of Khrushchev's agricultural policy have not at least something to do with one of the big surprises provided by this Congress - - the obsessive harping on the crimes and misdeeds of", "From the convulsive quivers of the man's shoulders it was plain he had resumed the weeping. He reminded Matsuo of a similar thing he had witnessed in China. In China it was a baby sitting on a railroad platform, smudged, blood - specked, with the village burning about him and shells exploding.", "The Lalauries were at the top rung of the social ladder, and even a jury didn't feel privileged to doubt the veracity of so illustrious a lady. Moreover, runaway slaves frequently got into serious trouble in New Orleans'dives. So the verdict was ` ` death at the hands of a person or persons unknown'', and the elite of the city, accepting Delphine's testimony, welcomed her and the doctor back into the fold.", "I recalled sympathetically the Duke's complaint in Browning's ` ` My Last Duchess''. ) He smiled, and said a word or two to the interpreter, who turned to me, ` ` The President wonders where you are going after you leave Taipei''?? That, I smarted, is a royal rebuff if ever there was one.", "The marine shut his eyes. ` ` Are you a thrower of flame, marine''?? Matsuo took the small knife from its scabbard and laid it on the ground, out of the marine's reach and away from their shadows."]
---------------------------
example output paragraph: 
["The the was the thely he was be ` the the Communists of '. the men for the problems. The the in the know, the Soviet Mary are the a a much of thehrushchev'promised to The thing be justify what the people would the Soviet of thehrushchev's s policy might been been least be to be with the of the great problems. by the new. - - Sovietcctructionion propaganday of the Soviet of thesrandeds. the", "The the firstfineslsivelyuavering, the dead's body. was the that was been his struggle. The was ofie'the man sort. had seen. the. The the, was the man, in a rock bed, thecadged by and - stainedhardskled on and a blood of... the..", "The `ouau were not the same of of of the line scenes and the the few of't want that to be the peoplerandies of the many -rating men man would The,, thes were were to trouble trouble with the Orleans, ss. The, men was that ` `'the end of the few who family who to '. the the ` of the community were the theete's death, was the to the others, to the city.", "` ',ally, man of s wife to the, s office ` I dear Goodbye, '. ` ` said, ` said, little. two more the other. ` was to the. ` ` I man'what I are going to him '.''.? was I saidened. was it goodistgnantkeable I asked is a more", "The manstered eyes and ` ` I you going man -'the? Mrs '?? Heinmoto said the rifle bottle out the hookcabrd in pulled it on the counter. and of sight saddles s eyes. the from the eyes."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.83it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.23it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.27it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.37it/s][A100%|██████████| 29/29 [00:00<00:00, 66.40it/s]
Mean Perplexity: 1193.127228133788
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 59%|█████▉    | 2433/4096 [19:48<1:13:45,  2.66s/it] 59%|█████▉    | 2434/4096 [19:48<55:15,  1.99s/it]   59%|█████▉    | 2435/4096 [19:49<42:14,  1.53s/it] 59%|█████▉    | 2436/4096 [19:49<33:07,  1.20s/it] 59%|█████▉    | 2437/4096 [19:50<26:46,  1.03it/s] 60%|█████▉    | 2438/4096 [19:50<22:18,  1.24it/s] 60%|█████▉    | 2439/4096 [19:50<19:11,  1.44it/s] 60%|█████▉    | 2440/4096 [19:51<17:01,  1.62it/s] 60%|█████▉    | 2441/4096 [19:51<15:29,  1.78it/s] 60%|█████▉    | 2442/4096 [19:52<14:25,  1.91it/s] 60%|█████▉    | 2443/4096 [19:52<13:39,  2.02it/s] 60%|█████▉    | 2444/4096 [19:53<13:07,  2.10it/s] 60%|█████▉    | 2445/4096 [19:53<12:45,  2.16it/s] 60%|█████▉    | 2446/4096 [19:53<12:28,  2.20it/s] 60%|█████▉    | 2447/4096 [19:54<12:17,  2.23it/s] 60%|█████▉    | 2448/4096 [19:54<12:10,  2.26it/s] 60%|█████▉    | 2449/4096 [19:55<12:05,  2.27it/s] 60%|█████▉    | 2450/4096 [19:55<12:00,  2.28it/s] 60%|█████▉    | 2451/4096 [19:56<11:57,  2.29it/s] 60%|█████▉    | 2452/4096 [19:56<11:55,  2.30it/s] 60%|█████▉    | 2453/4096 [19:56<11:53,  2.30it/s] 60%|█████▉    | 2454/4096 [19:57<11:52,  2.30it/s] 60%|█████▉    | 2455/4096 [19:57<11:51,  2.31it/s] 60%|█████▉    | 2456/4096 [19:58<11:50,  2.31it/s] 60%|█████▉    | 2457/4096 [19:58<11:50,  2.31it/s] 60%|██████    | 2458/4096 [19:59<11:50,  2.30it/s] 60%|██████    | 2459/4096 [19:59<11:50,  2.30it/s] 60%|██████    | 2460/4096 [19:59<11:49,  2.31it/s] 60%|██████    | 2461/4096 [20:00<11:49,  2.31it/s] 60%|██████    | 2462/4096 [20:00<11:48,  2.31it/s] 60%|██████    | 2463/4096 [20:01<11:47,  2.31it/s] 60%|██████    | 2464/4096 [20:01<11:47,  2.31it/s] 60%|██████    | 2465/4096 [20:02<11:47,  2.31it/s] 60%|██████    | 2466/4096 [20:02<11:45,  2.31it/s] 60%|██████    | 2467/4096 [20:03<11:45,  2.31it/s] 60%|██████    | 2468/4096 [20:03<11:45,  2.31it/s] 60%|██████    | 2469/4096 [20:03<11:44,  2.31it/s] 60%|██████    | 2470/4096 [20:04<11:43,  2.31it/s] 60%|██████    | 2471/4096 [20:04<11:44,  2.31it/s] 60%|██████    | 2472/4096 [20:05<11:45,  2.30it/s] 60%|██████    | 2473/4096 [20:05<11:44,  2.30it/s] 60%|██████    | 2474/4096 [20:06<11:45,  2.30it/s] 60%|██████    | 2475/4096 [20:06<11:44,  2.30it/s] 60%|██████    | 2476/4096 [20:06<11:44,  2.30it/s] 60%|██████    | 2477/4096 [20:07<11:44,  2.30it/s] 60%|██████    | 2478/4096 [20:07<11:42,  2.30it/s] 61%|██████    | 2479/4096 [20:08<11:41,  2.30it/s] 61%|██████    | 2480/4096 [20:08<11:39,  2.31it/s] 61%|██████    | 2481/4096 [20:09<11:39,  2.31it/s] 61%|██████    | 2482/4096 [20:09<11:39,  2.31it/s] 61%|██████    | 2483/4096 [20:09<11:39,  2.30it/s] 61%|██████    | 2484/4096 [20:10<11:40,  2.30it/s] 61%|██████    | 2485/4096 [20:10<11:38,  2.31it/s] 61%|██████    | 2486/4096 [20:11<11:37,  2.31it/s] 61%|██████    | 2487/4096 [20:11<11:36,  2.31it/s] 61%|██████    | 2488/4096 [20:12<11:36,  2.31it/s] 61%|██████    | 2489/4096 [20:12<11:37,  2.30it/s] 61%|██████    | 2490/4096 [20:12<11:37,  2.30it/s] 61%|██████    | 2491/4096 [20:13<11:37,  2.30it/s] 61%|██████    | 2492/4096 [20:13<11:36,  2.30it/s] 61%|██████    | 2493/4096 [20:14<11:35,  2.30it/s] 61%|██████    | 2494/4096 [20:14<11:35,  2.30it/s] 61%|██████    | 2495/4096 [20:15<11:34,  2.30it/s] 61%|██████    | 2496/4096 [20:15<11:33,  2.31it/s] 61%|██████    | 2497/4096 [20:16<11:33,  2.31it/s] 61%|██████    | 2498/4096 [20:16<11:32,  2.31it/s] 61%|██████    | 2499/4096 [20:16<11:32,  2.31it/s] 61%|██████    | 2500/4096 [20:17<11:31,  2.31it/s] 61%|██████    | 2501/4096 [20:17<11:30,  2.31it/s] 61%|██████    | 2502/4096 [20:18<11:29,  2.31it/s] 61%|██████    | 2503/4096 [20:18<11:29,  2.31it/s] 61%|██████    | 2504/4096 [20:19<11:29,  2.31it/s] 61%|██████    | 2505/4096 [20:19<11:27,  2.31it/s] 61%|██████    | 2506/4096 [20:19<11:28,  2.31it/s] 61%|██████    | 2507/4096 [20:20<11:27,  2.31it/s] 61%|██████    | 2508/4096 [20:20<11:27,  2.31it/s] 61%|██████▏   | 2509/4096 [20:21<11:28,  2.31it/s] 61%|██████▏   | 2510/4096 [20:21<11:26,  2.31it/s] 61%|██████▏   | 2511/4096 [20:22<11:26,  2.31it/s] 61%|██████▏   | 2512/4096 [20:22<11:26,  2.31it/s] 61%|██████▏   | 2513/4096 [20:22<11:25,  2.31it/s] 61%|██████▏   | 2514/4096 [20:23<11:24,  2.31it/s] 61%|██████▏   | 2515/4096 [20:23<11:25,  2.31it/s] 61%|██████▏   | 2516/4096 [20:24<11:25,  2.31it/s] 61%|██████▏   | 2517/4096 [20:24<11:23,  2.31it/s] 61%|██████▏   | 2518/4096 [20:25<11:24,  2.31it/s] 61%|██████▏   | 2519/4096 [20:25<11:23,  2.31it/s] 62%|██████▏   | 2520/4096 [20:25<11:23,  2.30it/s] 62%|██████▏   | 2521/4096 [20:26<11:22,  2.31it/s] 62%|██████▏   | 2522/4096 [20:26<11:23,  2.30it/s] 62%|██████▏   | 2523/4096 [20:27<11:22,  2.31it/s] 62%|██████▏   | 2524/4096 [20:27<11:20,  2.31it/s] 62%|██████▏   | 2525/4096 [20:28<11:19,  2.31it/s] 62%|██████▏   | 2526/4096 [20:28<11:18,  2.31it/s] 62%|██████▏   | 2527/4096 [20:29<11:18,  2.31it/s] 62%|██████▏   | 2528/4096 [20:29<11:17,  2.31it/s] 62%|██████▏   | 2529/4096 [20:29<11:17,  2.31it/s] 62%|██████▏   | 2530/4096 [20:30<11:17,  2.31it/s] 62%|██████▏   | 2531/4096 [20:30<11:16,  2.31it/s] 62%|██████▏   | 2532/4096 [20:31<11:16,  2.31it/s] 62%|██████▏   | 2533/4096 [20:31<11:16,  2.31it/s] 62%|██████▏   | 2534/4096 [20:32<11:15,  2.31it/s] 62%|██████▏   | 2535/4096 [20:32<11:15,  2.31it/s] 62%|██████▏   | 2536/4096 [20:32<11:14,  2.31it/s] 62%|██████▏   | 2537/4096 [20:33<11:14,  2.31it/s] 62%|██████▏   | 2538/4096 [20:33<11:15,  2.31it/s] 62%|██████▏   | 2539/4096 [20:34<11:14,  2.31it/s] 62%|██████▏   | 2540/4096 [20:34<11:13,  2.31it/s] 62%|██████▏   | 2541/4096 [20:35<11:13,  2.31it/s] 62%|██████▏   | 2542/4096 [20:35<11:12,  2.31it/s] 62%|██████▏   | 2543/4096 [20:35<11:11,  2.31it/s] 62%|██████▏   | 2544/4096 [20:36<11:11,  2.31it/s] 62%|██████▏   | 2545/4096 [20:36<11:10,  2.31it/s] 62%|██████▏   | 2546/4096 [20:37<11:09,  2.31it/s] 62%|██████▏   | 2547/4096 [20:37<11:09,  2.31it/s] 62%|██████▏   | 2548/4096 [20:38<11:08,  2.32it/s] 62%|██████▏   | 2549/4096 [20:38<11:08,  2.31it/s] 62%|██████▏   | 2550/4096 [20:38<11:08,  2.31it/s] 62%|██████▏   | 2551/4096 [20:39<11:07,  2.31it/s] 62%|██████▏   | 2552/4096 [20:39<11:07,  2.31it/s] 62%|██████▏   | 2553/4096 [20:40<11:07,  2.31it/s] 62%|██████▏   | 2554/4096 [20:40<11:07,  2.31it/s] 62%|██████▏   | 2555/4096 [20:41<11:07,  2.31it/s] 62%|██████▏   | 2556/4096 [20:41<11:07,  2.31it/s] 62%|██████▏   | 2557/4096 [20:41<11:07,  2.31it/s] 62%|██████▏   | 2558/4096 [20:42<11:06,  2.31it/s] 62%|██████▏   | 2559/4096 [20:42<11:06,  2.31it/s] 62%|██████▎   | 2560/4096 [20:43<09:09,  2.80it/s]loss: 1.427667260169983
===========================
epoch 20/32 | loss: 1.427667260169983
---------------------------
example true genres: 
tensor([0, 0, 1, 1, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 1, 0, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.949671772428884
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["He grinned at them wolfishly. ` ` That ain't your name, Mister'', the boy accused. ` ` And you know - - you're right''!!", "` ` You will stay here thirty minutes after the others go home this afternoon and work your problems''. And so when the others stampeded out that afternoon Jack remained docilely in his seat near a window, looking out in what he hoped was a pitiable manner, while the other kids laughed and yelled in at him and made faces as they dispersed, going home. He scarcely saw them.", "Secondly, they depend on America's ` ` moral cooperation'' when the crucial moment arrives. They hope that if history vouchsafes the West another Budapest, we will receive the opportunity gladly. I remarked jocularly to the President that the future of China would be far more certain if he would invite a planeload of selected American Liberals to Quemoy on an odd day.", "Now, more than five years later, I cannot in any realistic sense be called a trained soldier. But, in spite of this, I, at present a man 31 years of age and a College Professor, have been recalled ` ` by direction of the President'' to report on November 25th to Fort Devens, Massachusetts, for another twelve months of Active Duty as an Sp 4 ( the equivalent of a PFC ). Today, seven years after the date of my initial induction", "She : ` ` By subway or cab''?? That exchange was not only possible but commonplace last week in Manhattan, as more and more New Yorkers were discovering 29th Street and Eighth Avenue, where half a dozen small nightclubs with names like Arabian Nights, Grecian Palace and Egyptian Gardens are the American inpost of belly dancing. Several more will open soon."]
---------------------------
example output paragraph: 
["` ', him,ishly, ` ` You'' t your business'Mr''! he boy said. ` ` You you'what - you're the''!!", "He ` I'never here in minutes'I fire'out to morning. then on men with '. He he he they girl camee their the morning they'behindzele and, the arms, the tree, they at at the was was would doing reliefiful.. he he rest two were at their at their the. then a at if were. and to to was remembered the go", "Thely, the are on the's prestige ` ` moral and '. the United international is, The'to the the comesigores,y the United will way will the will not a opportunity toly to The hope,estcily, the editor of the United would the will be a less better than the was not a conference to of American American citizens. joinllonllo. the expedition occasion.", 'The, I time one minutes ago, I am haveject sense sense of able a professional officer, I, as order of the circumstances Mr am Mr least, very who - old service, a full officer, I been a to ` ` the of the University of s. the on the 9, the Douglaselo, a, a a year -, service Service duty a officer.rite - - Colonel of a year.,,, I days old my fact, the discharge discharge to', 'The was ` ` The the stations subways s,? The is is the a a to toplace in year, the, the the than more often Yorks, now that Street Street the Street, the the the dozen blocks apartmentss are a named the names, thelamourv dancess the Gardens, located most Express theero thewalk. of of be up as']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.25it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.24it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.29it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.43it/s][A100%|██████████| 29/29 [00:00<00:00, 65.46it/s]
Mean Perplexity: 1183.3351320396107
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 63%|██████▎   | 2561/4096 [20:49<59:04,  2.31s/it] 63%|██████▎   | 2562/4096 [20:50<44:39,  1.75s/it] 63%|██████▎   | 2563/4096 [20:50<34:33,  1.35s/it] 63%|██████▎   | 2564/4096 [20:51<27:29,  1.08s/it] 63%|██████▎   | 2565/4096 [20:51<22:31,  1.13it/s] 63%|██████▎   | 2566/4096 [20:52<19:04,  1.34it/s] 63%|██████▎   | 2567/4096 [20:52<16:38,  1.53it/s] 63%|██████▎   | 2568/4096 [20:52<14:56,  1.71it/s] 63%|██████▎   | 2569/4096 [20:53<13:45,  1.85it/s] 63%|██████▎   | 2570/4096 [20:53<12:55,  1.97it/s] 63%|██████▎   | 2571/4096 [20:54<12:19,  2.06it/s] 63%|██████▎   | 2572/4096 [20:54<11:54,  2.13it/s] 63%|██████▎   | 2573/4096 [20:55<11:38,  2.18it/s] 63%|██████▎   | 2574/4096 [20:55<11:26,  2.22it/s] 63%|██████▎   | 2575/4096 [20:55<11:17,  2.24it/s] 63%|██████▎   | 2576/4096 [20:56<11:12,  2.26it/s] 63%|██████▎   | 2577/4096 [20:56<11:06,  2.28it/s] 63%|██████▎   | 2578/4096 [20:57<11:03,  2.29it/s] 63%|██████▎   | 2579/4096 [20:57<11:01,  2.29it/s] 63%|██████▎   | 2580/4096 [20:58<10:59,  2.30it/s] 63%|██████▎   | 2581/4096 [20:58<10:57,  2.30it/s] 63%|██████▎   | 2582/4096 [20:58<10:56,  2.31it/s] 63%|██████▎   | 2583/4096 [20:59<10:56,  2.30it/s] 63%|██████▎   | 2584/4096 [20:59<10:54,  2.31it/s] 63%|██████▎   | 2585/4096 [21:00<10:54,  2.31it/s] 63%|██████▎   | 2586/4096 [21:00<10:54,  2.31it/s] 63%|██████▎   | 2587/4096 [21:01<10:53,  2.31it/s] 63%|██████▎   | 2588/4096 [21:01<10:52,  2.31it/s] 63%|██████▎   | 2589/4096 [21:02<10:52,  2.31it/s] 63%|██████▎   | 2590/4096 [21:02<10:51,  2.31it/s] 63%|██████▎   | 2591/4096 [21:02<10:51,  2.31it/s] 63%|██████▎   | 2592/4096 [21:03<10:51,  2.31it/s] 63%|██████▎   | 2593/4096 [21:03<10:51,  2.31it/s] 63%|██████▎   | 2594/4096 [21:04<10:50,  2.31it/s] 63%|██████▎   | 2595/4096 [21:04<10:50,  2.31it/s] 63%|██████▎   | 2596/4096 [21:05<10:49,  2.31it/s] 63%|██████▎   | 2597/4096 [21:05<10:48,  2.31it/s] 63%|██████▎   | 2598/4096 [21:05<10:47,  2.31it/s] 63%|██████▎   | 2599/4096 [21:06<10:46,  2.31it/s] 63%|██████▎   | 2600/4096 [21:06<10:46,  2.31it/s] 64%|██████▎   | 2601/4096 [21:07<10:46,  2.31it/s] 64%|██████▎   | 2602/4096 [21:07<10:46,  2.31it/s] 64%|██████▎   | 2603/4096 [21:08<10:46,  2.31it/s] 64%|██████▎   | 2604/4096 [21:08<10:46,  2.31it/s] 64%|██████▎   | 2605/4096 [21:08<10:45,  2.31it/s] 64%|██████▎   | 2606/4096 [21:09<10:44,  2.31it/s] 64%|██████▎   | 2607/4096 [21:09<10:45,  2.31it/s] 64%|██████▎   | 2608/4096 [21:10<10:44,  2.31it/s] 64%|██████▎   | 2609/4096 [21:10<10:43,  2.31it/s] 64%|██████▎   | 2610/4096 [21:11<10:43,  2.31it/s] 64%|██████▎   | 2611/4096 [21:11<10:43,  2.31it/s] 64%|██████▍   | 2612/4096 [21:11<10:43,  2.30it/s] 64%|██████▍   | 2613/4096 [21:12<10:43,  2.31it/s] 64%|██████▍   | 2614/4096 [21:12<10:42,  2.31it/s] 64%|██████▍   | 2615/4096 [21:13<10:42,  2.31it/s] 64%|██████▍   | 2616/4096 [21:13<10:41,  2.31it/s] 64%|██████▍   | 2617/4096 [21:14<10:41,  2.31it/s] 64%|██████▍   | 2618/4096 [21:14<10:41,  2.30it/s] 64%|██████▍   | 2619/4096 [21:15<10:41,  2.30it/s] 64%|██████▍   | 2620/4096 [21:15<10:39,  2.31it/s] 64%|██████▍   | 2621/4096 [21:15<10:38,  2.31it/s] 64%|██████▍   | 2622/4096 [21:16<10:38,  2.31it/s] 64%|██████▍   | 2623/4096 [21:16<10:39,  2.30it/s] 64%|██████▍   | 2624/4096 [21:17<10:38,  2.31it/s] 64%|██████▍   | 2625/4096 [21:17<10:38,  2.30it/s] 64%|██████▍   | 2626/4096 [21:18<10:38,  2.30it/s] 64%|██████▍   | 2627/4096 [21:18<10:36,  2.31it/s] 64%|██████▍   | 2628/4096 [21:18<10:36,  2.31it/s] 64%|██████▍   | 2629/4096 [21:19<10:34,  2.31it/s] 64%|██████▍   | 2630/4096 [21:19<10:34,  2.31it/s] 64%|██████▍   | 2631/4096 [21:20<10:35,  2.31it/s] 64%|██████▍   | 2632/4096 [21:20<10:34,  2.31it/s] 64%|██████▍   | 2633/4096 [21:21<10:33,  2.31it/s] 64%|██████▍   | 2634/4096 [21:21<10:34,  2.30it/s] 64%|██████▍   | 2635/4096 [21:21<10:33,  2.31it/s] 64%|██████▍   | 2636/4096 [21:22<10:33,  2.30it/s] 64%|██████▍   | 2637/4096 [21:22<10:32,  2.31it/s] 64%|██████▍   | 2638/4096 [21:23<10:31,  2.31it/s] 64%|██████▍   | 2639/4096 [21:23<10:30,  2.31it/s] 64%|██████▍   | 2640/4096 [21:24<10:30,  2.31it/s] 64%|██████▍   | 2641/4096 [21:24<10:30,  2.31it/s] 65%|██████▍   | 2642/4096 [21:24<10:29,  2.31it/s] 65%|██████▍   | 2643/4096 [21:25<10:28,  2.31it/s] 65%|██████▍   | 2644/4096 [21:25<10:27,  2.31it/s] 65%|██████▍   | 2645/4096 [21:26<10:27,  2.31it/s] 65%|██████▍   | 2646/4096 [21:26<10:27,  2.31it/s] 65%|██████▍   | 2647/4096 [21:27<10:25,  2.32it/s] 65%|██████▍   | 2648/4096 [21:27<10:26,  2.31it/s] 65%|██████▍   | 2649/4096 [21:28<10:25,  2.31it/s] 65%|██████▍   | 2650/4096 [21:28<10:25,  2.31it/s] 65%|██████▍   | 2651/4096 [21:28<10:24,  2.31it/s] 65%|██████▍   | 2652/4096 [21:29<10:24,  2.31it/s] 65%|██████▍   | 2653/4096 [21:29<10:24,  2.31it/s] 65%|██████▍   | 2654/4096 [21:30<10:24,  2.31it/s] 65%|██████▍   | 2655/4096 [21:30<10:23,  2.31it/s] 65%|██████▍   | 2656/4096 [21:31<10:23,  2.31it/s] 65%|██████▍   | 2657/4096 [21:31<10:23,  2.31it/s] 65%|██████▍   | 2658/4096 [21:31<10:22,  2.31it/s] 65%|██████▍   | 2659/4096 [21:32<10:21,  2.31it/s] 65%|██████▍   | 2660/4096 [21:32<10:21,  2.31it/s] 65%|██████▍   | 2661/4096 [21:33<10:21,  2.31it/s] 65%|██████▍   | 2662/4096 [21:33<10:20,  2.31it/s] 65%|██████▌   | 2663/4096 [21:34<10:20,  2.31it/s] 65%|██████▌   | 2664/4096 [21:34<10:18,  2.32it/s] 65%|██████▌   | 2665/4096 [21:34<10:18,  2.31it/s] 65%|██████▌   | 2666/4096 [21:35<10:17,  2.32it/s] 65%|██████▌   | 2667/4096 [21:35<10:17,  2.31it/s] 65%|██████▌   | 2668/4096 [21:36<10:18,  2.31it/s] 65%|██████▌   | 2669/4096 [21:36<10:18,  2.31it/s] 65%|██████▌   | 2670/4096 [21:37<10:17,  2.31it/s] 65%|██████▌   | 2671/4096 [21:37<10:16,  2.31it/s] 65%|██████▌   | 2672/4096 [21:37<10:16,  2.31it/s] 65%|██████▌   | 2673/4096 [21:38<10:15,  2.31it/s] 65%|██████▌   | 2674/4096 [21:38<10:15,  2.31it/s] 65%|██████▌   | 2675/4096 [21:39<10:15,  2.31it/s] 65%|██████▌   | 2676/4096 [21:39<10:14,  2.31it/s] 65%|██████▌   | 2677/4096 [21:40<10:13,  2.31it/s] 65%|██████▌   | 2678/4096 [21:40<10:13,  2.31it/s] 65%|██████▌   | 2679/4096 [21:40<10:14,  2.31it/s] 65%|██████▌   | 2680/4096 [21:41<10:13,  2.31it/s] 65%|██████▌   | 2681/4096 [21:41<10:12,  2.31it/s] 65%|██████▌   | 2682/4096 [21:42<10:12,  2.31it/s] 66%|██████▌   | 2683/4096 [21:42<10:12,  2.31it/s] 66%|██████▌   | 2684/4096 [21:43<10:11,  2.31it/s] 66%|██████▌   | 2685/4096 [21:43<10:11,  2.31it/s] 66%|██████▌   | 2686/4096 [21:44<10:11,  2.30it/s] 66%|██████▌   | 2687/4096 [21:44<10:11,  2.31it/s] 66%|██████▌   | 2688/4096 [21:44<08:22,  2.80it/s]loss: 0.9682710766792297
===========================
epoch 21/32 | loss: 0.9682710766792297
---------------------------
example true genres: 
tensor([1, 1, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 1, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.949671772428884
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['Our chief aim becomes that of finding favor in neutralist eyes. If we grasp this orientation as a key, our national conduct in all of the events here mentioned becomes intelligible. And it becomes clear why in general we cannot take the initiative against the Left.', "A brief for the negativeI disagree with Mr. Burnham's position on the Common Market ( Nov. 18 ) as a desirable organization for us to join. For him to ignore the political consequences involved in an Atlantic Union of this kind is difficult to understand.", "The slender, handsome fellow was called Dandy Brandon by the other slaves. He was gifted with animal magnetism and a potent allure for women of any race. But Dandy had had little experience with girls on his master's plantation in Bayou St. John.", 'The marine was alone, for they were impatient people and by now would have vied to knock him from the tree. Down the tree he scrambled and knelt at the edge of foliage. The marine was sprawled some thirty yards away, one arm extended.', "One by one he tossed the objects aside. He didn't smoke and could not light fires with a flintless lighter ; ; he had no use any longer for exact time, even had the watch been running."]
---------------------------
example output paragraph: 
['The own is is to the the outable theism countries. The the are the point of a whole to the own interests is the areas the world is is, irrelevant theligentgible. The it is clear that the the that must be part risk in the Communist wing', 'The few discussion a editorss with the. Kennedyman. s view on the issue Sense. the. 1 ) ) a result statement to the. be the The example, be the fact considerations of in the un policy. the kind of a to justify.', "He man man muscular man was a a '.. the way two, He was a to the crueltyism, physical great talent - to his. all age. He he was was a a time with the, the own. s ranch. theou,. Louis,", 'He mans not, and he were not.. shouting now, be beenoc for the the. the saddle. He the slope, saw to climbed down the side of the. He man was a out time yards away. his of in,', "He of the, had the gun into and He had't care, he not stand them. his flameirt. blade. ; he had no idea to of. iting. but with no time. fixed."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.44it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.18it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.16it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.52it/s][A100%|██████████| 29/29 [00:00<00:00, 66.54it/s]
Mean Perplexity: 1188.5461459838066
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 66%|██████▌   | 2689/4096 [21:51<52:39,  2.25s/it] 66%|██████▌   | 2690/4096 [21:51<39:52,  1.70s/it] 66%|██████▌   | 2691/4096 [21:52<31:08,  1.33s/it] 66%|██████▌   | 2692/4096 [21:52<24:48,  1.06s/it] 66%|██████▌   | 2693/4096 [21:53<20:23,  1.15it/s] 66%|██████▌   | 2694/4096 [21:53<17:17,  1.35it/s] 66%|██████▌   | 2695/4096 [21:53<15:07,  1.54it/s] 66%|██████▌   | 2696/4096 [21:54<13:36,  1.71it/s] 66%|██████▌   | 2697/4096 [21:54<12:33,  1.86it/s] 66%|██████▌   | 2698/4096 [21:55<11:48,  1.97it/s] 66%|██████▌   | 2699/4096 [21:55<11:18,  2.06it/s] 66%|██████▌   | 2700/4096 [21:56<10:55,  2.13it/s] 66%|██████▌   | 2701/4096 [21:56<10:39,  2.18it/s] 66%|██████▌   | 2702/4096 [21:56<10:27,  2.22it/s] 66%|██████▌   | 2703/4096 [21:57<10:20,  2.25it/s] 66%|██████▌   | 2704/4096 [21:57<10:14,  2.27it/s] 66%|██████▌   | 2705/4096 [21:58<10:09,  2.28it/s] 66%|██████▌   | 2706/4096 [21:58<10:07,  2.29it/s] 66%|██████▌   | 2707/4096 [21:59<10:05,  2.29it/s] 66%|██████▌   | 2708/4096 [21:59<10:03,  2.30it/s] 66%|██████▌   | 2709/4096 [21:59<10:01,  2.31it/s] 66%|██████▌   | 2710/4096 [22:00<10:00,  2.31it/s] 66%|██████▌   | 2711/4096 [22:00<10:00,  2.31it/s] 66%|██████▌   | 2712/4096 [22:01<09:59,  2.31it/s] 66%|██████▌   | 2713/4096 [22:01<09:58,  2.31it/s] 66%|██████▋   | 2714/4096 [22:02<09:57,  2.31it/s] 66%|██████▋   | 2715/4096 [22:02<09:56,  2.31it/s] 66%|██████▋   | 2716/4096 [22:03<09:57,  2.31it/s] 66%|██████▋   | 2717/4096 [22:03<09:57,  2.31it/s] 66%|██████▋   | 2718/4096 [22:03<09:55,  2.31it/s] 66%|██████▋   | 2719/4096 [22:04<09:56,  2.31it/s] 66%|██████▋   | 2720/4096 [22:04<09:56,  2.31it/s] 66%|██████▋   | 2721/4096 [22:05<09:55,  2.31it/s] 66%|██████▋   | 2722/4096 [22:05<09:55,  2.31it/s] 66%|██████▋   | 2723/4096 [22:06<09:53,  2.31it/s] 67%|██████▋   | 2724/4096 [22:06<09:53,  2.31it/s] 67%|██████▋   | 2725/4096 [22:06<09:52,  2.31it/s] 67%|██████▋   | 2726/4096 [22:07<09:51,  2.32it/s] 67%|██████▋   | 2727/4096 [22:07<09:52,  2.31it/s] 67%|██████▋   | 2728/4096 [22:08<09:52,  2.31it/s] 67%|██████▋   | 2729/4096 [22:08<09:52,  2.31it/s] 67%|██████▋   | 2730/4096 [22:09<09:51,  2.31it/s] 67%|██████▋   | 2731/4096 [22:09<09:50,  2.31it/s] 67%|██████▋   | 2732/4096 [22:09<09:50,  2.31it/s] 67%|██████▋   | 2733/4096 [22:10<09:49,  2.31it/s] 67%|██████▋   | 2734/4096 [22:10<09:49,  2.31it/s] 67%|██████▋   | 2735/4096 [22:11<09:48,  2.31it/s] 67%|██████▋   | 2736/4096 [22:11<09:47,  2.31it/s] 67%|██████▋   | 2737/4096 [22:12<09:47,  2.31it/s] 67%|██████▋   | 2738/4096 [22:12<09:46,  2.31it/s] 67%|██████▋   | 2739/4096 [22:12<09:46,  2.31it/s] 67%|██████▋   | 2740/4096 [22:13<09:45,  2.31it/s] 67%|██████▋   | 2741/4096 [22:13<09:45,  2.31it/s] 67%|██████▋   | 2742/4096 [22:14<09:44,  2.32it/s] 67%|██████▋   | 2743/4096 [22:14<09:43,  2.32it/s] 67%|██████▋   | 2744/4096 [22:15<09:43,  2.32it/s] 67%|██████▋   | 2745/4096 [22:15<09:43,  2.32it/s] 67%|██████▋   | 2746/4096 [22:15<09:42,  2.32it/s] 67%|██████▋   | 2747/4096 [22:16<09:43,  2.31it/s] 67%|██████▋   | 2748/4096 [22:16<09:44,  2.31it/s] 67%|██████▋   | 2749/4096 [22:17<09:44,  2.30it/s] 67%|██████▋   | 2750/4096 [22:17<09:44,  2.30it/s] 67%|██████▋   | 2751/4096 [22:18<09:43,  2.31it/s] 67%|██████▋   | 2752/4096 [22:18<09:42,  2.31it/s] 67%|██████▋   | 2753/4096 [22:19<09:42,  2.30it/s] 67%|██████▋   | 2754/4096 [22:19<09:41,  2.31it/s] 67%|██████▋   | 2755/4096 [22:19<09:41,  2.31it/s] 67%|██████▋   | 2756/4096 [22:20<09:41,  2.31it/s] 67%|██████▋   | 2757/4096 [22:20<09:41,  2.30it/s] 67%|██████▋   | 2758/4096 [22:21<09:41,  2.30it/s] 67%|██████▋   | 2759/4096 [22:21<09:40,  2.30it/s] 67%|██████▋   | 2760/4096 [22:22<09:39,  2.31it/s] 67%|██████▋   | 2761/4096 [22:22<09:38,  2.31it/s] 67%|██████▋   | 2762/4096 [22:22<09:37,  2.31it/s] 67%|██████▋   | 2763/4096 [22:23<09:37,  2.31it/s] 67%|██████▋   | 2764/4096 [22:23<09:37,  2.31it/s] 68%|██████▊   | 2765/4096 [22:24<09:35,  2.31it/s] 68%|██████▊   | 2766/4096 [22:24<09:35,  2.31it/s] 68%|██████▊   | 2767/4096 [22:25<09:35,  2.31it/s] 68%|██████▊   | 2768/4096 [22:25<09:35,  2.31it/s] 68%|██████▊   | 2769/4096 [22:25<09:34,  2.31it/s] 68%|██████▊   | 2770/4096 [22:26<09:33,  2.31it/s] 68%|██████▊   | 2771/4096 [22:26<09:34,  2.31it/s] 68%|██████▊   | 2772/4096 [22:27<09:33,  2.31it/s] 68%|██████▊   | 2773/4096 [22:27<09:32,  2.31it/s] 68%|██████▊   | 2774/4096 [22:28<09:32,  2.31it/s] 68%|██████▊   | 2775/4096 [22:28<09:31,  2.31it/s] 68%|██████▊   | 2776/4096 [22:28<09:31,  2.31it/s] 68%|██████▊   | 2777/4096 [22:29<09:31,  2.31it/s] 68%|██████▊   | 2778/4096 [22:29<09:30,  2.31it/s] 68%|██████▊   | 2779/4096 [22:30<09:30,  2.31it/s] 68%|██████▊   | 2780/4096 [22:30<09:31,  2.30it/s] 68%|██████▊   | 2781/4096 [22:31<09:30,  2.30it/s] 68%|██████▊   | 2782/4096 [22:31<09:30,  2.30it/s] 68%|██████▊   | 2783/4096 [22:32<09:29,  2.30it/s] 68%|██████▊   | 2784/4096 [22:32<09:29,  2.30it/s] 68%|██████▊   | 2785/4096 [22:32<09:28,  2.30it/s] 68%|██████▊   | 2786/4096 [22:33<09:29,  2.30it/s] 68%|██████▊   | 2787/4096 [22:33<09:29,  2.30it/s] 68%|██████▊   | 2788/4096 [22:34<09:28,  2.30it/s] 68%|██████▊   | 2789/4096 [22:34<09:27,  2.30it/s] 68%|██████▊   | 2790/4096 [22:35<09:26,  2.31it/s] 68%|██████▊   | 2791/4096 [22:35<09:26,  2.30it/s] 68%|██████▊   | 2792/4096 [22:35<09:25,  2.31it/s] 68%|██████▊   | 2793/4096 [22:36<09:24,  2.31it/s] 68%|██████▊   | 2794/4096 [22:36<09:24,  2.31it/s] 68%|██████▊   | 2795/4096 [22:37<09:24,  2.31it/s] 68%|██████▊   | 2796/4096 [22:37<09:23,  2.31it/s] 68%|██████▊   | 2797/4096 [22:38<09:24,  2.30it/s] 68%|██████▊   | 2798/4096 [22:38<09:23,  2.30it/s] 68%|██████▊   | 2799/4096 [22:38<09:23,  2.30it/s] 68%|██████▊   | 2800/4096 [22:39<09:24,  2.30it/s] 68%|██████▊   | 2801/4096 [22:39<09:23,  2.30it/s] 68%|██████▊   | 2802/4096 [22:40<09:23,  2.29it/s] 68%|██████▊   | 2803/4096 [22:40<09:22,  2.30it/s] 68%|██████▊   | 2804/4096 [22:41<09:20,  2.30it/s] 68%|██████▊   | 2805/4096 [22:41<09:20,  2.30it/s] 69%|██████▊   | 2806/4096 [22:42<09:19,  2.30it/s] 69%|██████▊   | 2807/4096 [22:42<09:19,  2.30it/s] 69%|██████▊   | 2808/4096 [22:42<09:18,  2.30it/s] 69%|██████▊   | 2809/4096 [22:43<09:18,  2.30it/s] 69%|██████▊   | 2810/4096 [22:43<09:18,  2.30it/s] 69%|██████▊   | 2811/4096 [22:44<09:17,  2.30it/s] 69%|██████▊   | 2812/4096 [22:44<09:17,  2.30it/s] 69%|██████▊   | 2813/4096 [22:45<09:17,  2.30it/s] 69%|██████▊   | 2814/4096 [22:45<09:17,  2.30it/s] 69%|██████▊   | 2815/4096 [22:45<09:16,  2.30it/s] 69%|██████▉   | 2816/4096 [22:46<07:36,  2.81it/s]loss: 3.2794439792633057
===========================
epoch 22/32 | loss: 3.2794439792633057
---------------------------
example true genres: 
tensor([1, 0, 1, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 1, 1, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9452954048140044
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["Nowadays, more and more, all he needs is someone else's book. To get started, he does not scan the world about him ; ; he and his prospective producer just read the bestseller lists.", 'Hand grenades. The bobbing head was a poor target, so Matsuo shot him in the upper trunk. The marine spun, clapping a hand high on his chest, and dived forward.', "A brief for the negativeI disagree with Mr. Burnham's position on the Common Market ( Nov. 18 ) as a desirable organization for us to join. For him to ignore the political consequences involved in an Atlantic Union of this kind is difficult to understand.", 'There ought to be a point beyond which we will not allow ourselves to go regardless of what Russia does. The refusal to resume atmospheric testing would be a good start. Ecumenical hopes', "One less shouldn't matter to him''. Aristide Devol, the sardonic manservant who had been brought in chains years before from his native Sierra Leone, smiled thinly and touched his well - brushed beaver hat. His bold eyes raked the woman, and a perceptive spectator might sense that there was more to their relationship than that of slave to owner."]
---------------------------
example output paragraph: 
['He, he time more time he the is to the who who s job. He be him, he is not want the ads in the. ; he is his wife husbands, about it news bookss.', "Heling, He manulge head was a good,. but heee'at. a chest half. He man was around andlattered his few.. his shoulder. and hed..", 'The new discussion a editorsnc with the. Kennedyman. s view on the issue Sense. the. 1 ) ) a result result to the. be with The example, be the fact considerations of in the economic policy. the sort of a to justify.', 'If is to be a good to the we can not be them to try. of the is is. The Russians of negotiate the tests will be a mistake sign to Theasily Russia we', "He of than't be to him, '. He wasina waselo was the manneic man,ably, had been a to to to ago, the father home,. was,ly at then his lips - worn hairads. He face face were the crowd in and he mant manto of be a the was no than do own than the. a - the."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.27it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.23it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.09it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.04it/s][A100%|██████████| 29/29 [00:00<00:00, 66.19it/s]
Mean Perplexity: 1208.4869638355347
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 69%|██████▉   | 2817/4096 [22:52<48:57,  2.30s/it] 69%|██████▉   | 2818/4096 [22:53<37:04,  1.74s/it] 69%|██████▉   | 2819/4096 [22:53<28:41,  1.35s/it] 69%|██████▉   | 2820/4096 [22:54<22:50,  1.07s/it] 69%|██████▉   | 2821/4096 [22:54<18:43,  1.13it/s] 69%|██████▉   | 2822/4096 [22:55<15:51,  1.34it/s] 69%|██████▉   | 2823/4096 [22:55<13:50,  1.53it/s] 69%|██████▉   | 2824/4096 [22:55<12:26,  1.70it/s] 69%|██████▉   | 2825/4096 [22:56<11:27,  1.85it/s] 69%|██████▉   | 2826/4096 [22:56<10:46,  1.97it/s] 69%|██████▉   | 2827/4096 [22:57<10:16,  2.06it/s] 69%|██████▉   | 2828/4096 [22:57<09:55,  2.13it/s] 69%|██████▉   | 2829/4096 [22:58<09:42,  2.18it/s] 69%|██████▉   | 2830/4096 [22:58<09:32,  2.21it/s] 69%|██████▉   | 2831/4096 [22:58<09:25,  2.24it/s] 69%|██████▉   | 2832/4096 [22:59<09:21,  2.25it/s] 69%|██████▉   | 2833/4096 [22:59<09:16,  2.27it/s] 69%|██████▉   | 2834/4096 [23:00<09:14,  2.28it/s] 69%|██████▉   | 2835/4096 [23:00<09:12,  2.28it/s] 69%|██████▉   | 2836/4096 [23:01<09:10,  2.29it/s] 69%|██████▉   | 2837/4096 [23:01<09:09,  2.29it/s] 69%|██████▉   | 2838/4096 [23:02<09:07,  2.30it/s] 69%|██████▉   | 2839/4096 [23:02<09:06,  2.30it/s] 69%|██████▉   | 2840/4096 [23:02<09:07,  2.30it/s] 69%|██████▉   | 2841/4096 [23:03<09:06,  2.30it/s] 69%|██████▉   | 2842/4096 [23:03<09:04,  2.30it/s] 69%|██████▉   | 2843/4096 [23:04<09:04,  2.30it/s] 69%|██████▉   | 2844/4096 [23:04<09:04,  2.30it/s] 69%|██████▉   | 2845/4096 [23:05<09:03,  2.30it/s] 69%|██████▉   | 2846/4096 [23:05<09:03,  2.30it/s] 70%|██████▉   | 2847/4096 [23:05<09:02,  2.30it/s] 70%|██████▉   | 2848/4096 [23:06<09:02,  2.30it/s] 70%|██████▉   | 2849/4096 [23:06<09:01,  2.30it/s] 70%|██████▉   | 2850/4096 [23:07<09:00,  2.30it/s] 70%|██████▉   | 2851/4096 [23:07<09:00,  2.30it/s] 70%|██████▉   | 2852/4096 [23:08<08:58,  2.31it/s] 70%|██████▉   | 2853/4096 [23:08<08:57,  2.31it/s] 70%|██████▉   | 2854/4096 [23:08<08:57,  2.31it/s] 70%|██████▉   | 2855/4096 [23:09<08:57,  2.31it/s] 70%|██████▉   | 2856/4096 [23:09<08:57,  2.31it/s] 70%|██████▉   | 2857/4096 [23:10<08:56,  2.31it/s] 70%|██████▉   | 2858/4096 [23:10<08:55,  2.31it/s] 70%|██████▉   | 2859/4096 [23:11<08:55,  2.31it/s] 70%|██████▉   | 2860/4096 [23:11<08:56,  2.30it/s] 70%|██████▉   | 2861/4096 [23:12<08:56,  2.30it/s] 70%|██████▉   | 2862/4096 [23:12<08:55,  2.31it/s] 70%|██████▉   | 2863/4096 [23:12<08:55,  2.30it/s] 70%|██████▉   | 2864/4096 [23:13<08:55,  2.30it/s] 70%|██████▉   | 2865/4096 [23:13<08:54,  2.30it/s] 70%|██████▉   | 2866/4096 [23:14<08:53,  2.31it/s] 70%|██████▉   | 2867/4096 [23:14<08:54,  2.30it/s] 70%|███████   | 2868/4096 [23:15<08:54,  2.30it/s] 70%|███████   | 2869/4096 [23:15<08:54,  2.30it/s] 70%|███████   | 2870/4096 [23:15<08:53,  2.30it/s] 70%|███████   | 2871/4096 [23:16<08:52,  2.30it/s] 70%|███████   | 2872/4096 [23:16<08:51,  2.30it/s] 70%|███████   | 2873/4096 [23:17<08:51,  2.30it/s] 70%|███████   | 2874/4096 [23:17<08:50,  2.30it/s] 70%|███████   | 2875/4096 [23:18<08:50,  2.30it/s] 70%|███████   | 2876/4096 [23:18<08:50,  2.30it/s] 70%|███████   | 2877/4096 [23:18<08:49,  2.30it/s] 70%|███████   | 2878/4096 [23:19<08:48,  2.30it/s] 70%|███████   | 2879/4096 [23:19<08:48,  2.30it/s] 70%|███████   | 2880/4096 [23:20<08:47,  2.31it/s] 70%|███████   | 2881/4096 [23:20<08:46,  2.31it/s] 70%|███████   | 2882/4096 [23:21<08:45,  2.31it/s] 70%|███████   | 2883/4096 [23:21<08:45,  2.31it/s] 70%|███████   | 2884/4096 [23:21<08:45,  2.31it/s] 70%|███████   | 2885/4096 [23:22<08:45,  2.31it/s] 70%|███████   | 2886/4096 [23:22<08:45,  2.30it/s] 70%|███████   | 2887/4096 [23:23<08:45,  2.30it/s] 71%|███████   | 2888/4096 [23:23<08:44,  2.30it/s] 71%|███████   | 2889/4096 [23:24<08:44,  2.30it/s] 71%|███████   | 2890/4096 [23:24<08:43,  2.30it/s] 71%|███████   | 2891/4096 [23:25<08:43,  2.30it/s] 71%|███████   | 2892/4096 [23:25<08:42,  2.30it/s] 71%|███████   | 2893/4096 [23:25<08:41,  2.31it/s] 71%|███████   | 2894/4096 [23:26<08:40,  2.31it/s] 71%|███████   | 2895/4096 [23:26<08:39,  2.31it/s] 71%|███████   | 2896/4096 [23:27<08:37,  2.32it/s] 71%|███████   | 2897/4096 [23:27<08:37,  2.32it/s] 71%|███████   | 2898/4096 [23:28<08:37,  2.31it/s] 71%|███████   | 2899/4096 [23:28<08:38,  2.31it/s] 71%|███████   | 2900/4096 [23:28<08:38,  2.31it/s] 71%|███████   | 2901/4096 [23:29<08:38,  2.31it/s] 71%|███████   | 2902/4096 [23:29<08:38,  2.30it/s] 71%|███████   | 2903/4096 [23:30<08:37,  2.30it/s] 71%|███████   | 2904/4096 [23:30<08:37,  2.31it/s] 71%|███████   | 2905/4096 [23:31<08:36,  2.31it/s] 71%|███████   | 2906/4096 [23:31<08:36,  2.31it/s] 71%|███████   | 2907/4096 [23:31<08:36,  2.30it/s] 71%|███████   | 2908/4096 [23:32<08:41,  2.28it/s] 71%|███████   | 2909/4096 [23:32<08:39,  2.29it/s] 71%|███████   | 2910/4096 [23:33<08:38,  2.29it/s] 71%|███████   | 2911/4096 [23:33<08:36,  2.29it/s] 71%|███████   | 2912/4096 [23:34<08:36,  2.29it/s] 71%|███████   | 2913/4096 [23:34<08:34,  2.30it/s] 71%|███████   | 2914/4096 [23:35<08:33,  2.30it/s] 71%|███████   | 2915/4096 [23:35<08:32,  2.30it/s] 71%|███████   | 2916/4096 [23:35<08:32,  2.30it/s] 71%|███████   | 2917/4096 [23:36<08:31,  2.30it/s] 71%|███████   | 2918/4096 [23:36<08:32,  2.30it/s] 71%|███████▏  | 2919/4096 [23:37<08:31,  2.30it/s] 71%|███████▏  | 2920/4096 [23:37<08:30,  2.30it/s] 71%|███████▏  | 2921/4096 [23:38<08:29,  2.31it/s] 71%|███████▏  | 2922/4096 [23:38<08:28,  2.31it/s] 71%|███████▏  | 2923/4096 [23:38<08:27,  2.31it/s] 71%|███████▏  | 2924/4096 [23:39<08:26,  2.31it/s] 71%|███████▏  | 2925/4096 [23:39<08:26,  2.31it/s] 71%|███████▏  | 2926/4096 [23:40<08:26,  2.31it/s] 71%|███████▏  | 2927/4096 [23:40<08:26,  2.31it/s] 71%|███████▏  | 2928/4096 [23:41<08:26,  2.31it/s] 72%|███████▏  | 2929/4096 [23:41<08:26,  2.30it/s] 72%|███████▏  | 2930/4096 [23:41<08:26,  2.30it/s] 72%|███████▏  | 2931/4096 [23:42<08:25,  2.30it/s] 72%|███████▏  | 2932/4096 [23:42<08:26,  2.30it/s] 72%|███████▏  | 2933/4096 [23:43<08:26,  2.30it/s] 72%|███████▏  | 2934/4096 [23:43<08:25,  2.30it/s] 72%|███████▏  | 2935/4096 [23:44<08:25,  2.30it/s] 72%|███████▏  | 2936/4096 [23:44<08:24,  2.30it/s] 72%|███████▏  | 2937/4096 [23:45<08:24,  2.30it/s] 72%|███████▏  | 2938/4096 [23:45<08:23,  2.30it/s] 72%|███████▏  | 2939/4096 [23:45<08:22,  2.30it/s] 72%|███████▏  | 2940/4096 [23:46<08:21,  2.30it/s] 72%|███████▏  | 2941/4096 [23:46<08:21,  2.30it/s] 72%|███████▏  | 2942/4096 [23:47<08:22,  2.30it/s] 72%|███████▏  | 2943/4096 [23:47<08:21,  2.30it/s] 72%|███████▏  | 2944/4096 [23:47<06:51,  2.80it/s]loss: 2.1994457244873047
===========================
epoch 23/32 | loss: 2.1994457244873047
---------------------------
example true genres: 
tensor([1, 0, 1, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 1, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9474835886214442
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['The impression was unmistakable that, whatever one may choose to call it, natural law is a functioning generality with a certain objective existence. Another question that arose was the nature of the dialogue itself. The stimulus from the confrontation of philosophical systems involving certain differences was undeniable.', "It is not good, Mr. Waddell : you will do him great harm''. There was no doubt that Herr Schaffner meant every word of what he said. Waddell came back from the door and sat on a bunk.", 'as a result buyers, the public, must be satisfied with second - rate teachers. But this is not the real problem ; ; the rub arises from the fact that teachers are usually paid on the basis of time served rather than quality.', "Let me set the record this time, and let me get back OK, so the German will give me the exclusive. And make my life different and better from this time on. Amen''.", "` ` Aristide!! I want you to find Monsieur Prieur at once and give him this money for the boy's purchase. There's $ 600 in gold in this chamois sack."]
---------------------------
example output paragraph: 
['The fact of that. the in the of be, do,, thely, not matter, principle. a certain degree.. The reason was would from to question of the law between. The question of the firsts the views of the aspects. thatueble.', "He was not much for but. Poaldy'` know not it to harm. '. He was a doubt that he Herrhaff had'no word in his he said.agrag had to to the office and said down the chairunk in", 'The the result, are the buyer can and have informed by the choice hand prices, The the is not a way deal of ; the publicbish from the lack that the are not not by fee basis of the.. than than.', "I me go up rest on morning. and I me know it to. I I rest will be you a same right I I you name back. better. the world.. I I ',", 'He ` I,ui,! You am you to see the forrandtain for the. I him a letter to the sum. s sake. I is s a 1, gold. the boxapise,.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.54it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.25it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.24it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.12it/s][A100%|██████████| 29/29 [00:00<00:00, 66.18it/s]
Mean Perplexity: 1287.9606507716608
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 72%|███████▏  | 2945/4096 [23:54<44:55,  2.34s/it] 72%|███████▏  | 2946/4096 [23:55<33:57,  1.77s/it] 72%|███████▏  | 2947/4096 [23:55<26:14,  1.37s/it] 72%|███████▏  | 2948/4096 [23:56<20:50,  1.09s/it] 72%|███████▏  | 2949/4096 [23:56<17:10,  1.11it/s] 72%|███████▏  | 2950/4096 [23:56<14:30,  1.32it/s] 72%|███████▏  | 2951/4096 [23:57<12:37,  1.51it/s] 72%|███████▏  | 2952/4096 [23:57<11:18,  1.69it/s] 72%|███████▏  | 2953/4096 [23:58<10:23,  1.83it/s] 72%|███████▏  | 2954/4096 [23:58<09:44,  1.95it/s] 72%|███████▏  | 2955/4096 [23:59<09:16,  2.05it/s] 72%|███████▏  | 2956/4096 [23:59<08:58,  2.12it/s] 72%|███████▏  | 2957/4096 [24:00<08:44,  2.17it/s] 72%|███████▏  | 2958/4096 [24:00<08:35,  2.21it/s] 72%|███████▏  | 2959/4096 [24:00<08:28,  2.24it/s] 72%|███████▏  | 2960/4096 [24:01<08:22,  2.26it/s] 72%|███████▏  | 2961/4096 [24:01<08:19,  2.27it/s] 72%|███████▏  | 2962/4096 [24:02<08:17,  2.28it/s] 72%|███████▏  | 2963/4096 [24:02<08:15,  2.29it/s] 72%|███████▏  | 2964/4096 [24:03<08:14,  2.29it/s] 72%|███████▏  | 2965/4096 [24:03<08:12,  2.30it/s] 72%|███████▏  | 2966/4096 [24:03<08:11,  2.30it/s] 72%|███████▏  | 2967/4096 [24:04<08:10,  2.30it/s] 72%|███████▏  | 2968/4096 [24:04<08:09,  2.30it/s] 72%|███████▏  | 2969/4096 [24:05<08:08,  2.31it/s] 73%|███████▎  | 2970/4096 [24:05<08:08,  2.31it/s] 73%|███████▎  | 2971/4096 [24:06<08:07,  2.31it/s] 73%|███████▎  | 2972/4096 [24:06<08:07,  2.31it/s] 73%|███████▎  | 2973/4096 [24:06<08:08,  2.30it/s] 73%|███████▎  | 2974/4096 [24:07<08:07,  2.30it/s] 73%|███████▎  | 2975/4096 [24:07<08:06,  2.30it/s] 73%|███████▎  | 2976/4096 [24:08<08:10,  2.29it/s] 73%|███████▎  | 2977/4096 [24:08<08:08,  2.29it/s] 73%|███████▎  | 2978/4096 [24:09<08:06,  2.30it/s] 73%|███████▎  | 2979/4096 [24:09<08:06,  2.30it/s] 73%|███████▎  | 2980/4096 [24:09<08:04,  2.30it/s] 73%|███████▎  | 2981/4096 [24:10<08:03,  2.30it/s] 73%|███████▎  | 2982/4096 [24:10<08:03,  2.31it/s] 73%|███████▎  | 2983/4096 [24:11<08:02,  2.31it/s] 73%|███████▎  | 2984/4096 [24:11<08:01,  2.31it/s] 73%|███████▎  | 2985/4096 [24:12<08:00,  2.31it/s] 73%|███████▎  | 2986/4096 [24:12<08:00,  2.31it/s] 73%|███████▎  | 2987/4096 [24:13<07:59,  2.31it/s] 73%|███████▎  | 2988/4096 [24:13<07:59,  2.31it/s] 73%|███████▎  | 2989/4096 [24:13<07:59,  2.31it/s] 73%|███████▎  | 2990/4096 [24:14<07:59,  2.31it/s] 73%|███████▎  | 2991/4096 [24:14<07:59,  2.31it/s] 73%|███████▎  | 2992/4096 [24:15<07:57,  2.31it/s] 73%|███████▎  | 2993/4096 [24:15<07:57,  2.31it/s] 73%|███████▎  | 2994/4096 [24:16<07:57,  2.31it/s] 73%|███████▎  | 2995/4096 [24:16<07:56,  2.31it/s] 73%|███████▎  | 2996/4096 [24:16<07:57,  2.31it/s] 73%|███████▎  | 2997/4096 [24:17<07:56,  2.31it/s] 73%|███████▎  | 2998/4096 [24:17<07:56,  2.31it/s] 73%|███████▎  | 2999/4096 [24:18<07:55,  2.31it/s] 73%|███████▎  | 3000/4096 [24:18<07:54,  2.31it/s] 73%|███████▎  | 3001/4096 [24:19<07:54,  2.31it/s] 73%|███████▎  | 3002/4096 [24:19<07:53,  2.31it/s] 73%|███████▎  | 3003/4096 [24:19<07:53,  2.31it/s] 73%|███████▎  | 3004/4096 [24:20<07:53,  2.31it/s] 73%|███████▎  | 3005/4096 [24:20<07:53,  2.31it/s] 73%|███████▎  | 3006/4096 [24:21<07:52,  2.31it/s] 73%|███████▎  | 3007/4096 [24:21<07:52,  2.30it/s] 73%|███████▎  | 3008/4096 [24:22<07:51,  2.31it/s] 73%|███████▎  | 3009/4096 [24:22<07:51,  2.31it/s] 73%|███████▎  | 3010/4096 [24:22<07:50,  2.31it/s] 74%|███████▎  | 3011/4096 [24:23<07:49,  2.31it/s] 74%|███████▎  | 3012/4096 [24:23<07:50,  2.30it/s] 74%|███████▎  | 3013/4096 [24:24<07:49,  2.31it/s] 74%|███████▎  | 3014/4096 [24:24<07:48,  2.31it/s] 74%|███████▎  | 3015/4096 [24:25<07:48,  2.31it/s] 74%|███████▎  | 3016/4096 [24:25<07:48,  2.30it/s] 74%|███████▎  | 3017/4096 [24:26<07:48,  2.31it/s] 74%|███████▎  | 3018/4096 [24:26<07:47,  2.30it/s] 74%|███████▎  | 3019/4096 [24:26<07:47,  2.30it/s] 74%|███████▎  | 3020/4096 [24:27<07:46,  2.30it/s] 74%|███████▍  | 3021/4096 [24:27<07:46,  2.30it/s] 74%|███████▍  | 3022/4096 [24:28<07:45,  2.31it/s] 74%|███████▍  | 3023/4096 [24:28<07:45,  2.31it/s] 74%|███████▍  | 3024/4096 [24:29<07:44,  2.31it/s] 74%|███████▍  | 3025/4096 [24:29<07:44,  2.31it/s] 74%|███████▍  | 3026/4096 [24:29<07:43,  2.31it/s] 74%|███████▍  | 3027/4096 [24:30<07:43,  2.30it/s] 74%|███████▍  | 3028/4096 [24:30<07:42,  2.31it/s] 74%|███████▍  | 3029/4096 [24:31<07:42,  2.31it/s] 74%|███████▍  | 3030/4096 [24:31<07:41,  2.31it/s] 74%|███████▍  | 3031/4096 [24:32<07:42,  2.31it/s] 74%|███████▍  | 3032/4096 [24:32<07:41,  2.31it/s] 74%|███████▍  | 3033/4096 [24:32<07:41,  2.31it/s] 74%|███████▍  | 3034/4096 [24:33<07:40,  2.31it/s] 74%|███████▍  | 3035/4096 [24:33<07:40,  2.31it/s] 74%|███████▍  | 3036/4096 [24:34<07:39,  2.31it/s] 74%|███████▍  | 3037/4096 [24:34<07:39,  2.31it/s] 74%|███████▍  | 3038/4096 [24:35<07:40,  2.30it/s] 74%|███████▍  | 3039/4096 [24:35<07:39,  2.30it/s] 74%|███████▍  | 3040/4096 [24:36<07:38,  2.30it/s] 74%|███████▍  | 3041/4096 [24:36<07:39,  2.30it/s] 74%|███████▍  | 3042/4096 [24:36<07:38,  2.30it/s] 74%|███████▍  | 3043/4096 [24:37<07:37,  2.30it/s] 74%|███████▍  | 3044/4096 [24:37<07:39,  2.29it/s] 74%|███████▍  | 3045/4096 [24:38<07:38,  2.29it/s] 74%|███████▍  | 3046/4096 [24:38<07:36,  2.30it/s] 74%|███████▍  | 3047/4096 [24:39<07:35,  2.30it/s] 74%|███████▍  | 3048/4096 [24:39<07:34,  2.30it/s] 74%|███████▍  | 3049/4096 [24:39<07:35,  2.30it/s] 74%|███████▍  | 3050/4096 [24:40<07:34,  2.30it/s] 74%|███████▍  | 3051/4096 [24:40<07:33,  2.31it/s] 75%|███████▍  | 3052/4096 [24:41<07:33,  2.30it/s] 75%|███████▍  | 3053/4096 [24:41<07:31,  2.31it/s] 75%|███████▍  | 3054/4096 [24:42<07:32,  2.31it/s] 75%|███████▍  | 3055/4096 [24:42<07:31,  2.31it/s] 75%|███████▍  | 3056/4096 [24:42<07:30,  2.31it/s] 75%|███████▍  | 3057/4096 [24:43<07:29,  2.31it/s] 75%|███████▍  | 3058/4096 [24:43<07:29,  2.31it/s] 75%|███████▍  | 3059/4096 [24:44<07:28,  2.31it/s] 75%|███████▍  | 3060/4096 [24:44<07:28,  2.31it/s] 75%|███████▍  | 3061/4096 [24:45<07:28,  2.31it/s] 75%|███████▍  | 3062/4096 [24:45<07:28,  2.31it/s] 75%|███████▍  | 3063/4096 [24:45<07:27,  2.31it/s] 75%|███████▍  | 3064/4096 [24:46<07:26,  2.31it/s] 75%|███████▍  | 3065/4096 [24:46<07:26,  2.31it/s] 75%|███████▍  | 3066/4096 [24:47<07:25,  2.31it/s] 75%|███████▍  | 3067/4096 [24:47<07:25,  2.31it/s] 75%|███████▍  | 3068/4096 [24:48<07:24,  2.31it/s] 75%|███████▍  | 3069/4096 [24:48<07:24,  2.31it/s] 75%|███████▍  | 3070/4096 [24:49<07:23,  2.31it/s] 75%|███████▍  | 3071/4096 [24:49<07:23,  2.31it/s] 75%|███████▌  | 3072/4096 [24:49<06:05,  2.80it/s]loss: 1.8460345268249512
===========================
epoch 24/32 | loss: 1.8460345268249512
---------------------------
example true genres: 
tensor([1, 0, 0, 1, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 0, 1, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9387308533916849
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["N. C.'s editorial ` ` Confrontation'' is a stunning piece of writing. I would hope that Sargent Shriver will encourage everyone entering the Peace Corps to read it. The important people to humanity are not the Khrushchevs and the Castros but the Schweitzers and the Dooleys, and the others like them whose names we will never know.", 'The first two or three days they went home early. All, that is, except Jack. He hung around the schoolhouse, watching through a window from outside while Miss Langford straightened desks and put the room in order.', 'The jungle did not retort. The sitter remained seated hugging the tree. Before long the atmosphere reverted to its old normalcy, and insects hummed and birds occasionally called.', "It was expected that the comparison of different approaches to ethics would produce a better grasp of each other's positions and better comprehension of one's own. But a realization that each group has much of substance to learn from the other also developed, and a strong conviction grew that each had insights and dimensions to contribute to ethically acceptable solutions of urgent political issues. One effect of the spirited give - and - take of these discussions was to focus attention on practical applications and the necessity", "But the ships are very slow now, and we don't get so many sailors any more''. The uptown crowd has moved in, and what girl worth her seventh veil would trade a turtleneck sweater for a button - down collar?? A short, tormented span"]
---------------------------
example output paragraph: 
["Theerve S.'s wordss ` Thegraing'', the statement statement of the that The believe never to thergent'mper'be the who the United Corps to join the. United news who see and the afraid onlyhrushchev who who the United Castro who the peoplehuwitzweier who the Communistsn who who the United who us. lives are have never forget.", 'The girl few days three days were were out to, The the they was the the for and The was up in door,, watching the the few, the the he Bartonston studied hiss and started the keys. the to', 'The trees was not seemlentt. The treestered sat in. the tree. The long the sun cooled to peaceful normal peaceful state. the the flutteredmed along hum. fluttered.', 'The is not to the public of the views to the and be a greater understanding of the other. s views. the understandingplacerehension of the of s own. The the more of the approach of a of the to the. the past. reflects from and the new understanding that from the individual a in understanding of be to the understanding understanding understanding. the problems problems. The of of this new of of up up for understanding - the considerations, to be on on the problems of to importance of', "The the girl are not big and. and the'' t want far much of. more than '. The girlturn is is been toward and and the is says to money - is be for little,.. a pinkless up shirt.? girl - thined girl of"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.61it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.63it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.83it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.84it/s][A100%|██████████| 29/29 [00:00<00:00, 66.86it/s]
Mean Perplexity: 1286.6728346175646
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 75%|███████▌  | 3073/4096 [24:56<39:12,  2.30s/it] 75%|███████▌  | 3074/4096 [24:56<29:38,  1.74s/it] 75%|███████▌  | 3075/4096 [24:57<23:02,  1.35s/it] 75%|███████▌  | 3076/4096 [24:57<18:18,  1.08s/it] 75%|███████▌  | 3077/4096 [24:58<15:00,  1.13it/s] 75%|███████▌  | 3078/4096 [24:58<12:47,  1.33it/s] 75%|███████▌  | 3079/4096 [24:59<11:07,  1.52it/s] 75%|███████▌  | 3080/4096 [24:59<09:58,  1.70it/s] 75%|███████▌  | 3081/4096 [24:59<09:10,  1.85it/s] 75%|███████▌  | 3082/4096 [25:00<08:36,  1.96it/s] 75%|███████▌  | 3083/4096 [25:00<08:12,  2.06it/s] 75%|███████▌  | 3084/4096 [25:01<07:56,  2.13it/s] 75%|███████▌  | 3085/4096 [25:01<07:44,  2.18it/s] 75%|███████▌  | 3086/4096 [25:02<07:35,  2.22it/s] 75%|███████▌  | 3087/4096 [25:02<07:28,  2.25it/s] 75%|███████▌  | 3088/4096 [25:02<07:25,  2.26it/s] 75%|███████▌  | 3089/4096 [25:03<07:21,  2.28it/s] 75%|███████▌  | 3090/4096 [25:03<07:18,  2.29it/s] 75%|███████▌  | 3091/4096 [25:04<07:17,  2.30it/s] 75%|███████▌  | 3092/4096 [25:04<07:16,  2.30it/s] 76%|███████▌  | 3093/4096 [25:05<07:15,  2.30it/s] 76%|███████▌  | 3094/4096 [25:05<07:14,  2.30it/s] 76%|███████▌  | 3095/4096 [25:06<07:13,  2.31it/s] 76%|███████▌  | 3096/4096 [25:06<07:13,  2.30it/s] 76%|███████▌  | 3097/4096 [25:06<07:12,  2.31it/s] 76%|███████▌  | 3098/4096 [25:07<07:12,  2.31it/s] 76%|███████▌  | 3099/4096 [25:07<07:12,  2.31it/s] 76%|███████▌  | 3100/4096 [25:08<07:11,  2.31it/s] 76%|███████▌  | 3101/4096 [25:08<07:11,  2.31it/s] 76%|███████▌  | 3102/4096 [25:09<07:10,  2.31it/s] 76%|███████▌  | 3103/4096 [25:09<07:10,  2.31it/s] 76%|███████▌  | 3104/4096 [25:09<07:09,  2.31it/s] 76%|███████▌  | 3105/4096 [25:10<07:09,  2.31it/s] 76%|███████▌  | 3106/4096 [25:10<07:09,  2.31it/s] 76%|███████▌  | 3107/4096 [25:11<07:08,  2.31it/s] 76%|███████▌  | 3108/4096 [25:11<07:07,  2.31it/s] 76%|███████▌  | 3109/4096 [25:12<07:07,  2.31it/s] 76%|███████▌  | 3110/4096 [25:12<07:06,  2.31it/s] 76%|███████▌  | 3111/4096 [25:12<07:06,  2.31it/s] 76%|███████▌  | 3112/4096 [25:13<07:06,  2.31it/s] 76%|███████▌  | 3113/4096 [25:13<07:06,  2.31it/s] 76%|███████▌  | 3114/4096 [25:14<07:05,  2.31it/s] 76%|███████▌  | 3115/4096 [25:14<07:05,  2.31it/s] 76%|███████▌  | 3116/4096 [25:15<07:04,  2.31it/s] 76%|███████▌  | 3117/4096 [25:15<07:04,  2.31it/s] 76%|███████▌  | 3118/4096 [25:15<07:04,  2.31it/s] 76%|███████▌  | 3119/4096 [25:16<07:03,  2.31it/s] 76%|███████▌  | 3120/4096 [25:16<07:02,  2.31it/s] 76%|███████▌  | 3121/4096 [25:17<07:02,  2.31it/s] 76%|███████▌  | 3122/4096 [25:17<07:01,  2.31it/s] 76%|███████▌  | 3123/4096 [25:18<07:01,  2.31it/s] 76%|███████▋  | 3124/4096 [25:18<07:00,  2.31it/s] 76%|███████▋  | 3125/4096 [25:19<06:59,  2.31it/s] 76%|███████▋  | 3126/4096 [25:19<06:59,  2.31it/s] 76%|███████▋  | 3127/4096 [25:19<06:59,  2.31it/s] 76%|███████▋  | 3128/4096 [25:20<06:58,  2.31it/s] 76%|███████▋  | 3129/4096 [25:20<06:58,  2.31it/s] 76%|███████▋  | 3130/4096 [25:21<06:58,  2.31it/s] 76%|███████▋  | 3131/4096 [25:21<06:57,  2.31it/s] 76%|███████▋  | 3132/4096 [25:22<06:58,  2.31it/s] 76%|███████▋  | 3133/4096 [25:22<06:57,  2.31it/s] 77%|███████▋  | 3134/4096 [25:22<06:56,  2.31it/s] 77%|███████▋  | 3135/4096 [25:23<06:56,  2.31it/s] 77%|███████▋  | 3136/4096 [25:23<06:56,  2.31it/s] 77%|███████▋  | 3137/4096 [25:24<06:55,  2.31it/s] 77%|███████▋  | 3138/4096 [25:24<06:55,  2.31it/s] 77%|███████▋  | 3139/4096 [25:25<06:55,  2.30it/s] 77%|███████▋  | 3140/4096 [25:25<06:55,  2.30it/s] 77%|███████▋  | 3141/4096 [25:25<06:54,  2.30it/s] 77%|███████▋  | 3142/4096 [25:26<06:54,  2.30it/s] 77%|███████▋  | 3143/4096 [25:26<06:53,  2.30it/s] 77%|███████▋  | 3144/4096 [25:27<06:52,  2.31it/s] 77%|███████▋  | 3145/4096 [25:27<06:52,  2.31it/s] 77%|███████▋  | 3146/4096 [25:28<06:52,  2.30it/s] 77%|███████▋  | 3147/4096 [25:28<06:51,  2.31it/s] 77%|███████▋  | 3148/4096 [25:28<06:50,  2.31it/s] 77%|███████▋  | 3149/4096 [25:29<06:50,  2.31it/s] 77%|███████▋  | 3150/4096 [25:29<06:49,  2.31it/s] 77%|███████▋  | 3151/4096 [25:30<06:48,  2.31it/s] 77%|███████▋  | 3152/4096 [25:30<06:48,  2.31it/s] 77%|███████▋  | 3153/4096 [25:31<06:47,  2.31it/s] 77%|███████▋  | 3154/4096 [25:31<06:47,  2.31it/s] 77%|███████▋  | 3155/4096 [25:32<06:47,  2.31it/s] 77%|███████▋  | 3156/4096 [25:32<06:46,  2.31it/s] 77%|███████▋  | 3157/4096 [25:32<06:46,  2.31it/s] 77%|███████▋  | 3158/4096 [25:33<06:46,  2.31it/s] 77%|███████▋  | 3159/4096 [25:33<06:45,  2.31it/s] 77%|███████▋  | 3160/4096 [25:34<06:45,  2.31it/s] 77%|███████▋  | 3161/4096 [25:34<06:44,  2.31it/s] 77%|███████▋  | 3162/4096 [25:35<06:44,  2.31it/s] 77%|███████▋  | 3163/4096 [25:35<06:44,  2.31it/s] 77%|███████▋  | 3164/4096 [25:35<06:44,  2.30it/s] 77%|███████▋  | 3165/4096 [25:36<06:43,  2.31it/s] 77%|███████▋  | 3166/4096 [25:36<06:43,  2.31it/s] 77%|███████▋  | 3167/4096 [25:37<06:42,  2.31it/s] 77%|███████▋  | 3168/4096 [25:37<06:42,  2.31it/s] 77%|███████▋  | 3169/4096 [25:38<06:42,  2.31it/s] 77%|███████▋  | 3170/4096 [25:38<06:41,  2.30it/s] 77%|███████▋  | 3171/4096 [25:38<06:41,  2.31it/s] 77%|███████▋  | 3172/4096 [25:39<06:40,  2.31it/s] 77%|███████▋  | 3173/4096 [25:39<06:39,  2.31it/s] 77%|███████▋  | 3174/4096 [25:40<06:39,  2.31it/s] 78%|███████▊  | 3175/4096 [25:40<06:38,  2.31it/s] 78%|███████▊  | 3176/4096 [25:41<06:37,  2.31it/s] 78%|███████▊  | 3177/4096 [25:41<06:37,  2.31it/s] 78%|███████▊  | 3178/4096 [25:41<06:37,  2.31it/s] 78%|███████▊  | 3179/4096 [25:42<06:36,  2.31it/s] 78%|███████▊  | 3180/4096 [25:42<06:36,  2.31it/s] 78%|███████▊  | 3181/4096 [25:43<06:36,  2.31it/s] 78%|███████▊  | 3182/4096 [25:43<06:35,  2.31it/s] 78%|███████▊  | 3183/4096 [25:44<06:35,  2.31it/s] 78%|███████▊  | 3184/4096 [25:44<06:35,  2.31it/s] 78%|███████▊  | 3185/4096 [25:45<06:35,  2.30it/s] 78%|███████▊  | 3186/4096 [25:45<06:34,  2.31it/s] 78%|███████▊  | 3187/4096 [25:45<06:34,  2.30it/s] 78%|███████▊  | 3188/4096 [25:46<06:34,  2.30it/s] 78%|███████▊  | 3189/4096 [25:46<06:33,  2.31it/s] 78%|███████▊  | 3190/4096 [25:47<06:32,  2.31it/s] 78%|███████▊  | 3191/4096 [25:47<06:32,  2.31it/s] 78%|███████▊  | 3192/4096 [25:48<06:32,  2.31it/s] 78%|███████▊  | 3193/4096 [25:48<06:31,  2.31it/s] 78%|███████▊  | 3194/4096 [25:48<06:30,  2.31it/s] 78%|███████▊  | 3195/4096 [25:49<06:29,  2.31it/s] 78%|███████▊  | 3196/4096 [25:49<06:29,  2.31it/s] 78%|███████▊  | 3197/4096 [25:50<06:29,  2.31it/s] 78%|███████▊  | 3198/4096 [25:50<06:28,  2.31it/s] 78%|███████▊  | 3199/4096 [25:51<06:28,  2.31it/s] 78%|███████▊  | 3200/4096 [25:51<05:19,  2.80it/s]loss: 2.668126106262207
===========================
epoch 25/32 | loss: 2.668126106262207
---------------------------
example true genres: 
tensor([0, 0, 0, 0, 1], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 0, 0, 1], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.949671772428884
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["If the old fool argues about the price, tell him I shall order my husband not to treat him as a patient any longer. Prieur has gout and depends on Louis'pills and bleedings. Besides, he owns 300 slaves.", "` ` Hell you say''?? He said, lapsing into the profanity he often used when away from his parents and especially when he was with Charles. ` ` How'd you do it''??", 'All tourists come to Florida. This will help him to get out of his little tackle shop. Yes!!', "The man tilted back his head and went through the pantomime of drinking from a container. He performed the act twice more, and the begging in his tone grew more distinct. ` ` Sake''??", "` ` Molotov altogether rejects the line of peaceful coexistence, reducing this concept merely to the state of peace or rather, the absence of war at a given moment, and to a denial of the possibility of averting a world war. His views, in fact, coincide with those of foreign enemies of peaceful coexistence, who look upon it merely as a variant of the ` ` cold war'' or of an ` ` armed peace''. One"]
---------------------------
example output paragraph: 
['He he man mans, the son of he him, will have him own. to be him as a child. man. Heeerct, no to to demands on the. s. his.., he is no,,', "` ` I'' it '?? He said, `e his the seatnounceity. had did to he from his wife. his when he was trying the. ` ` I do s you do that? '??", 'He of are to town. He is be me get get back of town pocket town..,!', "He man'his and head and said on the door.ically. the. the bottle. He said the same,,, and then other. his mouth. louder urgent. ` ` Ix'',?", "The ` Thergenh'is the right of de,herenceistence, but the of of to the point'the. peace, as right of a, all time time. and the the sense that the right that a nuclearing a nuclear '. The view are in the, justify with the of the policy, the nationsvetistence, have have at the as as a neutral of the Western ` ` war''. the the un `'struggle''. view"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.25it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.35it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.40it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.38it/s][A100%|██████████| 29/29 [00:00<00:00, 65.47it/s]
Mean Perplexity: 1244.2320317575134
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 78%|███████▊  | 3201/4096 [25:58<34:30,  2.31s/it] 78%|███████▊  | 3202/4096 [25:58<26:04,  1.75s/it] 78%|███████▊  | 3203/4096 [25:58<20:10,  1.36s/it] 78%|███████▊  | 3204/4096 [25:59<16:02,  1.08s/it] 78%|███████▊  | 3205/4096 [25:59<13:08,  1.13it/s] 78%|███████▊  | 3206/4096 [26:00<11:06,  1.33it/s] 78%|███████▊  | 3207/4096 [26:00<09:41,  1.53it/s] 78%|███████▊  | 3208/4096 [26:01<08:42,  1.70it/s] 78%|███████▊  | 3209/4096 [26:01<07:59,  1.85it/s] 78%|███████▊  | 3210/4096 [26:02<07:30,  1.97it/s] 78%|███████▊  | 3211/4096 [26:02<07:09,  2.06it/s] 78%|███████▊  | 3212/4096 [26:02<06:55,  2.13it/s] 78%|███████▊  | 3213/4096 [26:03<06:44,  2.18it/s] 78%|███████▊  | 3214/4096 [26:03<06:37,  2.22it/s] 78%|███████▊  | 3215/4096 [26:04<06:32,  2.24it/s] 79%|███████▊  | 3216/4096 [26:04<06:28,  2.26it/s] 79%|███████▊  | 3217/4096 [26:05<06:28,  2.26it/s] 79%|███████▊  | 3218/4096 [26:05<06:26,  2.27it/s] 79%|███████▊  | 3219/4096 [26:05<06:24,  2.28it/s] 79%|███████▊  | 3220/4096 [26:06<06:22,  2.29it/s] 79%|███████▊  | 3221/4096 [26:06<06:21,  2.30it/s] 79%|███████▊  | 3222/4096 [26:07<06:19,  2.30it/s] 79%|███████▊  | 3223/4096 [26:07<06:18,  2.30it/s] 79%|███████▊  | 3224/4096 [26:08<06:17,  2.31it/s] 79%|███████▊  | 3225/4096 [26:08<06:17,  2.31it/s] 79%|███████▉  | 3226/4096 [26:08<06:16,  2.31it/s] 79%|███████▉  | 3227/4096 [26:09<06:16,  2.31it/s] 79%|███████▉  | 3228/4096 [26:09<06:16,  2.31it/s] 79%|███████▉  | 3229/4096 [26:10<06:16,  2.30it/s] 79%|███████▉  | 3230/4096 [26:10<06:16,  2.30it/s] 79%|███████▉  | 3231/4096 [26:11<06:15,  2.30it/s] 79%|███████▉  | 3232/4096 [26:11<06:14,  2.30it/s] 79%|███████▉  | 3233/4096 [26:12<06:14,  2.31it/s] 79%|███████▉  | 3234/4096 [26:12<06:13,  2.30it/s] 79%|███████▉  | 3235/4096 [26:12<06:13,  2.31it/s] 79%|███████▉  | 3236/4096 [26:13<06:12,  2.31it/s] 79%|███████▉  | 3237/4096 [26:13<06:12,  2.31it/s] 79%|███████▉  | 3238/4096 [26:14<06:11,  2.31it/s] 79%|███████▉  | 3239/4096 [26:14<06:11,  2.31it/s] 79%|███████▉  | 3240/4096 [26:15<06:11,  2.31it/s] 79%|███████▉  | 3241/4096 [26:15<06:10,  2.31it/s] 79%|███████▉  | 3242/4096 [26:15<06:10,  2.31it/s] 79%|███████▉  | 3243/4096 [26:16<06:09,  2.31it/s] 79%|███████▉  | 3244/4096 [26:16<06:09,  2.31it/s] 79%|███████▉  | 3245/4096 [26:17<06:08,  2.31it/s] 79%|███████▉  | 3246/4096 [26:17<06:08,  2.31it/s] 79%|███████▉  | 3247/4096 [26:18<06:07,  2.31it/s] 79%|███████▉  | 3248/4096 [26:18<06:06,  2.31it/s] 79%|███████▉  | 3249/4096 [26:18<06:06,  2.31it/s] 79%|███████▉  | 3250/4096 [26:19<06:06,  2.31it/s] 79%|███████▉  | 3251/4096 [26:19<06:06,  2.31it/s] 79%|███████▉  | 3252/4096 [26:20<06:05,  2.31it/s] 79%|███████▉  | 3253/4096 [26:20<06:04,  2.31it/s] 79%|███████▉  | 3254/4096 [26:21<06:05,  2.31it/s] 79%|███████▉  | 3255/4096 [26:21<06:04,  2.31it/s] 79%|███████▉  | 3256/4096 [26:21<06:03,  2.31it/s] 80%|███████▉  | 3257/4096 [26:22<06:03,  2.31it/s] 80%|███████▉  | 3258/4096 [26:22<06:03,  2.31it/s] 80%|███████▉  | 3259/4096 [26:23<06:02,  2.31it/s] 80%|███████▉  | 3260/4096 [26:23<06:02,  2.31it/s] 80%|███████▉  | 3261/4096 [26:24<06:01,  2.31it/s] 80%|███████▉  | 3262/4096 [26:24<06:01,  2.31it/s] 80%|███████▉  | 3263/4096 [26:24<06:00,  2.31it/s] 80%|███████▉  | 3264/4096 [26:25<06:01,  2.30it/s] 80%|███████▉  | 3265/4096 [26:25<06:00,  2.30it/s] 80%|███████▉  | 3266/4096 [26:26<06:00,  2.30it/s] 80%|███████▉  | 3267/4096 [26:26<05:59,  2.31it/s] 80%|███████▉  | 3268/4096 [26:27<05:58,  2.31it/s] 80%|███████▉  | 3269/4096 [26:27<05:58,  2.31it/s] 80%|███████▉  | 3270/4096 [26:28<05:57,  2.31it/s] 80%|███████▉  | 3271/4096 [26:28<05:57,  2.31it/s] 80%|███████▉  | 3272/4096 [26:28<05:56,  2.31it/s] 80%|███████▉  | 3273/4096 [26:29<05:56,  2.31it/s] 80%|███████▉  | 3274/4096 [26:29<05:56,  2.31it/s] 80%|███████▉  | 3275/4096 [26:30<05:55,  2.31it/s] 80%|███████▉  | 3276/4096 [26:30<05:55,  2.31it/s] 80%|████████  | 3277/4096 [26:31<05:54,  2.31it/s] 80%|████████  | 3278/4096 [26:31<05:54,  2.31it/s] 80%|████████  | 3279/4096 [26:31<05:55,  2.30it/s] 80%|████████  | 3280/4096 [26:32<05:54,  2.30it/s] 80%|████████  | 3281/4096 [26:32<05:54,  2.30it/s] 80%|████████  | 3282/4096 [26:33<05:53,  2.30it/s] 80%|████████  | 3283/4096 [26:33<05:53,  2.30it/s] 80%|████████  | 3284/4096 [26:34<05:52,  2.30it/s] 80%|████████  | 3285/4096 [26:34<05:51,  2.31it/s] 80%|████████  | 3286/4096 [26:34<05:51,  2.31it/s] 80%|████████  | 3287/4096 [26:35<05:50,  2.31it/s] 80%|████████  | 3288/4096 [26:35<05:50,  2.30it/s] 80%|████████  | 3289/4096 [26:36<05:50,  2.30it/s] 80%|████████  | 3290/4096 [26:36<05:49,  2.31it/s] 80%|████████  | 3291/4096 [26:37<05:48,  2.31it/s] 80%|████████  | 3292/4096 [26:37<05:48,  2.31it/s] 80%|████████  | 3293/4096 [26:38<05:48,  2.31it/s] 80%|████████  | 3294/4096 [26:38<05:47,  2.31it/s] 80%|████████  | 3295/4096 [26:38<05:46,  2.31it/s] 80%|████████  | 3296/4096 [26:39<05:46,  2.31it/s] 80%|████████  | 3297/4096 [26:39<05:45,  2.31it/s] 81%|████████  | 3298/4096 [26:40<05:45,  2.31it/s] 81%|████████  | 3299/4096 [26:40<05:45,  2.31it/s] 81%|████████  | 3300/4096 [26:41<05:44,  2.31it/s] 81%|████████  | 3301/4096 [26:41<05:44,  2.31it/s] 81%|████████  | 3302/4096 [26:41<05:44,  2.31it/s] 81%|████████  | 3303/4096 [26:42<05:43,  2.31it/s] 81%|████████  | 3304/4096 [26:42<05:43,  2.31it/s] 81%|████████  | 3305/4096 [26:43<05:43,  2.31it/s] 81%|████████  | 3306/4096 [26:43<05:42,  2.31it/s] 81%|████████  | 3307/4096 [26:44<05:42,  2.30it/s] 81%|████████  | 3308/4096 [26:44<05:41,  2.31it/s] 81%|████████  | 3309/4096 [26:44<05:41,  2.31it/s] 81%|████████  | 3310/4096 [26:45<05:40,  2.31it/s] 81%|████████  | 3311/4096 [26:45<05:40,  2.31it/s] 81%|████████  | 3312/4096 [26:46<05:39,  2.31it/s] 81%|████████  | 3313/4096 [26:46<05:39,  2.31it/s] 81%|████████  | 3314/4096 [26:47<05:39,  2.30it/s] 81%|████████  | 3315/4096 [26:47<05:39,  2.30it/s] 81%|████████  | 3316/4096 [26:47<05:38,  2.31it/s] 81%|████████  | 3317/4096 [26:48<05:38,  2.30it/s] 81%|████████  | 3318/4096 [26:48<05:37,  2.30it/s] 81%|████████  | 3319/4096 [26:49<05:37,  2.30it/s] 81%|████████  | 3320/4096 [26:49<05:37,  2.30it/s] 81%|████████  | 3321/4096 [26:50<05:37,  2.29it/s] 81%|████████  | 3322/4096 [26:50<05:37,  2.29it/s] 81%|████████  | 3323/4096 [26:51<05:36,  2.30it/s] 81%|████████  | 3324/4096 [26:51<05:35,  2.30it/s] 81%|████████  | 3325/4096 [26:51<05:35,  2.30it/s] 81%|████████  | 3326/4096 [26:52<05:34,  2.30it/s] 81%|████████  | 3327/4096 [26:52<05:33,  2.30it/s] 81%|████████▏ | 3328/4096 [26:52<04:34,  2.80it/s]loss: 2.350902557373047
===========================
epoch 26/32 | loss: 2.350902557373047
---------------------------
example true genres: 
tensor([1, 1, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9518599562363238
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["No matter that the Katanga operation is strategically insane in terms of Western interests in Africa. ( Even granted that the Congo should be unified, you don't protect Western security by first removing the pro - Western weight from the power equilibrium. )", 'A 60 mm. mortar and a 57 mm. recoilless rifle owned by Lauchli were brought along. The mortar was equipped with dummy shells and the recoilless rifle was deactivated. After a tortuous drive in an open truck and a World War 2, army jeep down soggy trails, the band arrived at a small clearing squeezed between a long, low ridge and a creek - filled gully.', "` ` Another youth, Madame''?? The coachman said softly. ` ` This one is a tender chicken, oui??", "You did this you like to hurt to beat people I want to go home''. These were the last words he ever uttered. Convulsively, he spat up some blood and collapsed into the arms of Senator Gaston Berche, crimsoning the frilly shirt and waistcoat the politician wore.", "There is nothing for you'', Matsuo said. ` ` Your superiors will certainly beat you for your desertion, besides the dishonor of it. I've nothing for you''."]
---------------------------
example output paragraph: 
["The more how the countryanga has has a important. the of the interests. the. The The the the the United is be a in it see't think the interests, the entering the threat - Western powers of the West of.", 'The small - rifle As rifle rifle - rifle Asued rifle was a adernerg a in with The rifles a with ayte rifles. a cartridgebu was rifle. aton. a shortggleify flight, the area area, a truck War 2, the wasoggeds theak terrain were the truck was at a post field. into a tree line narrow,. a few. sized areaully.', "` ` I man'I, '?? ` girl was said,. ` ` You is is the newest. a '.?", "He'not to'to do him death him with said to do on to '. He men men men words he spoke heard. `cedinglsively he he shook out the of from spat to the dirt of the Barton Claytonauer. his and his manroths face. hisless. shirt '.", "` is no to you to '. heinji said. ` ` I life'not not you'this crimeser. and you otheroutunta'your. don ve got to you to '."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 64.73it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.29it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.67it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.69it/s][A100%|██████████| 29/29 [00:00<00:00, 65.66it/s]
Mean Perplexity: 1198.503896767514
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 81%|████████▏ | 3329/4096 [26:59<29:34,  2.31s/it] 81%|████████▏ | 3330/4096 [27:00<22:19,  1.75s/it] 81%|████████▏ | 3331/4096 [27:00<17:15,  1.35s/it] 81%|████████▏ | 3332/4096 [27:01<13:44,  1.08s/it] 81%|████████▏ | 3333/4096 [27:01<11:15,  1.13it/s] 81%|████████▏ | 3334/4096 [27:01<09:30,  1.33it/s] 81%|████████▏ | 3335/4096 [27:02<08:17,  1.53it/s] 81%|████████▏ | 3336/4096 [27:02<07:27,  1.70it/s] 81%|████████▏ | 3337/4096 [27:03<06:51,  1.84it/s] 81%|████████▏ | 3338/4096 [27:03<06:26,  1.96it/s] 82%|████████▏ | 3339/4096 [27:04<06:08,  2.06it/s] 82%|████████▏ | 3340/4096 [27:04<05:55,  2.12it/s] 82%|████████▏ | 3341/4096 [27:05<05:46,  2.18it/s] 82%|████████▏ | 3342/4096 [27:05<05:40,  2.22it/s] 82%|████████▏ | 3343/4096 [27:05<05:35,  2.24it/s] 82%|████████▏ | 3344/4096 [27:06<05:32,  2.26it/s] 82%|████████▏ | 3345/4096 [27:06<05:30,  2.27it/s] 82%|████████▏ | 3346/4096 [27:07<05:28,  2.28it/s] 82%|████████▏ | 3347/4096 [27:07<05:27,  2.29it/s] 82%|████████▏ | 3348/4096 [27:08<05:25,  2.30it/s] 82%|████████▏ | 3349/4096 [27:08<05:24,  2.30it/s] 82%|████████▏ | 3350/4096 [27:08<05:24,  2.30it/s] 82%|████████▏ | 3351/4096 [27:09<05:23,  2.30it/s] 82%|████████▏ | 3352/4096 [27:09<05:22,  2.30it/s] 82%|████████▏ | 3353/4096 [27:10<05:22,  2.30it/s] 82%|████████▏ | 3354/4096 [27:10<05:22,  2.30it/s] 82%|████████▏ | 3355/4096 [27:11<05:21,  2.30it/s] 82%|████████▏ | 3356/4096 [27:11<05:21,  2.30it/s] 82%|████████▏ | 3357/4096 [27:11<05:20,  2.30it/s] 82%|████████▏ | 3358/4096 [27:12<05:20,  2.30it/s] 82%|████████▏ | 3359/4096 [27:12<05:20,  2.30it/s] 82%|████████▏ | 3360/4096 [27:13<05:19,  2.30it/s] 82%|████████▏ | 3361/4096 [27:13<05:19,  2.30it/s] 82%|████████▏ | 3362/4096 [27:14<05:18,  2.31it/s] 82%|████████▏ | 3363/4096 [27:14<05:17,  2.31it/s] 82%|████████▏ | 3364/4096 [27:14<05:17,  2.31it/s] 82%|████████▏ | 3365/4096 [27:15<05:16,  2.31it/s] 82%|████████▏ | 3366/4096 [27:15<05:16,  2.31it/s] 82%|████████▏ | 3367/4096 [27:16<05:15,  2.31it/s] 82%|████████▏ | 3368/4096 [27:16<05:15,  2.31it/s] 82%|████████▏ | 3369/4096 [27:17<05:14,  2.31it/s] 82%|████████▏ | 3370/4096 [27:17<05:14,  2.31it/s] 82%|████████▏ | 3371/4096 [27:18<05:13,  2.31it/s] 82%|████████▏ | 3372/4096 [27:18<05:14,  2.31it/s] 82%|████████▏ | 3373/4096 [27:18<05:13,  2.30it/s] 82%|████████▏ | 3374/4096 [27:19<05:12,  2.31it/s] 82%|████████▏ | 3375/4096 [27:19<05:12,  2.31it/s] 82%|████████▏ | 3376/4096 [27:20<05:11,  2.31it/s] 82%|████████▏ | 3377/4096 [27:20<05:10,  2.31it/s] 82%|████████▏ | 3378/4096 [27:21<05:10,  2.31it/s] 82%|████████▏ | 3379/4096 [27:21<05:10,  2.31it/s] 83%|████████▎ | 3380/4096 [27:21<05:10,  2.31it/s] 83%|████████▎ | 3381/4096 [27:22<05:09,  2.31it/s] 83%|████████▎ | 3382/4096 [27:22<05:09,  2.31it/s] 83%|████████▎ | 3383/4096 [27:23<05:08,  2.31it/s] 83%|████████▎ | 3384/4096 [27:23<05:08,  2.31it/s] 83%|████████▎ | 3385/4096 [27:24<05:07,  2.31it/s] 83%|████████▎ | 3386/4096 [27:24<05:07,  2.31it/s] 83%|████████▎ | 3387/4096 [27:24<05:07,  2.31it/s] 83%|████████▎ | 3388/4096 [27:25<05:07,  2.30it/s] 83%|████████▎ | 3389/4096 [27:25<05:07,  2.30it/s] 83%|████████▎ | 3390/4096 [27:26<05:07,  2.30it/s] 83%|████████▎ | 3391/4096 [27:26<05:06,  2.30it/s] 83%|████████▎ | 3392/4096 [27:27<05:06,  2.30it/s] 83%|████████▎ | 3393/4096 [27:27<05:06,  2.30it/s] 83%|████████▎ | 3394/4096 [27:28<05:05,  2.30it/s] 83%|████████▎ | 3395/4096 [27:28<05:04,  2.30it/s] 83%|████████▎ | 3396/4096 [27:28<05:04,  2.30it/s] 83%|████████▎ | 3397/4096 [27:29<05:03,  2.30it/s] 83%|████████▎ | 3398/4096 [27:29<05:02,  2.31it/s] 83%|████████▎ | 3399/4096 [27:30<05:01,  2.31it/s] 83%|████████▎ | 3400/4096 [27:30<05:01,  2.31it/s] 83%|████████▎ | 3401/4096 [27:31<05:00,  2.31it/s] 83%|████████▎ | 3402/4096 [27:31<05:00,  2.31it/s] 83%|████████▎ | 3403/4096 [27:31<05:00,  2.31it/s] 83%|████████▎ | 3404/4096 [27:32<05:00,  2.31it/s] 83%|████████▎ | 3405/4096 [27:32<04:59,  2.31it/s] 83%|████████▎ | 3406/4096 [27:33<04:59,  2.31it/s] 83%|████████▎ | 3407/4096 [27:33<04:58,  2.31it/s] 83%|████████▎ | 3408/4096 [27:34<04:57,  2.31it/s] 83%|████████▎ | 3409/4096 [27:34<04:57,  2.31it/s] 83%|████████▎ | 3410/4096 [27:34<04:57,  2.31it/s] 83%|████████▎ | 3411/4096 [27:35<04:57,  2.30it/s] 83%|████████▎ | 3412/4096 [27:35<04:56,  2.31it/s] 83%|████████▎ | 3413/4096 [27:36<04:56,  2.31it/s] 83%|████████▎ | 3414/4096 [27:36<04:55,  2.30it/s] 83%|████████▎ | 3415/4096 [27:37<04:55,  2.30it/s] 83%|████████▎ | 3416/4096 [27:37<04:55,  2.30it/s] 83%|████████▎ | 3417/4096 [27:37<04:54,  2.30it/s] 83%|████████▎ | 3418/4096 [27:38<04:53,  2.31it/s] 83%|████████▎ | 3419/4096 [27:38<04:53,  2.31it/s] 83%|████████▎ | 3420/4096 [27:39<04:53,  2.31it/s] 84%|████████▎ | 3421/4096 [27:39<04:52,  2.31it/s] 84%|████████▎ | 3422/4096 [27:40<04:51,  2.31it/s] 84%|████████▎ | 3423/4096 [27:40<04:51,  2.31it/s] 84%|████████▎ | 3424/4096 [27:41<04:51,  2.31it/s] 84%|████████▎ | 3425/4096 [27:41<04:51,  2.31it/s] 84%|████████▎ | 3426/4096 [27:41<04:50,  2.31it/s] 84%|████████▎ | 3427/4096 [27:42<04:49,  2.31it/s] 84%|████████▎ | 3428/4096 [27:42<04:49,  2.31it/s] 84%|████████▎ | 3429/4096 [27:43<04:49,  2.31it/s] 84%|████████▎ | 3430/4096 [27:43<04:48,  2.31it/s] 84%|████████▍ | 3431/4096 [27:44<04:48,  2.30it/s] 84%|████████▍ | 3432/4096 [27:44<04:47,  2.31it/s] 84%|████████▍ | 3433/4096 [27:44<04:47,  2.31it/s] 84%|████████▍ | 3434/4096 [27:45<04:46,  2.31it/s] 84%|████████▍ | 3435/4096 [27:45<04:45,  2.31it/s] 84%|████████▍ | 3436/4096 [27:46<04:45,  2.31it/s] 84%|████████▍ | 3437/4096 [27:46<04:45,  2.31it/s] 84%|████████▍ | 3438/4096 [27:47<04:45,  2.31it/s] 84%|████████▍ | 3439/4096 [27:47<04:45,  2.30it/s] 84%|████████▍ | 3440/4096 [27:47<04:44,  2.30it/s] 84%|████████▍ | 3441/4096 [27:48<04:44,  2.30it/s] 84%|████████▍ | 3442/4096 [27:48<04:43,  2.30it/s] 84%|████████▍ | 3443/4096 [27:49<04:44,  2.30it/s] 84%|████████▍ | 3444/4096 [27:49<04:43,  2.30it/s] 84%|████████▍ | 3445/4096 [27:50<04:43,  2.30it/s] 84%|████████▍ | 3446/4096 [27:50<04:42,  2.30it/s] 84%|████████▍ | 3447/4096 [27:50<04:42,  2.30it/s] 84%|████████▍ | 3448/4096 [27:51<04:41,  2.30it/s] 84%|████████▍ | 3449/4096 [27:51<04:41,  2.30it/s] 84%|████████▍ | 3450/4096 [27:52<04:40,  2.30it/s] 84%|████████▍ | 3451/4096 [27:52<04:40,  2.30it/s] 84%|████████▍ | 3452/4096 [27:53<04:40,  2.30it/s] 84%|████████▍ | 3453/4096 [27:53<04:39,  2.30it/s] 84%|████████▍ | 3454/4096 [27:54<04:38,  2.30it/s] 84%|████████▍ | 3455/4096 [27:54<04:37,  2.31it/s] 84%|████████▍ | 3456/4096 [27:54<03:48,  2.81it/s]loss: 1.7123401165008545
===========================
epoch 27/32 | loss: 1.7123401165008545
---------------------------
example true genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 0, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9518599562363238
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['Along each side of the room were six tiered bunks, each one screened off with a curtain. And projecting wickedly through these curtains were the gleaming muzzles of six rifles, all trained on Billy Tilghman. The fighting marshal had walked right into a trap and at any moment six slugs might slam into his hide.', "Kitty inquired politely. ` ` Where from''?? He mused.", "The lad's once superb body was a mass of scars and welts. His pinched face showed the ravages of malnutrition. Feebly he pointed an accusing finger at Madame Lalaurie and shouted : ` ` Evil woman!!", 'The youth with the snake had a natural pride and joy of life which appealed to the woman. Lithe and muscular, he had well - molded features, and his light color told of the European ancestors who had been intimate with the slave women of his family. The haughty white girl turned to a distinguished, hawk - faced man standing at her side and murmured : ` ` Look at your watch, Col. Garvier.', "His room will be ready shortly''. The physician led the horses to the stable after a cursory glance at the cringing slave. Had Dandy been older or wiser, instinct might have warned him that he would be well advised to flee from the Lalauries'tender care if he valued his life."]
---------------------------
example output paragraph: 
["The the other of the hall, two mened windowsaless, and of of by the a gun. The the aly light the two, a men,s of a pistols. a of by the 'ruusker'The men was was been through into the corner, charged the moment. feethrs had have them the back.", "` nodded.. ` ` What '? '?? ` asked.", "The mans s face frightenedly was swollen savage of trembling. spt were The face face was a strainosyged of arar. Heatherbled, saw out armly at the 'e'' pointed to ` ` You!!!", 'He man was the g was been long beauty in a in his, he to him young. Heberthe the man muscular was been known knowned the with and he handsome eyes was him a beauty beauty of had been a with the Sioux.. the childhood. He manmm man man, a the man face blonded eyed face. beside the side. took a ` ` You at me face. Lord. Morganaliren.', "He mind was not empty for as '. The hall had him two to the hall and the momentspor glance at the twoy figure. He he suspected thought aware than lessr, heually not been him that he would not able off to leave. the houseandau.'quarters. he found himself life."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.00it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.45it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.53it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.46it/s][A100%|██████████| 29/29 [00:00<00:00, 66.55it/s]
Mean Perplexity: 1382.9300980849653
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 84%|████████▍ | 3457/4096 [28:01<24:36,  2.31s/it] 84%|████████▍ | 3458/4096 [28:01<18:34,  1.75s/it] 84%|████████▍ | 3459/4096 [28:02<14:22,  1.35s/it] 84%|████████▍ | 3460/4096 [28:02<11:31,  1.09s/it] 84%|████████▍ | 3461/4096 [28:03<09:25,  1.12it/s] 85%|████████▍ | 3462/4096 [28:03<07:57,  1.33it/s] 85%|████████▍ | 3463/4096 [28:04<06:56,  1.52it/s] 85%|████████▍ | 3464/4096 [28:04<06:12,  1.69it/s] 85%|████████▍ | 3465/4096 [28:05<05:43,  1.84it/s] 85%|████████▍ | 3466/4096 [28:05<05:21,  1.96it/s] 85%|████████▍ | 3467/4096 [28:05<05:07,  2.05it/s] 85%|████████▍ | 3468/4096 [28:06<04:56,  2.12it/s] 85%|████████▍ | 3469/4096 [28:06<04:48,  2.17it/s] 85%|████████▍ | 3470/4096 [28:07<04:43,  2.21it/s] 85%|████████▍ | 3471/4096 [28:07<04:39,  2.24it/s] 85%|████████▍ | 3472/4096 [28:08<04:36,  2.26it/s] 85%|████████▍ | 3473/4096 [28:08<04:34,  2.27it/s] 85%|████████▍ | 3474/4096 [28:08<04:32,  2.28it/s] 85%|████████▍ | 3475/4096 [28:09<04:31,  2.29it/s] 85%|████████▍ | 3476/4096 [28:09<04:30,  2.29it/s] 85%|████████▍ | 3477/4096 [28:10<04:30,  2.29it/s] 85%|████████▍ | 3478/4096 [28:10<04:29,  2.30it/s] 85%|████████▍ | 3479/4096 [28:11<04:28,  2.30it/s] 85%|████████▍ | 3480/4096 [28:11<04:27,  2.30it/s] 85%|████████▍ | 3481/4096 [28:11<04:27,  2.30it/s] 85%|████████▌ | 3482/4096 [28:12<04:27,  2.30it/s] 85%|████████▌ | 3483/4096 [28:12<04:26,  2.30it/s] 85%|████████▌ | 3484/4096 [28:13<04:25,  2.30it/s] 85%|████████▌ | 3485/4096 [28:13<04:25,  2.30it/s] 85%|████████▌ | 3486/4096 [28:14<04:24,  2.31it/s] 85%|████████▌ | 3487/4096 [28:14<04:23,  2.31it/s] 85%|████████▌ | 3488/4096 [28:14<04:23,  2.31it/s] 85%|████████▌ | 3489/4096 [28:15<04:22,  2.31it/s] 85%|████████▌ | 3490/4096 [28:15<04:22,  2.31it/s] 85%|████████▌ | 3491/4096 [28:16<04:22,  2.31it/s] 85%|████████▌ | 3492/4096 [28:16<04:21,  2.31it/s] 85%|████████▌ | 3493/4096 [28:17<04:21,  2.31it/s] 85%|████████▌ | 3494/4096 [28:17<04:20,  2.31it/s] 85%|████████▌ | 3495/4096 [28:18<04:20,  2.30it/s] 85%|████████▌ | 3496/4096 [28:18<04:20,  2.30it/s] 85%|████████▌ | 3497/4096 [28:18<04:19,  2.31it/s] 85%|████████▌ | 3498/4096 [28:19<04:19,  2.31it/s] 85%|████████▌ | 3499/4096 [28:19<04:18,  2.31it/s] 85%|████████▌ | 3500/4096 [28:20<04:18,  2.31it/s] 85%|████████▌ | 3501/4096 [28:20<04:18,  2.30it/s] 85%|████████▌ | 3502/4096 [28:21<04:17,  2.30it/s] 86%|████████▌ | 3503/4096 [28:21<04:17,  2.30it/s] 86%|████████▌ | 3504/4096 [28:21<04:17,  2.30it/s] 86%|████████▌ | 3505/4096 [28:22<04:16,  2.30it/s] 86%|████████▌ | 3506/4096 [28:22<04:16,  2.30it/s] 86%|████████▌ | 3507/4096 [28:23<04:15,  2.30it/s] 86%|████████▌ | 3508/4096 [28:23<04:15,  2.30it/s] 86%|████████▌ | 3509/4096 [28:24<04:14,  2.30it/s] 86%|████████▌ | 3510/4096 [28:24<04:14,  2.30it/s] 86%|████████▌ | 3511/4096 [28:24<04:13,  2.30it/s] 86%|████████▌ | 3512/4096 [28:25<04:13,  2.30it/s] 86%|████████▌ | 3513/4096 [28:25<04:12,  2.31it/s] 86%|████████▌ | 3514/4096 [28:26<04:12,  2.30it/s] 86%|████████▌ | 3515/4096 [28:26<04:12,  2.30it/s] 86%|████████▌ | 3516/4096 [28:27<04:11,  2.30it/s] 86%|████████▌ | 3517/4096 [28:27<04:11,  2.30it/s] 86%|████████▌ | 3518/4096 [28:28<04:11,  2.30it/s] 86%|████████▌ | 3519/4096 [28:28<04:10,  2.30it/s] 86%|████████▌ | 3520/4096 [28:28<04:09,  2.31it/s] 86%|████████▌ | 3521/4096 [28:29<04:09,  2.31it/s] 86%|████████▌ | 3522/4096 [28:29<04:08,  2.31it/s] 86%|████████▌ | 3523/4096 [28:30<04:08,  2.31it/s] 86%|████████▌ | 3524/4096 [28:30<04:07,  2.31it/s] 86%|████████▌ | 3525/4096 [28:31<04:07,  2.31it/s] 86%|████████▌ | 3526/4096 [28:31<04:06,  2.31it/s] 86%|████████▌ | 3527/4096 [28:31<04:06,  2.31it/s] 86%|████████▌ | 3528/4096 [28:32<04:06,  2.31it/s] 86%|████████▌ | 3529/4096 [28:32<04:06,  2.30it/s] 86%|████████▌ | 3530/4096 [28:33<04:05,  2.31it/s] 86%|████████▌ | 3531/4096 [28:33<04:04,  2.31it/s] 86%|████████▌ | 3532/4096 [28:34<04:04,  2.31it/s] 86%|████████▋ | 3533/4096 [28:34<04:04,  2.30it/s] 86%|████████▋ | 3534/4096 [28:34<04:03,  2.31it/s] 86%|████████▋ | 3535/4096 [28:35<04:03,  2.31it/s] 86%|████████▋ | 3536/4096 [28:35<04:02,  2.31it/s] 86%|████████▋ | 3537/4096 [28:36<04:02,  2.31it/s] 86%|████████▋ | 3538/4096 [28:36<04:01,  2.31it/s] 86%|████████▋ | 3539/4096 [28:37<04:01,  2.31it/s] 86%|████████▋ | 3540/4096 [28:37<04:00,  2.31it/s] 86%|████████▋ | 3541/4096 [28:37<04:00,  2.31it/s] 86%|████████▋ | 3542/4096 [28:38<03:59,  2.31it/s] 86%|████████▋ | 3543/4096 [28:38<03:59,  2.31it/s] 87%|████████▋ | 3544/4096 [28:39<03:59,  2.31it/s] 87%|████████▋ | 3545/4096 [28:39<03:58,  2.31it/s] 87%|████████▋ | 3546/4096 [28:40<03:58,  2.31it/s] 87%|████████▋ | 3547/4096 [28:40<03:58,  2.31it/s] 87%|████████▋ | 3548/4096 [28:41<03:57,  2.31it/s] 87%|████████▋ | 3549/4096 [28:41<03:56,  2.31it/s] 87%|████████▋ | 3550/4096 [28:41<03:56,  2.31it/s] 87%|████████▋ | 3551/4096 [28:42<03:56,  2.31it/s] 87%|████████▋ | 3552/4096 [28:42<03:56,  2.30it/s] 87%|████████▋ | 3553/4096 [28:43<03:55,  2.31it/s] 87%|████████▋ | 3554/4096 [28:43<03:54,  2.31it/s] 87%|████████▋ | 3555/4096 [28:44<03:54,  2.30it/s] 87%|████████▋ | 3556/4096 [28:44<03:54,  2.30it/s] 87%|████████▋ | 3557/4096 [28:44<03:54,  2.30it/s] 87%|████████▋ | 3558/4096 [28:45<03:53,  2.31it/s] 87%|████████▋ | 3559/4096 [28:45<03:52,  2.31it/s] 87%|████████▋ | 3560/4096 [28:46<03:51,  2.31it/s] 87%|████████▋ | 3561/4096 [28:46<03:51,  2.31it/s] 87%|████████▋ | 3562/4096 [28:47<03:51,  2.31it/s] 87%|████████▋ | 3563/4096 [28:47<03:50,  2.31it/s] 87%|████████▋ | 3564/4096 [28:47<03:50,  2.31it/s] 87%|████████▋ | 3565/4096 [28:48<03:49,  2.31it/s] 87%|████████▋ | 3566/4096 [28:48<03:49,  2.31it/s] 87%|████████▋ | 3567/4096 [28:49<03:49,  2.31it/s] 87%|████████▋ | 3568/4096 [28:49<03:48,  2.31it/s] 87%|████████▋ | 3569/4096 [28:50<03:48,  2.31it/s] 87%|████████▋ | 3570/4096 [28:50<03:47,  2.31it/s] 87%|████████▋ | 3571/4096 [28:50<03:47,  2.31it/s] 87%|████████▋ | 3572/4096 [28:51<03:46,  2.31it/s] 87%|████████▋ | 3573/4096 [28:51<03:46,  2.31it/s] 87%|████████▋ | 3574/4096 [28:52<03:45,  2.31it/s] 87%|████████▋ | 3575/4096 [28:52<03:45,  2.31it/s] 87%|████████▋ | 3576/4096 [28:53<03:45,  2.31it/s] 87%|████████▋ | 3577/4096 [28:53<03:44,  2.31it/s] 87%|████████▋ | 3578/4096 [28:54<03:44,  2.31it/s] 87%|████████▋ | 3579/4096 [28:54<03:43,  2.31it/s] 87%|████████▋ | 3580/4096 [28:54<03:43,  2.31it/s] 87%|████████▋ | 3581/4096 [28:55<03:42,  2.31it/s] 87%|████████▋ | 3582/4096 [28:55<03:42,  2.31it/s] 87%|████████▋ | 3583/4096 [28:56<03:41,  2.31it/s] 88%|████████▊ | 3584/4096 [28:56<03:02,  2.81it/s]loss: 2.2570505142211914
===========================
epoch 28/32 | loss: 2.2570505142211914
---------------------------
example true genres: 
tensor([0, 1, 1, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 0, 1, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9321663019693655
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["` ` If I don't come out within half an hour ride back to town and bring out a posse''. Leaving his rifle in the wagon, Tilghman walked up to the door and hammered on it. There was no reply so he shoved it open with his foot and stepped inside.", 'There is not anywhere on the frontiers of freedom a more highly mobilized force for liberation. The moment of truth is the moment of crisis. During the slow buildup, the essence of a policy or a man is concealed under embroidered details, fine words, strutting gestures.', 'Here the two leaders, DePugh and Lauchli, hastened to put the group through its paces. The Minutemen were instructed in the use of terrain for concealment. They were shown how to advance against an enemy outpost atop a cleared ridge.', 'By our policy the West was - - is - - split. But the key revelation is not new. The controlling pattern was first displayed in the Hungary - Suez crisis of November 1956.', "I don't get it why this time I should pull such a stupid trick''. ` ` Well, I get it'', Artie said, still on the ladder. ` ` You are a big muscle - bound ape and you got this idea about setting a record."]
---------------------------
example output paragraph: 
["He ` If you'' t get back here the of hour I I to the'get him a rifleonye of '. He the horse behind the saddle, heru lookeduy took out to the wagon and waiteded the the. was no sign from he could the back. a hand. swung out.", 'The is no a in the continent.. the.cc tolerant definedilized and of the. The political is the is not moment when the is The the struggle and of of the political of the nation is politicalcc is the in the collar of the art and etctiuming,,', 'The, first men of theKaul, Wilsonronerg were been to to move the signal on the first.. The firstute was had not to the formation of the and thement. The were not the to maneuver. the enemy.. the hill hill.', "The the end, Soviet Berlin a - the the - the. The the Soviet points that the. The United power of the introduced in the Soviet'- -. 1960 1960.", "`'' t know it out'is'' have it a thing trick''. ` ` I, I don it'', he said said, ` thinking the edge. ` ` I'right man man man man man man'you are a big of it up fire on"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.67it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.97it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.06it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.03it/s][A100%|██████████| 29/29 [00:00<00:00, 66.07it/s]
Mean Perplexity: 1324.457482369254
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 88%|████████▊ | 3585/4096 [29:03<19:47,  2.32s/it] 88%|████████▊ | 3586/4096 [29:03<14:55,  1.76s/it] 88%|████████▊ | 3587/4096 [29:04<11:31,  1.36s/it] 88%|████████▊ | 3588/4096 [29:04<09:09,  1.08s/it] 88%|████████▊ | 3589/4096 [29:05<07:29,  1.13it/s] 88%|████████▊ | 3590/4096 [29:05<06:19,  1.33it/s] 88%|████████▊ | 3591/4096 [29:05<05:30,  1.53it/s] 88%|████████▊ | 3592/4096 [29:06<04:56,  1.70it/s] 88%|████████▊ | 3593/4096 [29:06<04:32,  1.85it/s] 88%|████████▊ | 3594/4096 [29:07<04:15,  1.97it/s] 88%|████████▊ | 3595/4096 [29:07<04:03,  2.06it/s] 88%|████████▊ | 3596/4096 [29:08<03:54,  2.13it/s] 88%|████████▊ | 3597/4096 [29:08<03:48,  2.18it/s] 88%|████████▊ | 3598/4096 [29:08<03:44,  2.21it/s] 88%|████████▊ | 3599/4096 [29:09<03:41,  2.24it/s] 88%|████████▊ | 3600/4096 [29:09<03:39,  2.26it/s] 88%|████████▊ | 3601/4096 [29:10<03:37,  2.28it/s] 88%|████████▊ | 3602/4096 [29:10<03:36,  2.28it/s] 88%|████████▊ | 3603/4096 [29:11<03:35,  2.29it/s] 88%|████████▊ | 3604/4096 [29:11<03:34,  2.29it/s] 88%|████████▊ | 3605/4096 [29:11<03:33,  2.30it/s] 88%|████████▊ | 3606/4096 [29:12<03:32,  2.31it/s] 88%|████████▊ | 3607/4096 [29:12<03:31,  2.31it/s] 88%|████████▊ | 3608/4096 [29:13<03:31,  2.31it/s] 88%|████████▊ | 3609/4096 [29:13<03:30,  2.31it/s] 88%|████████▊ | 3610/4096 [29:14<03:30,  2.31it/s] 88%|████████▊ | 3611/4096 [29:14<03:29,  2.31it/s] 88%|████████▊ | 3612/4096 [29:14<03:29,  2.31it/s] 88%|████████▊ | 3613/4096 [29:15<03:29,  2.31it/s] 88%|████████▊ | 3614/4096 [29:15<03:28,  2.31it/s] 88%|████████▊ | 3615/4096 [29:16<03:28,  2.31it/s] 88%|████████▊ | 3616/4096 [29:16<03:27,  2.31it/s] 88%|████████▊ | 3617/4096 [29:17<03:27,  2.31it/s] 88%|████████▊ | 3618/4096 [29:17<03:27,  2.31it/s] 88%|████████▊ | 3619/4096 [29:17<03:26,  2.31it/s] 88%|████████▊ | 3620/4096 [29:18<03:25,  2.31it/s] 88%|████████▊ | 3621/4096 [29:18<03:25,  2.31it/s] 88%|████████▊ | 3622/4096 [29:19<03:24,  2.31it/s] 88%|████████▊ | 3623/4096 [29:19<03:24,  2.32it/s] 88%|████████▊ | 3624/4096 [29:20<03:23,  2.31it/s] 89%|████████▊ | 3625/4096 [29:20<03:24,  2.31it/s] 89%|████████▊ | 3626/4096 [29:21<03:23,  2.31it/s] 89%|████████▊ | 3627/4096 [29:21<03:22,  2.31it/s] 89%|████████▊ | 3628/4096 [29:21<03:22,  2.31it/s] 89%|████████▊ | 3629/4096 [29:22<03:22,  2.31it/s] 89%|████████▊ | 3630/4096 [29:22<03:21,  2.31it/s] 89%|████████▊ | 3631/4096 [29:23<03:21,  2.31it/s] 89%|████████▊ | 3632/4096 [29:23<03:21,  2.30it/s] 89%|████████▊ | 3633/4096 [29:24<03:20,  2.31it/s] 89%|████████▊ | 3634/4096 [29:24<03:20,  2.30it/s] 89%|████████▊ | 3635/4096 [29:24<03:19,  2.31it/s] 89%|████████▉ | 3636/4096 [29:25<03:19,  2.31it/s] 89%|████████▉ | 3637/4096 [29:25<03:19,  2.31it/s] 89%|████████▉ | 3638/4096 [29:26<03:18,  2.31it/s] 89%|████████▉ | 3639/4096 [29:26<03:17,  2.31it/s] 89%|████████▉ | 3640/4096 [29:27<03:17,  2.31it/s] 89%|████████▉ | 3641/4096 [29:27<03:17,  2.31it/s] 89%|████████▉ | 3642/4096 [29:27<03:16,  2.31it/s] 89%|████████▉ | 3643/4096 [29:28<03:16,  2.31it/s] 89%|████████▉ | 3644/4096 [29:28<03:15,  2.31it/s] 89%|████████▉ | 3645/4096 [29:29<03:15,  2.31it/s] 89%|████████▉ | 3646/4096 [29:29<03:14,  2.31it/s] 89%|████████▉ | 3647/4096 [29:30<03:14,  2.31it/s] 89%|████████▉ | 3648/4096 [29:30<03:13,  2.31it/s] 89%|████████▉ | 3649/4096 [29:30<03:13,  2.31it/s] 89%|████████▉ | 3650/4096 [29:31<03:13,  2.31it/s] 89%|████████▉ | 3651/4096 [29:31<03:12,  2.31it/s] 89%|████████▉ | 3652/4096 [29:32<03:12,  2.30it/s] 89%|████████▉ | 3653/4096 [29:32<03:12,  2.30it/s] 89%|████████▉ | 3654/4096 [29:33<03:11,  2.30it/s] 89%|████████▉ | 3655/4096 [29:33<03:11,  2.30it/s] 89%|████████▉ | 3656/4096 [29:34<03:10,  2.31it/s] 89%|████████▉ | 3657/4096 [29:34<03:09,  2.31it/s] 89%|████████▉ | 3658/4096 [29:34<03:09,  2.31it/s] 89%|████████▉ | 3659/4096 [29:35<03:09,  2.31it/s] 89%|████████▉ | 3660/4096 [29:35<03:08,  2.31it/s] 89%|████████▉ | 3661/4096 [29:36<03:08,  2.31it/s] 89%|████████▉ | 3662/4096 [29:36<03:08,  2.31it/s] 89%|████████▉ | 3663/4096 [29:37<03:07,  2.31it/s] 89%|████████▉ | 3664/4096 [29:37<03:07,  2.31it/s] 89%|████████▉ | 3665/4096 [29:37<03:06,  2.31it/s] 90%|████████▉ | 3666/4096 [29:38<03:06,  2.31it/s] 90%|████████▉ | 3667/4096 [29:38<03:05,  2.31it/s] 90%|████████▉ | 3668/4096 [29:39<03:05,  2.31it/s] 90%|████████▉ | 3669/4096 [29:39<03:04,  2.31it/s] 90%|████████▉ | 3670/4096 [29:40<03:04,  2.31it/s] 90%|████████▉ | 3671/4096 [29:40<03:03,  2.31it/s] 90%|████████▉ | 3672/4096 [29:40<03:03,  2.31it/s] 90%|████████▉ | 3673/4096 [29:41<03:02,  2.31it/s] 90%|████████▉ | 3674/4096 [29:41<03:02,  2.31it/s] 90%|████████▉ | 3675/4096 [29:42<03:02,  2.31it/s] 90%|████████▉ | 3676/4096 [29:42<03:01,  2.31it/s] 90%|████████▉ | 3677/4096 [29:43<03:01,  2.31it/s] 90%|████████▉ | 3678/4096 [29:43<03:00,  2.31it/s] 90%|████████▉ | 3679/4096 [29:43<03:00,  2.31it/s] 90%|████████▉ | 3680/4096 [29:44<02:59,  2.31it/s] 90%|████████▉ | 3681/4096 [29:44<02:59,  2.31it/s] 90%|████████▉ | 3682/4096 [29:45<02:59,  2.31it/s] 90%|████████▉ | 3683/4096 [29:45<02:59,  2.31it/s] 90%|████████▉ | 3684/4096 [29:46<02:58,  2.31it/s] 90%|████████▉ | 3685/4096 [29:46<02:57,  2.31it/s] 90%|████████▉ | 3686/4096 [29:47<02:57,  2.31it/s] 90%|█████████ | 3687/4096 [29:47<02:57,  2.31it/s] 90%|█████████ | 3688/4096 [29:47<02:56,  2.31it/s] 90%|█████████ | 3689/4096 [29:48<02:56,  2.31it/s] 90%|█████████ | 3690/4096 [29:48<02:55,  2.31it/s] 90%|█████████ | 3691/4096 [29:49<02:55,  2.31it/s] 90%|█████████ | 3692/4096 [29:49<02:54,  2.31it/s] 90%|█████████ | 3693/4096 [29:50<02:54,  2.31it/s] 90%|█████████ | 3694/4096 [29:50<02:54,  2.31it/s] 90%|█████████ | 3695/4096 [29:50<02:53,  2.31it/s] 90%|█████████ | 3696/4096 [29:51<02:52,  2.31it/s] 90%|█████████ | 3697/4096 [29:51<02:52,  2.31it/s] 90%|█████████ | 3698/4096 [29:52<02:52,  2.31it/s] 90%|█████████ | 3699/4096 [29:52<02:51,  2.31it/s] 90%|█████████ | 3700/4096 [29:53<02:51,  2.31it/s] 90%|█████████ | 3701/4096 [29:53<02:51,  2.31it/s] 90%|█████████ | 3702/4096 [29:53<02:50,  2.31it/s] 90%|█████████ | 3703/4096 [29:54<02:50,  2.31it/s] 90%|█████████ | 3704/4096 [29:54<02:49,  2.31it/s] 90%|█████████ | 3705/4096 [29:55<02:49,  2.31it/s] 90%|█████████ | 3706/4096 [29:55<02:48,  2.31it/s] 91%|█████████ | 3707/4096 [29:56<02:48,  2.31it/s] 91%|█████████ | 3708/4096 [29:56<02:47,  2.31it/s] 91%|█████████ | 3709/4096 [29:56<02:47,  2.31it/s] 91%|█████████ | 3710/4096 [29:57<02:46,  2.31it/s] 91%|█████████ | 3711/4096 [29:57<02:46,  2.31it/s] 91%|█████████ | 3712/4096 [29:58<02:16,  2.81it/s]loss: 2.987614154815674
===========================
epoch 29/32 | loss: 2.987614154815674
---------------------------
example true genres: 
tensor([1, 1, 1, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 1, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9518599562363238
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["Consider this table : 1. Louis 14, - - Aj. ` ` With no strong men and no parliament to dispute his will, he was the government''.", 'Sometimes they get their initial experience in church haflis, conducted by Lebanese and Syrians in the U. S., where they dance with just as few veils across their bodies as in nightclubs. As the girls come to belly dancing from this and other origins, the melting pot has never bubbled more intriguingly. Some Manhattan examples :', 'The girls are kept booked and moving by several agents, notably voluble, black - bearded Murat Somay, a Manhattan Turk who is the Sol Hurok of the central abdomen. He can offer nine Turkish girls, plans to import at least 15 more. But a great many of the dancers are more or less native.', "You did this you like to hurt to beat people I want to go home''. These were the last words he ever uttered. Convulsively, he spat up some blood and collapsed into the arms of Senator Gaston Berche, crimsoning the frilly shirt and waistcoat the politician wore.", "The slender, handsome fellow was called Dandy Brandon by the other slaves. He was gifted with animal magnetism and a potent allure for women of any race. But Dandy had had little experience with girls on his master's plantation in Bayou St. John."]
---------------------------
example output paragraph: 
["Theing letter, ` : 3 Louis. 000 - ` letter. ` The the vote man, a other'oppose the appointment. he'a only's.", 'The the are a own reservations with the,rassryy and in a, others, the Dominican. S.. and they are in the the as peopleed, the street. they thes. They a first are out the dancing, the high the groups, the girls pot is been beend and. and. ofs are', 'He two are very in in are in the days, who theleyle, and - haireded,ttisro,, and young dancerrker is a onlyempster all Caribbean city. He is be a - k, who for be them least one,. he young deal of them young are in than less than than', "He'not to'to say him death him with said to do on to '. He men men men words he spoke heard. `cedinglsively he he shook out the of from spat to the dirt of the Barton Claytonauer, his and his manroths face. hisless. other '.", "He man man muscular man was a a '.. the way three. He was a with the rightsism, physical great appetite - to his. all age. He he was was a enough time with the, the own. s ranch. theou,. Louis, He"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.29it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.50it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.60it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 65.58it/s][A100%|██████████| 29/29 [00:00<00:00, 65.66it/s]
Mean Perplexity: 1347.8815382742516
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 91%|█████████ | 3713/4096 [30:04<14:25,  2.26s/it] 91%|█████████ | 3714/4096 [30:05<10:53,  1.71s/it] 91%|█████████ | 3715/4096 [30:05<08:31,  1.34s/it] 91%|█████████ | 3716/4096 [30:06<06:46,  1.07s/it] 91%|█████████ | 3717/4096 [30:06<05:32,  1.14it/s] 91%|█████████ | 3718/4096 [30:06<04:41,  1.34it/s] 91%|█████████ | 3719/4096 [30:07<04:05,  1.54it/s] 91%|█████████ | 3720/4096 [30:07<03:40,  1.71it/s] 91%|█████████ | 3721/4096 [30:08<03:22,  1.85it/s] 91%|█████████ | 3722/4096 [30:08<03:10,  1.97it/s] 91%|█████████ | 3723/4096 [30:09<03:01,  2.06it/s] 91%|█████████ | 3724/4096 [30:09<02:54,  2.13it/s] 91%|█████████ | 3725/4096 [30:09<02:50,  2.18it/s] 91%|█████████ | 3726/4096 [30:10<02:46,  2.22it/s] 91%|█████████ | 3727/4096 [30:10<02:44,  2.25it/s] 91%|█████████ | 3728/4096 [30:11<02:42,  2.27it/s] 91%|█████████ | 3729/4096 [30:11<02:40,  2.28it/s] 91%|█████████ | 3730/4096 [30:12<02:39,  2.29it/s] 91%|█████████ | 3731/4096 [30:12<02:39,  2.29it/s] 91%|█████████ | 3732/4096 [30:12<02:38,  2.30it/s] 91%|█████████ | 3733/4096 [30:13<02:37,  2.30it/s] 91%|█████████ | 3734/4096 [30:13<02:37,  2.30it/s] 91%|█████████ | 3735/4096 [30:14<02:36,  2.30it/s] 91%|█████████ | 3736/4096 [30:14<02:36,  2.30it/s] 91%|█████████ | 3737/4096 [30:15<02:35,  2.30it/s] 91%|█████████▏| 3738/4096 [30:15<02:35,  2.30it/s] 91%|█████████▏| 3739/4096 [30:16<02:34,  2.30it/s] 91%|█████████▏| 3740/4096 [30:16<02:34,  2.31it/s] 91%|█████████▏| 3741/4096 [30:16<02:33,  2.31it/s] 91%|█████████▏| 3742/4096 [30:17<02:33,  2.31it/s] 91%|█████████▏| 3743/4096 [30:17<02:32,  2.31it/s] 91%|█████████▏| 3744/4096 [30:18<02:32,  2.31it/s] 91%|█████████▏| 3745/4096 [30:18<02:32,  2.31it/s] 91%|█████████▏| 3746/4096 [30:19<02:31,  2.31it/s] 91%|█████████▏| 3747/4096 [30:19<02:31,  2.31it/s] 92%|█████████▏| 3748/4096 [30:19<02:30,  2.31it/s] 92%|█████████▏| 3749/4096 [30:20<02:30,  2.30it/s] 92%|█████████▏| 3750/4096 [30:20<02:30,  2.30it/s] 92%|█████████▏| 3751/4096 [30:21<02:29,  2.30it/s] 92%|█████████▏| 3752/4096 [30:21<02:29,  2.30it/s] 92%|█████████▏| 3753/4096 [30:22<02:29,  2.30it/s] 92%|█████████▏| 3754/4096 [30:22<02:28,  2.30it/s] 92%|█████████▏| 3755/4096 [30:22<02:27,  2.31it/s] 92%|█████████▏| 3756/4096 [30:23<02:27,  2.30it/s] 92%|█████████▏| 3757/4096 [30:23<02:27,  2.31it/s] 92%|█████████▏| 3758/4096 [30:24<02:26,  2.30it/s] 92%|█████████▏| 3759/4096 [30:24<02:26,  2.30it/s] 92%|█████████▏| 3760/4096 [30:25<02:25,  2.30it/s] 92%|█████████▏| 3761/4096 [30:25<02:25,  2.31it/s] 92%|█████████▏| 3762/4096 [30:25<02:24,  2.31it/s] 92%|█████████▏| 3763/4096 [30:26<02:24,  2.30it/s] 92%|█████████▏| 3764/4096 [30:26<02:24,  2.30it/s] 92%|█████████▏| 3765/4096 [30:27<02:23,  2.31it/s] 92%|█████████▏| 3766/4096 [30:27<02:23,  2.31it/s] 92%|█████████▏| 3767/4096 [30:28<02:22,  2.31it/s] 92%|█████████▏| 3768/4096 [30:28<02:22,  2.31it/s] 92%|█████████▏| 3769/4096 [30:29<02:22,  2.30it/s] 92%|█████████▏| 3770/4096 [30:29<02:21,  2.30it/s] 92%|█████████▏| 3771/4096 [30:29<02:21,  2.30it/s] 92%|█████████▏| 3772/4096 [30:30<02:20,  2.30it/s] 92%|█████████▏| 3773/4096 [30:30<02:20,  2.30it/s] 92%|█████████▏| 3774/4096 [30:31<02:19,  2.30it/s] 92%|█████████▏| 3775/4096 [30:31<02:19,  2.31it/s] 92%|█████████▏| 3776/4096 [30:32<02:18,  2.30it/s] 92%|█████████▏| 3777/4096 [30:32<02:18,  2.30it/s] 92%|█████████▏| 3778/4096 [30:32<02:18,  2.30it/s] 92%|█████████▏| 3779/4096 [30:33<02:17,  2.30it/s] 92%|█████████▏| 3780/4096 [30:33<02:17,  2.30it/s] 92%|█████████▏| 3781/4096 [30:34<02:16,  2.30it/s] 92%|█████████▏| 3782/4096 [30:34<02:16,  2.31it/s] 92%|█████████▏| 3783/4096 [30:35<02:15,  2.30it/s] 92%|█████████▏| 3784/4096 [30:35<02:15,  2.30it/s] 92%|█████████▏| 3785/4096 [30:35<02:14,  2.31it/s] 92%|█████████▏| 3786/4096 [30:36<02:14,  2.30it/s] 92%|█████████▏| 3787/4096 [30:36<02:14,  2.30it/s] 92%|█████████▏| 3788/4096 [30:37<02:13,  2.30it/s] 93%|█████████▎| 3789/4096 [30:37<02:13,  2.30it/s] 93%|█████████▎| 3790/4096 [30:38<02:12,  2.30it/s] 93%|█████████▎| 3791/4096 [30:38<02:12,  2.30it/s] 93%|█████████▎| 3792/4096 [30:39<02:11,  2.31it/s] 93%|█████████▎| 3793/4096 [30:39<02:11,  2.31it/s] 93%|█████████▎| 3794/4096 [30:39<02:10,  2.31it/s] 93%|█████████▎| 3795/4096 [30:40<02:10,  2.31it/s] 93%|█████████▎| 3796/4096 [30:40<02:09,  2.31it/s] 93%|█████████▎| 3797/4096 [30:41<02:09,  2.31it/s] 93%|█████████▎| 3798/4096 [30:41<02:09,  2.31it/s] 93%|█████████▎| 3799/4096 [30:42<02:08,  2.30it/s] 93%|█████████▎| 3800/4096 [30:42<02:08,  2.31it/s] 93%|█████████▎| 3801/4096 [30:42<02:07,  2.31it/s] 93%|█████████▎| 3802/4096 [30:43<02:07,  2.30it/s] 93%|█████████▎| 3803/4096 [30:43<02:07,  2.30it/s] 93%|█████████▎| 3804/4096 [30:44<02:06,  2.30it/s] 93%|█████████▎| 3805/4096 [30:44<02:06,  2.30it/s] 93%|█████████▎| 3806/4096 [30:45<02:06,  2.30it/s] 93%|█████████▎| 3807/4096 [30:45<02:05,  2.29it/s] 93%|█████████▎| 3808/4096 [30:45<02:05,  2.30it/s] 93%|█████████▎| 3809/4096 [30:46<02:04,  2.30it/s] 93%|█████████▎| 3810/4096 [30:46<02:04,  2.30it/s] 93%|█████████▎| 3811/4096 [30:47<02:03,  2.30it/s] 93%|█████████▎| 3812/4096 [30:47<02:03,  2.30it/s] 93%|█████████▎| 3813/4096 [30:48<02:02,  2.31it/s] 93%|█████████▎| 3814/4096 [30:48<02:02,  2.31it/s] 93%|█████████▎| 3815/4096 [30:48<02:01,  2.31it/s] 93%|█████████▎| 3816/4096 [30:49<02:01,  2.31it/s] 93%|█████████▎| 3817/4096 [30:49<02:00,  2.31it/s] 93%|█████████▎| 3818/4096 [30:50<02:00,  2.31it/s] 93%|█████████▎| 3819/4096 [30:50<02:00,  2.31it/s] 93%|█████████▎| 3820/4096 [30:51<01:59,  2.31it/s] 93%|█████████▎| 3821/4096 [30:51<01:59,  2.31it/s] 93%|█████████▎| 3822/4096 [30:52<01:58,  2.31it/s] 93%|█████████▎| 3823/4096 [30:52<01:58,  2.31it/s] 93%|█████████▎| 3824/4096 [30:52<01:57,  2.31it/s] 93%|█████████▎| 3825/4096 [30:53<01:57,  2.31it/s] 93%|█████████▎| 3826/4096 [30:53<01:57,  2.31it/s] 93%|█████████▎| 3827/4096 [30:54<01:56,  2.31it/s] 93%|█████████▎| 3828/4096 [30:54<01:55,  2.31it/s] 93%|█████████▎| 3829/4096 [30:55<01:55,  2.31it/s] 94%|█████████▎| 3830/4096 [30:55<01:54,  2.31it/s] 94%|█████████▎| 3831/4096 [30:55<01:54,  2.31it/s] 94%|█████████▎| 3832/4096 [30:56<01:54,  2.31it/s] 94%|█████████▎| 3833/4096 [30:56<01:53,  2.31it/s] 94%|█████████▎| 3834/4096 [30:57<01:53,  2.31it/s] 94%|█████████▎| 3835/4096 [30:57<01:53,  2.31it/s] 94%|█████████▎| 3836/4096 [30:58<01:52,  2.31it/s] 94%|█████████▎| 3837/4096 [30:58<01:52,  2.31it/s] 94%|█████████▎| 3838/4096 [30:58<01:51,  2.31it/s] 94%|█████████▎| 3839/4096 [30:59<01:51,  2.31it/s] 94%|█████████▍| 3840/4096 [30:59<01:31,  2.80it/s]loss: 2.309452533721924
===========================
epoch 30/32 | loss: 2.309452533721924
---------------------------
example true genres: 
tensor([0, 1, 0, 1, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 0, 1, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9518599562363238
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['Dr. Louis Lalaurie examined the inert form of the slave on the parquet dance floor and pronounced him dead. The ball broke up in confusion. Guests stared with horror at Madame Lalaurie and made speedy departures.', "This confuses me no end. If the Hessian troops sent here willy - nilly by the Hessian Government to fight for England in the 1770's were mercenaries, what shall we call the UN troops sent to the Congo willy - nilly by their governments to fight for the United Nations?? If the UN troops are not mercenaries then the Hessians were not mercenaries either.", "I know this from my talks with him''. ` ` Well, let's let him make up his own mind, OK''?? Waddell said.", "In their maneuvers last month, they wore World War 2, camouflage garb and helmets, and carried unloaded M - 1 rifles. The maneuvers were held ` ` in secret'' after a regional seminar for the Minutemen, held in nearby Shiloh, Ill., had been broken up the previous day by deputy sheriffs, who had arrested regional leader Richard Lauchli of Collinsville, Ill., and seized four operative weapons, including a Browning machine", "` ` Aristide!! I want you to find Monsieur Prieur at once and give him this money for the boy's purchase. There's $ 600 in gold in this chamois sack."]
---------------------------
example output paragraph: 
["Theess Conan Kee'the staineddentert of a dead girl the terraceoh... then a.. The bell was from in a.s were at a at the 'e'' at a up steps..", 'The istems the with longer of The the Britishian government are out to be the -gh, the thousandsian army, the for the, the Uniteds s war the, they we be have the United troops, to the United. be be -gh, force own. fight for the freedom States.? the British was are not ready, they Unitedian are the the for.', "`'that'the first. you. '. ` ` I, I's get'talk a something name name''''.? `hiskydy said.", "The the first, year, the were the War I uniforms and uniformsuerrote - weapons, and belts pistols - riflesK 21 and. The men were conducted at ` ` the, ', the single convention. the Uk and, which at Los barsmmerh, California -. The and been assigned into by grounds year. the K John. Lt had been the leaders Sam Rayrner, the,. Illinois.. B and the a mens and seized a dozen rifle pistol", 'He ` I, thou,! You am you to see the forelletain for the. I him a letter to the sum. s sake. I is s a 500, gold. the boxapise account.']
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.15it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.49it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.59it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.83it/s][A100%|██████████| 29/29 [00:00<00:00, 66.79it/s]
Mean Perplexity: 1342.598355416396
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 94%|█████████▍| 3841/4096 [31:06<09:44,  2.29s/it] 94%|█████████▍| 3842/4096 [31:06<07:21,  1.74s/it] 94%|█████████▍| 3843/4096 [31:07<05:41,  1.35s/it] 94%|█████████▍| 3844/4096 [31:07<04:30,  1.07s/it] 94%|█████████▍| 3845/4096 [31:08<03:41,  1.13it/s] 94%|█████████▍| 3846/4096 [31:08<03:06,  1.34it/s] 94%|█████████▍| 3847/4096 [31:08<02:42,  1.53it/s] 94%|█████████▍| 3848/4096 [31:09<02:25,  1.70it/s] 94%|█████████▍| 3849/4096 [31:09<02:13,  1.85it/s] 94%|█████████▍| 3850/4096 [31:10<02:05,  1.97it/s] 94%|█████████▍| 3851/4096 [31:10<01:59,  2.06it/s] 94%|█████████▍| 3852/4096 [31:11<01:54,  2.13it/s] 94%|█████████▍| 3853/4096 [31:11<01:51,  2.18it/s] 94%|█████████▍| 3854/4096 [31:12<01:49,  2.22it/s] 94%|█████████▍| 3855/4096 [31:12<01:47,  2.25it/s] 94%|█████████▍| 3856/4096 [31:12<01:46,  2.26it/s] 94%|█████████▍| 3857/4096 [31:13<01:44,  2.28it/s] 94%|█████████▍| 3858/4096 [31:13<01:44,  2.28it/s] 94%|█████████▍| 3859/4096 [31:14<01:43,  2.29it/s] 94%|█████████▍| 3860/4096 [31:14<01:42,  2.30it/s] 94%|█████████▍| 3861/4096 [31:15<01:41,  2.30it/s] 94%|█████████▍| 3862/4096 [31:15<01:41,  2.31it/s] 94%|█████████▍| 3863/4096 [31:15<01:40,  2.31it/s] 94%|█████████▍| 3864/4096 [31:16<01:40,  2.30it/s] 94%|█████████▍| 3865/4096 [31:16<01:40,  2.30it/s] 94%|█████████▍| 3866/4096 [31:17<01:39,  2.30it/s] 94%|█████████▍| 3867/4096 [31:17<01:39,  2.30it/s] 94%|█████████▍| 3868/4096 [31:18<01:38,  2.31it/s] 94%|█████████▍| 3869/4096 [31:18<01:38,  2.31it/s] 94%|█████████▍| 3870/4096 [31:18<01:37,  2.31it/s] 95%|█████████▍| 3871/4096 [31:19<01:37,  2.31it/s] 95%|█████████▍| 3872/4096 [31:19<01:37,  2.31it/s] 95%|█████████▍| 3873/4096 [31:20<01:36,  2.31it/s] 95%|█████████▍| 3874/4096 [31:20<01:36,  2.31it/s] 95%|█████████▍| 3875/4096 [31:21<01:35,  2.31it/s] 95%|█████████▍| 3876/4096 [31:21<01:35,  2.30it/s] 95%|█████████▍| 3877/4096 [31:21<01:34,  2.31it/s] 95%|█████████▍| 3878/4096 [31:22<01:34,  2.31it/s] 95%|█████████▍| 3879/4096 [31:22<01:33,  2.31it/s] 95%|█████████▍| 3880/4096 [31:23<01:33,  2.31it/s] 95%|█████████▍| 3881/4096 [31:23<01:33,  2.31it/s] 95%|█████████▍| 3882/4096 [31:24<01:32,  2.31it/s] 95%|█████████▍| 3883/4096 [31:24<01:32,  2.31it/s] 95%|█████████▍| 3884/4096 [31:25<01:31,  2.31it/s] 95%|█████████▍| 3885/4096 [31:25<01:31,  2.31it/s] 95%|█████████▍| 3886/4096 [31:25<01:31,  2.31it/s] 95%|█████████▍| 3887/4096 [31:26<01:30,  2.30it/s] 95%|█████████▍| 3888/4096 [31:26<01:30,  2.31it/s] 95%|█████████▍| 3889/4096 [31:27<01:29,  2.31it/s] 95%|█████████▍| 3890/4096 [31:27<01:29,  2.31it/s] 95%|█████████▍| 3891/4096 [31:28<01:28,  2.31it/s] 95%|█████████▌| 3892/4096 [31:28<01:28,  2.31it/s] 95%|█████████▌| 3893/4096 [31:28<01:27,  2.31it/s] 95%|█████████▌| 3894/4096 [31:29<01:27,  2.31it/s] 95%|█████████▌| 3895/4096 [31:29<01:27,  2.31it/s] 95%|█████████▌| 3896/4096 [31:30<01:26,  2.31it/s] 95%|█████████▌| 3897/4096 [31:30<01:26,  2.31it/s] 95%|█████████▌| 3898/4096 [31:31<01:25,  2.31it/s] 95%|█████████▌| 3899/4096 [31:31<01:25,  2.31it/s] 95%|█████████▌| 3900/4096 [31:31<01:24,  2.31it/s] 95%|█████████▌| 3901/4096 [31:32<01:24,  2.31it/s] 95%|█████████▌| 3902/4096 [31:32<01:23,  2.31it/s] 95%|█████████▌| 3903/4096 [31:33<01:23,  2.31it/s] 95%|█████████▌| 3904/4096 [31:33<01:23,  2.31it/s] 95%|█████████▌| 3905/4096 [31:34<01:22,  2.31it/s] 95%|█████████▌| 3906/4096 [31:34<01:22,  2.31it/s] 95%|█████████▌| 3907/4096 [31:34<01:21,  2.31it/s] 95%|█████████▌| 3908/4096 [31:35<01:21,  2.31it/s] 95%|█████████▌| 3909/4096 [31:35<01:21,  2.31it/s] 95%|█████████▌| 3910/4096 [31:36<01:20,  2.31it/s] 95%|█████████▌| 3911/4096 [31:36<01:20,  2.31it/s] 96%|█████████▌| 3912/4096 [31:37<01:19,  2.31it/s] 96%|█████████▌| 3913/4096 [31:37<01:19,  2.31it/s] 96%|█████████▌| 3914/4096 [31:38<01:18,  2.30it/s] 96%|█████████▌| 3915/4096 [31:38<01:18,  2.30it/s] 96%|█████████▌| 3916/4096 [31:38<01:18,  2.30it/s] 96%|█████████▌| 3917/4096 [31:39<01:17,  2.30it/s] 96%|█████████▌| 3918/4096 [31:39<01:17,  2.30it/s] 96%|█████████▌| 3919/4096 [31:40<01:16,  2.30it/s] 96%|█████████▌| 3920/4096 [31:40<01:16,  2.30it/s] 96%|█████████▌| 3921/4096 [31:41<01:15,  2.30it/s] 96%|█████████▌| 3922/4096 [31:41<01:15,  2.30it/s] 96%|█████████▌| 3923/4096 [31:41<01:15,  2.30it/s] 96%|█████████▌| 3924/4096 [31:42<01:14,  2.31it/s] 96%|█████████▌| 3925/4096 [31:42<01:14,  2.31it/s] 96%|█████████▌| 3926/4096 [31:43<01:13,  2.31it/s] 96%|█████████▌| 3927/4096 [31:43<01:13,  2.31it/s] 96%|█████████▌| 3928/4096 [31:44<01:12,  2.30it/s] 96%|█████████▌| 3929/4096 [31:44<01:12,  2.30it/s] 96%|█████████▌| 3930/4096 [31:44<01:11,  2.31it/s] 96%|█████████▌| 3931/4096 [31:45<01:11,  2.31it/s] 96%|█████████▌| 3932/4096 [31:45<01:11,  2.31it/s] 96%|█████████▌| 3933/4096 [31:46<01:10,  2.30it/s] 96%|█████████▌| 3934/4096 [31:46<01:10,  2.30it/s] 96%|█████████▌| 3935/4096 [31:47<01:09,  2.30it/s] 96%|█████████▌| 3936/4096 [31:47<01:09,  2.30it/s] 96%|█████████▌| 3937/4096 [31:47<01:08,  2.31it/s] 96%|█████████▌| 3938/4096 [31:48<01:08,  2.31it/s] 96%|█████████▌| 3939/4096 [31:48<01:08,  2.31it/s] 96%|█████████▌| 3940/4096 [31:49<01:07,  2.31it/s] 96%|█████████▌| 3941/4096 [31:49<01:07,  2.31it/s] 96%|█████████▌| 3942/4096 [31:50<01:06,  2.31it/s] 96%|█████████▋| 3943/4096 [31:50<01:06,  2.30it/s] 96%|█████████▋| 3944/4096 [31:51<01:05,  2.31it/s] 96%|█████████▋| 3945/4096 [31:51<01:05,  2.31it/s] 96%|█████████▋| 3946/4096 [31:51<01:05,  2.31it/s] 96%|█████████▋| 3947/4096 [31:52<01:04,  2.31it/s] 96%|█████████▋| 3948/4096 [31:52<01:04,  2.30it/s] 96%|█████████▋| 3949/4096 [31:53<01:03,  2.31it/s] 96%|█████████▋| 3950/4096 [31:53<01:03,  2.31it/s] 96%|█████████▋| 3951/4096 [31:54<01:02,  2.31it/s] 96%|█████████▋| 3952/4096 [31:54<01:02,  2.31it/s] 97%|█████████▋| 3953/4096 [31:54<01:02,  2.31it/s] 97%|█████████▋| 3954/4096 [31:55<01:01,  2.31it/s] 97%|█████████▋| 3955/4096 [31:55<01:01,  2.31it/s] 97%|█████████▋| 3956/4096 [31:56<01:00,  2.31it/s] 97%|█████████▋| 3957/4096 [31:56<01:00,  2.31it/s] 97%|█████████▋| 3958/4096 [31:57<00:59,  2.31it/s] 97%|█████████▋| 3959/4096 [31:57<00:59,  2.31it/s] 97%|█████████▋| 3960/4096 [31:57<00:58,  2.31it/s] 97%|█████████▋| 3961/4096 [31:58<00:58,  2.30it/s] 97%|█████████▋| 3962/4096 [31:58<00:58,  2.31it/s] 97%|█████████▋| 3963/4096 [31:59<00:57,  2.30it/s] 97%|█████████▋| 3964/4096 [31:59<00:57,  2.30it/s] 97%|█████████▋| 3965/4096 [32:00<00:56,  2.30it/s] 97%|█████████▋| 3966/4096 [32:00<00:56,  2.31it/s] 97%|█████████▋| 3967/4096 [32:00<00:55,  2.31it/s] 97%|█████████▋| 3968/4096 [32:01<00:45,  2.80it/s]loss: 1.6781314611434937
===========================
epoch 31/32 | loss: 1.6781314611434937
---------------------------
example true genres: 
tensor([1, 0, 1, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([1, 0, 1, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.9518599562363238
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
['It reappears, in whole or part, whenever a new crisis exposes the reality : in Cuba last spring ( with which the Dominican events of last month should be paired ) ; ; at the peaks of the nuclear test and the Berlin cycles ; ; in relation to Laos, Algeria, South Africa ; ;', 'It ended when he tumbled ; ; but jumping right up, he staggered in no particular direction. He wore no head cover of any kind and, more odd, had no visible weapon.', "Personally, it meant a great deal ; ; my only hope is that it will be shared by many, many others. ` ` Confrontation'' should fortify us all, whether in Southeast Asia or the U. S..", 'Sake. So that had been his difficulty. Drunk on sake, he must have wandered off from his bivouac.', "` ` O. K.'' Charles rose also, and the two of them moved over to join the girls. They played crack the whip a few minutes without mishap. Then when Miss Langford was on the end of the line of girls, Jack, in the middle of the line, gave an extra hard pull and the young teacher sprawled backwards, sitting down hard, her dress flying over her head."]
---------------------------
example output paragraph: 
['The issumingpears in and the terms two of the the new American occurss a crisis of the the, year, the the the United Republic have the year ) be the with, ; the least same of the crisis crisis, the fall Wall, ; the to the, the, Laos Africa, ;', 'He was with he was off ; he he off at, he was. a time direction. He was no hatless, his kind of he he than, he been intention movement.', "Theity, he is little good deal of ; country hope is to our is survive a in all people and people. ` Weteming'', beify the.. but we the Asia or the West. S..,", 'Hegging, He, he been a luck. Heunk, the, he thought have been off into the sleepstouac.', "He ` I'S.'', said to, and he other men them stood toward to the him others, The were theers ball and few minutes later incidentsunderstandingllinging The, he Bston came ready the porch of the game, the, he came and the lead of the game, turned him arm effort look. started other girl, out. and down on. and hands and from the feet."]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 66.22it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 66.32it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 66.56it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.65it/s][A100%|██████████| 29/29 [00:00<00:00, 66.65it/s]
Mean Perplexity: 1347.3952929540587
===========================
/home/mitryand/.local/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
 97%|█████████▋| 3969/4096 [32:07<04:49,  2.28s/it] 97%|█████████▋| 3970/4096 [32:08<03:37,  1.72s/it] 97%|█████████▋| 3971/4096 [32:08<02:47,  1.34s/it] 97%|█████████▋| 3972/4096 [32:09<02:12,  1.07s/it] 97%|█████████▋| 3973/4096 [32:09<01:47,  1.14it/s] 97%|█████████▋| 3974/4096 [32:10<01:30,  1.35it/s] 97%|█████████▋| 3975/4096 [32:10<01:18,  1.54it/s] 97%|█████████▋| 3976/4096 [32:10<01:10,  1.71it/s] 97%|█████████▋| 3977/4096 [32:11<01:04,  1.85it/s] 97%|█████████▋| 3978/4096 [32:11<00:59,  1.97it/s] 97%|█████████▋| 3979/4096 [32:12<00:56,  2.06it/s] 97%|█████████▋| 3980/4096 [32:12<00:54,  2.13it/s] 97%|█████████▋| 3981/4096 [32:13<00:52,  2.18it/s] 97%|█████████▋| 3982/4096 [32:13<00:51,  2.21it/s] 97%|█████████▋| 3983/4096 [32:14<00:50,  2.24it/s] 97%|█████████▋| 3984/4096 [32:14<00:49,  2.26it/s] 97%|█████████▋| 3985/4096 [32:14<00:48,  2.27it/s] 97%|█████████▋| 3986/4096 [32:15<00:48,  2.28it/s] 97%|█████████▋| 3987/4096 [32:15<00:47,  2.29it/s] 97%|█████████▋| 3988/4096 [32:16<00:47,  2.30it/s] 97%|█████████▋| 3989/4096 [32:16<00:46,  2.30it/s] 97%|█████████▋| 3990/4096 [32:17<00:45,  2.31it/s] 97%|█████████▋| 3991/4096 [32:17<00:45,  2.31it/s] 97%|█████████▋| 3992/4096 [32:17<00:45,  2.30it/s] 97%|█████████▋| 3993/4096 [32:18<00:44,  2.30it/s] 98%|█████████▊| 3994/4096 [32:18<00:44,  2.30it/s] 98%|█████████▊| 3995/4096 [32:19<00:43,  2.31it/s] 98%|█████████▊| 3996/4096 [32:19<00:43,  2.31it/s] 98%|█████████▊| 3997/4096 [32:20<00:42,  2.30it/s] 98%|█████████▊| 3998/4096 [32:20<00:42,  2.30it/s] 98%|█████████▊| 3999/4096 [32:20<00:42,  2.31it/s] 98%|█████████▊| 4000/4096 [32:21<00:41,  2.30it/s] 98%|█████████▊| 4001/4096 [32:21<00:41,  2.30it/s] 98%|█████████▊| 4002/4096 [32:22<00:40,  2.31it/s] 98%|█████████▊| 4003/4096 [32:22<00:40,  2.31it/s] 98%|█████████▊| 4004/4096 [32:23<00:39,  2.31it/s] 98%|█████████▊| 4005/4096 [32:23<00:39,  2.31it/s] 98%|█████████▊| 4006/4096 [32:23<00:38,  2.31it/s] 98%|█████████▊| 4007/4096 [32:24<00:38,  2.31it/s] 98%|█████████▊| 4008/4096 [32:24<00:38,  2.31it/s] 98%|█████████▊| 4009/4096 [32:25<00:37,  2.31it/s] 98%|█████████▊| 4010/4096 [32:25<00:37,  2.30it/s] 98%|█████████▊| 4011/4096 [32:26<00:36,  2.31it/s] 98%|█████████▊| 4012/4096 [32:26<00:36,  2.31it/s] 98%|█████████▊| 4013/4096 [32:27<00:35,  2.31it/s] 98%|█████████▊| 4014/4096 [32:27<00:35,  2.31it/s] 98%|█████████▊| 4015/4096 [32:27<00:35,  2.31it/s] 98%|█████████▊| 4016/4096 [32:28<00:34,  2.31it/s] 98%|█████████▊| 4017/4096 [32:28<00:34,  2.31it/s] 98%|█████████▊| 4018/4096 [32:29<00:33,  2.31it/s] 98%|█████████▊| 4019/4096 [32:29<00:33,  2.31it/s] 98%|█████████▊| 4020/4096 [32:30<00:32,  2.31it/s] 98%|█████████▊| 4021/4096 [32:30<00:32,  2.31it/s] 98%|█████████▊| 4022/4096 [32:30<00:32,  2.31it/s] 98%|█████████▊| 4023/4096 [32:31<00:31,  2.31it/s] 98%|█████████▊| 4024/4096 [32:31<00:31,  2.31it/s] 98%|█████████▊| 4025/4096 [32:32<00:30,  2.30it/s] 98%|█████████▊| 4026/4096 [32:32<00:30,  2.30it/s] 98%|█████████▊| 4027/4096 [32:33<00:29,  2.30it/s] 98%|█████████▊| 4028/4096 [32:33<00:29,  2.30it/s] 98%|█████████▊| 4029/4096 [32:33<00:29,  2.31it/s] 98%|█████████▊| 4030/4096 [32:34<00:28,  2.31it/s] 98%|█████████▊| 4031/4096 [32:34<00:28,  2.31it/s] 98%|█████████▊| 4032/4096 [32:35<00:27,  2.31it/s] 98%|█████████▊| 4033/4096 [32:35<00:27,  2.31it/s] 98%|█████████▊| 4034/4096 [32:36<00:26,  2.31it/s] 99%|█████████▊| 4035/4096 [32:36<00:26,  2.31it/s] 99%|█████████▊| 4036/4096 [32:36<00:25,  2.31it/s] 99%|█████████▊| 4037/4096 [32:37<00:25,  2.31it/s] 99%|█████████▊| 4038/4096 [32:37<00:25,  2.31it/s] 99%|█████████▊| 4039/4096 [32:38<00:24,  2.31it/s] 99%|█████████▊| 4040/4096 [32:38<00:24,  2.31it/s] 99%|█████████▊| 4041/4096 [32:39<00:23,  2.31it/s] 99%|█████████▊| 4042/4096 [32:39<00:23,  2.31it/s] 99%|█████████▊| 4043/4096 [32:40<00:22,  2.31it/s] 99%|█████████▊| 4044/4096 [32:40<00:22,  2.31it/s] 99%|█████████▉| 4045/4096 [32:40<00:22,  2.31it/s] 99%|█████████▉| 4046/4096 [32:41<00:21,  2.31it/s] 99%|█████████▉| 4047/4096 [32:41<00:21,  2.31it/s] 99%|█████████▉| 4048/4096 [32:42<00:20,  2.31it/s] 99%|█████████▉| 4049/4096 [32:42<00:20,  2.31it/s] 99%|█████████▉| 4050/4096 [32:43<00:19,  2.31it/s] 99%|█████████▉| 4051/4096 [32:43<00:19,  2.31it/s] 99%|█████████▉| 4052/4096 [32:43<00:19,  2.31it/s] 99%|█████████▉| 4053/4096 [32:44<00:18,  2.31it/s] 99%|█████████▉| 4054/4096 [32:44<00:18,  2.31it/s] 99%|█████████▉| 4055/4096 [32:45<00:17,  2.31it/s] 99%|█████████▉| 4056/4096 [32:45<00:17,  2.31it/s] 99%|█████████▉| 4057/4096 [32:46<00:16,  2.31it/s] 99%|█████████▉| 4058/4096 [32:46<00:16,  2.31it/s] 99%|█████████▉| 4059/4096 [32:46<00:16,  2.31it/s] 99%|█████████▉| 4060/4096 [32:47<00:15,  2.31it/s] 99%|█████████▉| 4061/4096 [32:47<00:15,  2.31it/s] 99%|█████████▉| 4062/4096 [32:48<00:14,  2.31it/s] 99%|█████████▉| 4063/4096 [32:48<00:14,  2.31it/s] 99%|█████████▉| 4064/4096 [32:49<00:13,  2.31it/s] 99%|█████████▉| 4065/4096 [32:49<00:13,  2.30it/s] 99%|█████████▉| 4066/4096 [32:49<00:13,  2.30it/s] 99%|█████████▉| 4067/4096 [32:50<00:12,  2.31it/s] 99%|█████████▉| 4068/4096 [32:50<00:12,  2.31it/s] 99%|█████████▉| 4069/4096 [32:51<00:11,  2.31it/s] 99%|█████████▉| 4070/4096 [32:51<00:11,  2.31it/s] 99%|█████████▉| 4071/4096 [32:52<00:10,  2.31it/s] 99%|█████████▉| 4072/4096 [32:52<00:10,  2.31it/s] 99%|█████████▉| 4073/4096 [32:52<00:09,  2.31it/s] 99%|█████████▉| 4074/4096 [32:53<00:09,  2.31it/s] 99%|█████████▉| 4075/4096 [32:53<00:09,  2.31it/s]100%|█████████▉| 4076/4096 [32:54<00:08,  2.31it/s]100%|█████████▉| 4077/4096 [32:54<00:08,  2.31it/s]100%|█████████▉| 4078/4096 [32:55<00:07,  2.31it/s]100%|█████████▉| 4079/4096 [32:55<00:07,  2.31it/s]100%|█████████▉| 4080/4096 [32:56<00:06,  2.31it/s]100%|█████████▉| 4081/4096 [32:56<00:06,  2.31it/s]100%|█████████▉| 4082/4096 [32:56<00:06,  2.31it/s]100%|█████████▉| 4083/4096 [32:57<00:05,  2.31it/s]100%|█████████▉| 4084/4096 [32:57<00:05,  2.31it/s]100%|█████████▉| 4085/4096 [32:58<00:04,  2.31it/s]100%|█████████▉| 4086/4096 [32:58<00:04,  2.31it/s]100%|█████████▉| 4087/4096 [32:59<00:03,  2.31it/s]100%|█████████▉| 4088/4096 [32:59<00:03,  2.30it/s]100%|█████████▉| 4089/4096 [32:59<00:03,  2.30it/s]100%|█████████▉| 4090/4096 [33:00<00:02,  2.30it/s]100%|█████████▉| 4091/4096 [33:00<00:02,  2.31it/s]100%|█████████▉| 4092/4096 [33:01<00:01,  2.30it/s]100%|█████████▉| 4093/4096 [33:01<00:01,  2.30it/s]100%|█████████▉| 4094/4096 [33:02<00:00,  2.30it/s]100%|█████████▉| 4095/4096 [33:02<00:00,  2.31it/s]100%|██████████| 4096/4096 [33:02<00:00,  2.80it/s]loss: 2.112313747406006
===========================
epoch 32/32 | loss: 2.112313747406006
---------------------------
example true genres: 
tensor([1, 1, 0, 0, 0], device='cuda:0')
---------------------------
example predicted genres: 
tensor([0, 1, 0, 0, 0], device='cuda:0')
---------------------------
Validation Classifier Accuracy: 0.949671772428884
===========================
Using pad_token, but it is not set yet.
---------------------------
example input paragraph: 
["And Marie Antoinette - - Jacqueline Bouvier. ` ` The beautiful and light - hearted''. 5.", "It's an eighteenth - century negative, man!! Suggest the following twenty - first - century amendment : By moving the term ` ` Republic'' to lower case, substituting the modern phrase, ` ` move ahead'' for the stodgy ` ` keep'', and by using the Postmaster's name on every envelope ( in caps, of course, with the ` ` in spite'' as faded as possible ), the slogan cannot fail", 'It was our hope to educate him and to give him his freedom when the right time came, for he was a bright and friendly youth who seemed worthy of our interest. After I paid Monsieur Prieur for Dandy, I brought him home, but he was ill at ease and ran away the same night. How he returned in such a ghastly condition, or why, I cannot say.', 'That was gonna be fun collecting!! But not just yet. Feathertop was a connoisseur.', "` ` Oooo, square bit'', Feathertop screwed his face up. This guy was strictly from Outsville. But nowhere!!"]
---------------------------
example output paragraph: 
["` the - -'- ` -ddenjou - ` ` The poet garden beautiful'filled'',.", "` is s a old century century century - but '! Theppest, ` day - five century century century, ` the the editor, ` `'', the its, thevert the English English of ` ` ` to''. the firsttumgraphic ', ` ` the ', the the the the phrase Office, s `, the word. ( English, ` course, the the ` ` ` the of '. the - the. ) the editor : be to", "I was not first that be him, that the him a life to he young of passed. and he was a man man brave man. had to to the cause. He all had him,elletain, his ', he had him back to and he was not, the with un mad from night way. could was to the a conditionrudgetly state. and he would not had know.", 'He was a go a..! It it all for. Theatherss a goodchiumur.', "` ` What 'oh! you up of '! heather said said up head.. ` time was a a thelaw. he was!"]
---------------------------

  0%|          | 0/29 [00:00<?, ?it/s][A
 24%|██▍       | 7/29 [00:00<00:00, 65.92it/s][A
 48%|████▊     | 14/29 [00:00<00:00, 65.55it/s][A
 72%|███████▏  | 21/29 [00:00<00:00, 65.90it/s][A
 97%|█████████▋| 28/29 [00:00<00:00, 66.19it/s][A100%|██████████| 29/29 [00:00<00:00, 66.09it/s]
Mean Perplexity: 1351.0435018727242
===========================
100%|██████████| 4096/4096 [33:09<00:00,  2.06it/s]
Traceback (most recent call last):
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 567, in <module>
    params, unknown = parser.parse_known_args()
  File "/home/mitryand/EECS-595-Final-Project-Style-Transfer/train_model.py", line 511, in main
    model, classifier = train_all(model, classifier, train_dataloader, eval_dataloader, params, input_tokenizer, output_tokenizer)
TypeError: cannot unpack non-iterable NoneType object
